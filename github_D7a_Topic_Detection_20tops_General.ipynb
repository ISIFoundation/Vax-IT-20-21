{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Detection General\n",
    "## In this notebook:\n",
    "For each time-specific dataset:\n",
    "* We import the matrix of raw count C (tweet-term matrix) and the fitted Count Vectorizer (the vocabulary) cv;\n",
    "* We fit the TFIDF method on the raw count matrix C;\n",
    "* We do idf normalization: to avoid the + 1 of the idf term (added to avoid division by zero), we subtract 1 to all the values in the idf vector;\n",
    "* We obtain the matrix X from the TFIDF and we fit the NMF method on the X;\n",
    "* We save to file the NMF, the W and the H and we print the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import decomposition\n",
    "import joblib\n",
    "from sklearn import feature_extraction\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_analyzer(text):\n",
    "    words = [w for w in token_pattern.findall(text.lower()) if w not in stop_words]\n",
    "    return bigram[words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-COVID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "[C0, cv0] = joblib.load('/../data/counts_vocabulary_i.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.13 ms, sys: 732 µs, total: 9.86 ms\n",
      "Wall time: 9.12 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#apply the tfidf weighting\n",
    "tfidf = feature_extraction.text.TfidfTransformer()\n",
    "tfidf.fit(C0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.3 ms, sys: 83 µs, total: 57.4 ms\n",
      "Wall time: 56.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#tfidf normalization: to avoid the + 1 of the idf term (added to avoid division by zero)\n",
    "tfidf.idf_ = tfidf.idf_ - 1\n",
    "X = tfidf.transform(C0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.5 s, sys: 3.83 s, total: 32.3 s\n",
      "Wall time: 18.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#fit the non-negative matrix factorization\n",
    "nr_topics = 20\n",
    "nmf = decomposition.NMF(nr_topics,\n",
    "                        beta_loss='frobenius', solver='cd',\n",
    "                        init='nndsvd', random_state=42)\n",
    "W = nmf.fit_transform(X)\n",
    "H = nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((204275, 20), (20, 7458))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape, H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/gcrupi/6_time_windows/sparse_matrices/top_model_timewindow/tfidf_preCOVID.joblib']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save the fitted tfidf object\n",
    "joblib.dump(tfidf, '/../data/tfidf_i.joblib', compress=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/gcrupi/6_time_windows/sparse_matrices/top_model_timewindow/WHnmf_preCOVID.joblib']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#saving nmf results\n",
    "joblib.dump([W,H,nmf], '/../data/WHnmf_i.joblib', compress=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=<function phrase_analyzer at 0x7fbea8286c10>,\n",
       "                max_df=0.5, min_df=10,\n",
       "                stop_words={'a', 'abbia', 'abbiamo', 'abbiano', 'abbiate',\n",
       "                            'about', 'above', 'ad', 'after', 'again', 'against',\n",
       "                            'agl', 'agli', 'ah', 'ai', 'ain', 'al', 'alcuni',\n",
       "                            'all', 'alla', 'alle', 'allo', 'allora', 'altre',\n",
       "                            'altri', 'altro', 'am', 'an', 'anche', 'ancora', ...})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#contains the vocabulary\n",
    "cv0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T 15] Stenght: 1146.24, words:  anni,influenza,bambini,stato,medici,legge,due,antinfluenzale,salute,bimbo\n",
      "[T 1] Stenght: 625.56, words:  info,trova,kg,mesi,maschio,affida,chip,adozione,taglia_media,futura_taglia\n",
      "[T 2] Stenght: 608.47, words:  chip_gratuiti,abruzzo,tel,nord,canile,giorni,nord_arriva,ovunque,tg_media,cuccioli\n",
      "[T 9] Stenght: 545.02, words:  castrato,sano,socievole,francesca,microchip,preaffido,anno,compatibile,box,info\n",
      "[T 0] Stenght: 513.95, words:  figlie,ucciderli,disagio,scema,spieghiamo,incatenata,crea,spiegare,vaccina,contatto\n",
      "[T 6] Stenght: 417.79, words:  sciopero,fame,decenni,male,studi,creare,figli,gente,ivrea,asilo\n",
      "[T 4] Stenght: 404.99, words:  roma,fine,coccole,macchia,gioca,maschi_femmine,moglie,troppo,canile,chippato\n",
      "[T 3] Stenght: 390.77, words:  morbillo,congo,epidemia,casi,morti,robertoburioni,mila,bambini,finora,ideologia\n",
      "[T 12] Stenght: 376.39, words:  pd,obbligo,gt,mes,tav,governo,tap,stopmes,approvata,tap_tav\n",
      "[T 14] Stenght: 370.60, words:  cucciolo,mesi,volevano,volontari,mario,vecchio,buoni,educati,fratello,entrambi\n",
      "[T 18] Stenght: 339.66, words:  esiste,paragonando,togliete,razziale,intelligenza,esclusione,internet,ebrei,scuole,qui\n",
      "[T 8] Stenght: 315.81, words:  ore,antinfluenzale,email,aggiornamento,appuntamento,raccomando,online,cambiare,arrivata,alcune\n",
      "[T 10] Stenght: 309.46, words:  giuro,prima,bellissimo,potenza,zia,baci,seconda,pronto,grazia,arrivo\n",
      "[T 16] Stenght: 284.57, words:  natale,buonissimo,patria,andati,arriva,gattino,chiusa,dorme,tg,davanti\n",
      "[T 7] Stenght: 277.68, words:  adottati,rimasto,messaggio,fratelli,lasciate,abbandonato,piccola,presto,insieme,whatsapp\n",
      "[T 19] Stenght: 269.11, words:  libertà,scelta,terapeutica,dibattito,scienza,fuori,cinema,sigaretta,pretendere,parli\n",
      "[T 11] Stenght: 256.10, words:  sole,richiamo,mandate,prego,rispondo,angolo,maschietto,lavoro,piccolo,taglia_medio\n",
      "[T 13] Stenght: 251.41, words:  salvano,ong,ue,sardine,leader,popoli,droga,associazioni,religione,pace\n",
      "[T 5] Stenght: 250.72, words:  caduto,angelo,cambiamenti_climatici,suicidioassistito,cielo,aborto,incinta,rimane,maria,viene\n",
      "[T 17] Stenght: 218.21, words:  voto,diritto,romani,vecchi,manifestazione,servito,abolizione,dissenso,pensato,politiche\n"
     ]
    }
   ],
   "source": [
    "#printing topics\n",
    "feature_names = np.array(cv0.get_feature_names())\n",
    "topic_strength = W.sum(axis=0)\n",
    "for i in topic_strength.argsort()[::-1]:\n",
    "    topic_words = feature_names[np.argsort(H[i])[::-1][:10]]\n",
    "    print(\"[T %d] Stenght: %.2f, words: \" % (i, topic_strength[i]), \",\".join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del C0, cv0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## early-COVID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "[C1, cv1] = joblib.load('/../data/counts_vocabulary_ii.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.15 ms, sys: 0 ns, total: 6.15 ms\n",
      "Wall time: 5.28 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#apply the tfidf weighting\n",
    "tfidf = feature_extraction.text.TfidfTransformer()\n",
    "tfidf.fit(C1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.6 ms, sys: 0 ns, total: 29.6 ms\n",
      "Wall time: 29.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#tfidf normalization: to avoid the + 1 of the idf term (added to avoid division by zero)\n",
    "tfidf.idf_ = tfidf.idf_ - 1\n",
    "X = tfidf.transform(C1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.1 s, sys: 2.6 s, total: 34.7 s\n",
      "Wall time: 24.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#fit the non-negative matrix factorization\n",
    "nr_topics = 20\n",
    "nmf = decomposition.NMF(nr_topics,\n",
    "                        beta_loss='frobenius', solver='cd',\n",
    "                        init='nndsvd', random_state=42)\n",
    "W = nmf.fit_transform(X)\n",
    "H = nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((125887, 20), (20, 6171))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape, H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/gcrupi/6_time_windows/sparse_matrices/top_model_timewindow/tfidf_earlyCOVID.joblib']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save the fitted tfidf object\n",
    "joblib.dump(tfidf, '/../data/tfidf_ii.joblib', compress=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/gcrupi/6_time_windows/sparse_matrices/top_model_timewindow/WHnmf_earlyCOVID.joblib']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#saving nmf results\n",
    "joblib.dump([W,H,nmf], '/../data/WHnmf_ii.joblib', compress=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=<function phrase_analyzer at 0x7fbea8286c10>,\n",
       "                max_df=0.5, min_df=10,\n",
       "                stop_words={'a', 'abbia', 'abbiamo', 'abbiano', 'abbiate',\n",
       "                            'about', 'above', 'ad', 'after', 'again', 'against',\n",
       "                            'agl', 'agli', 'ah', 'ai', 'ain', 'al', 'alcuni',\n",
       "                            'all', 'alla', 'alle', 'allo', 'allora', 'altre',\n",
       "                            'altri', 'altro', 'am', 'an', 'anche', 'ancora', ...})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#contains the vocabulary\n",
    "cv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T 16] Stenght: 676.96, words:  coronavirus,tempo,possibile,breve,spera,trovino,test,australiano_supera,laboratorio,aggiornamento\n",
      "[T 14] Stenght: 639.21, words:  info,adozione,preaffido,sverminato,socievole,affida,taglia_media,castrato,cerca_casa,affido\n",
      "[T 8] Stenght: 538.81, words:  virus,esiste,cinese,anni,gravi,trovare,gira,chiama,nuovo,paura\n",
      "[T 0] Stenght: 458.99, words:  vite_umane,desidererei,implorare,ceci,trovasse,piacere,immediatamente,tante,ginocchio,motivi\n",
      "[T 1] Stenght: 449.43, words:  mandare_scuola,garantire,cavernicoli,colpisce,improvvisamente,diventare,prevenzione,libertà,figli,vedere\n",
      "[T 19] Stenght: 445.28, words:  salvini,morbillo,obbligo,scusate,abolizione,epidemia,chiudere_porti,difenderci,vera,corso\n",
      "[T 13] Stenght: 417.75, words:  bambini,scuola,cina,esclusione,tornano,inclusiva,italiani,isolamento,andare_scuola,bambini_cinesi\n",
      "[T 2] Stenght: 417.18, words:  arroganti,suggerito,cialtroni,egoisti,ignoranti,difficile,conferma,questione,troppi,paese\n",
      "[T 18] Stenght: 374.70, words:  antinfluenzale,corona_virus,inutile,intanto,tanto,nessuno,medici,testa,anziani,palle\n",
      "[T 9] Stenght: 318.22, words:  tel,nord,abruzzo,arriva_ovunque,canile,mesi,chippato,anni,buonissimo,mix\n",
      "[T 17] Stenght: 265.11, words:  riportano_malattie,scomparse,immigrati,porti_aperti,eppure,tenere,continuano,figli,radiosavana,incredibile\n",
      "[T 3] Stenght: 251.43, words:  centro_nord,adottabile,provincia,trova,media,futura_taglia,asia,lasciate,messaggio,brutta\n",
      "[T 5] Stenght: 235.50, words:  istituto_migal,vicini,israele,coronavirus,stampa,fontana,produrrà,pronto,israeliano,settimane\n",
      "[T 7] Stenght: 229.23, words:  affidano,guardate,maschi,femmine,nome,mamma,futura_taglia,microchip,media,info\n",
      "[T 10] Stenght: 212.54, words:  penne,annuncia,interno,nascosto,finalmente,oms,epidemia,coronavirus,sperare,ottobre\n",
      "[T 12] Stenght: 211.24, words:  bill_gates,complotto,miliardi,corriere,dollari,gates,sapeva,regime,ricavato,folle\n",
      "[T 11] Stenght: 197.42, words:  indifferenza,prezioso,memoria,coltivare,ingiustizie,ciascuno,sofferenze_ricordare,coscienza,aiuta,pieno\n",
      "[T 6] Stenght: 168.30, words:  estremamente,tempi,untore,caccia,nas,parlando,parlano,pericolosi,imposto,regola\n",
      "[T 15] Stenght: 142.03, words:  credi,pagata,invasione,basti,shoah,soros,origine,diffondere,laboratorio,cinese\n",
      "[T 4] Stenght: 131.57, words:  costituzionale,ricorso,porte,corte,perso,voleva,scuola_bambini,quarantena,propone,scuole\n"
     ]
    }
   ],
   "source": [
    "#printing topics\n",
    "feature_names = np.array(cv1.get_feature_names())\n",
    "topic_strength = W.sum(axis=0)\n",
    "for i in topic_strength.argsort()[::-1]:\n",
    "    topic_words = feature_names[np.argsort(H[i])[::-1][:10]]\n",
    "    print(\"[T %d] Stenght: %.2f, words: \" % (i, topic_strength[i]), \",\".join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "del C1, cv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-VAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "[C2, cv2] = joblib.load('/../data/counts_vocabulary_iii.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.2 ms, sys: 8.16 ms, total: 45.4 ms\n",
      "Wall time: 44.7 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#apply the tfidf weighting\n",
    "tfidf = feature_extraction.text.TfidfTransformer()\n",
    "tfidf.fit(C2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 290 ms, sys: 31.9 ms, total: 321 ms\n",
      "Wall time: 320 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#tfidf normalization: to avoid the + 1 of the idf term (added to avoid division by zero)\n",
    "tfidf.idf_ = tfidf.idf_ - 1\n",
    "X = tfidf.transform(C2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 19s, sys: 21.2 s, total: 3min 40s\n",
      "Wall time: 3min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#fit the non-negative matrix factorization\n",
    "nr_topics = 20\n",
    "nmf = decomposition.NMF(nr_topics,\n",
    "                        beta_loss='frobenius', solver='cd',\n",
    "                        init='nndsvd', random_state=42)\n",
    "W = nmf.fit_transform(X)\n",
    "H = nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1036177, 20), (20, 32842))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape, H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/gcrupi/6_time_windows/sparse_matrices/top_model_timewindow/tfidf_preVAX.joblib']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save the fitted tfidf object\n",
    "joblib.dump(tfidf, '/../data/tfidf_iii.joblib', compress=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/gcrupi/6_time_windows/sparse_matrices/top_model_timewindow/WHnmf_preVAX.joblib']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#saving nmf results\n",
    "joblib.dump([W,H,nmf], '/../data/WHnmf_iii.joblib', compress=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=<function phrase_analyzer at 0x7fbea8286c10>,\n",
       "                max_df=0.5, min_df=10,\n",
       "                stop_words={'a', 'abbia', 'abbiamo', 'abbiano', 'abbiate',\n",
       "                            'about', 'above', 'ad', 'after', 'again', 'against',\n",
       "                            'agl', 'agli', 'ah', 'ai', 'ain', 'al', 'alcuni',\n",
       "                            'all', 'alla', 'alle', 'allo', 'allora', 'altre',\n",
       "                            'altri', 'altro', 'am', 'an', 'anche', 'ancora', ...})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#contains the vocabulary\n",
    "cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T 14] Stenght: 3265.23, words:  virus,prima,esiste,serve,nessuno,vogliono,cura,governo,fino,niente\n",
      "[T 1] Stenght: 2307.05, words:  info,mesi,trova,kg,anni,chippato,cucciolo,famiglia,chip,centro_nord\n",
      "[T 3] Stenght: 2031.60, words:  coronavirus,test,ansa,pre_clinici,pronto,israele,anti,primo,italiano,istituto_migal\n",
      "[T 5] Stenght: 1689.58, words:  sperimentazione,uomo,italiano,spallanzani,anti_covid,ricerca,iniziata,notizia,test,volontari\n",
      "[T 15] Stenght: 1686.86, words:  bill_gates,oms,governo,astrazeneca,effetti_collaterali,schöning,persone,bambini,milioni,italiani\n",
      "[T 19] Stenght: 1478.21, words:  influenza,cinese,vinto,gara,lombardia_azienda,certificato_aifa,lombardia,anziani,stato,bambini\n",
      "[T 16] Stenght: 1332.31, words:  antinfluenzali,euro,dosi,prezzo,marzo,rifiutato,milioni,comprato,farmaco,cinque_volte\n",
      "[T 4] Stenght: 1284.49, words:  putin,russia,figlia,russo,primo,esprimono_dubbi,scacco_matto,trasformato_fanatici,decantavano_fino,cartello_criminale\n",
      "[T 9] Stenght: 1276.31, words:  antinfluenzale,morti,tarro,prof_giulio,casi,attivatore,audizione,morto,campagna,bergamo\n",
      "[T 6] Stenght: 1269.99, words:  obbligatorio,renzi,dovrà,russo,vietato,radiosavana,facoltativo,produrlo,finanziatore,anti_covid\n",
      "[T 10] Stenght: 1251.49, words:  obbligo,zingaretti,mozione,settembre,ordinanza,medici,libertà,lazio,scelta,tar_annulla\n",
      "[T 17] Stenght: 1208.40, words:  trump,cura,mondiale,nuovo_ordine,operazione_terroristica,pezzo,fauci,usa,regeneron,gratis\n",
      "[T 0] Stenght: 1164.41, words:  covid,muoiono,morti,anti_flu,spagna,piu,ue,volontari,data,radiosavana\n",
      "[T 18] Stenght: 1161.38, words:  anni,ciao,scuola,mentre,chiamo,vado,chiedere,salvi,gesù,generazione\n",
      "[T 7] Stenght: 682.44, words:  polio,conte,usa,copertura,diretta_televisiva,criticate,fedez_chiaraferragni,elvis_presley,promuovere_uso,mascherina\n",
      "[T 2] Stenght: 655.38, words:  propongo,cavia,copia_incolla,vediamo,vivere,microchip,info_francesca,portiamo,preaffido,cucciolo\n",
      "[T 8] Stenght: 626.44, words:  mondo,germania,esclusiva,punto,brevetto,metterà,azienda_tedesca,singoli_paesi,tedeschi,america\n",
      "[T 12] Stenght: 535.81, words:  spallanzani,incredibile,nessuna_crisi,direttore,sperimenterà,rigo,qualsiasi_fascismo,malati,totalitario,nessun\n",
      "[T 11] Stenght: 362.46, words:  corona,ka,cancer,informatore_spiffera,diventeranno_sterili,defenza,big_pharma,russia,india,realdonaldtrump\n",
      "[T 13] Stenght: 259.44, words:  ricercatore,riusciamo_trovare,cos,mamma,coronavirus,cuccioli,madre,stato,medico,geni\n"
     ]
    }
   ],
   "source": [
    "#printing topics\n",
    "feature_names = np.array(cv2.get_feature_names())\n",
    "topic_strength = W.sum(axis=0)\n",
    "for i in topic_strength.argsort()[::-1]:\n",
    "    topic_words = feature_names[np.argsort(H[i])[::-1][:10]]\n",
    "    print(\"[T %d] Stenght: %.2f, words: \" % (i, topic_strength[i]), \",\".join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "del C2, cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## early-VAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "[C3, cv3] = joblib.load('/../data/counts_vocabulary_iv.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 205 ms, sys: 32.1 ms, total: 237 ms\n",
      "Wall time: 235 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#apply the tfidf weighting\n",
    "tfidf = feature_extraction.text.TfidfTransformer()\n",
    "tfidf.fit(C3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.72 s, sys: 120 ms, total: 1.84 s\n",
      "Wall time: 1.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#tfidf normalization: to avoid the + 1 of the idf term (added to avoid division by zero)\n",
    "tfidf.idf_ = tfidf.idf_ - 1\n",
    "X = tfidf.transform(C3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16min 7s, sys: 1min 10s, total: 17min 18s\n",
      "Wall time: 15min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#fit the non-negative matrix factorization\n",
    "nr_topics = 20\n",
    "nmf = decomposition.NMF(nr_topics,\n",
    "                        beta_loss='frobenius', solver='cd',\n",
    "                        init='nndsvd', random_state=42)\n",
    "W = nmf.fit_transform(X)\n",
    "H = nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5137559, 20), (20, 95428))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape, H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/gcrupi/6_time_windows/sparse_matrices/top_model_timewindow/tfidf_earlyVAX.joblib']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save the fitted tfidf object\n",
    "joblib.dump(tfidf, '/../data/tfidf_iv.joblib', compress=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/gcrupi/6_time_windows/sparse_matrices/top_model_timewindow/WHnmf_earlyVAX.joblib']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#saving nmf results\n",
    "joblib.dump([W,H,nmf], '/../data/WHnmf_iv.joblib', compress=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=<function phrase_analyzer at 0x7fbea8286c10>,\n",
       "                max_df=0.5, min_df=10,\n",
       "                stop_words={'a', 'abbia', 'abbiamo', 'abbiano', 'abbiate',\n",
       "                            'about', 'above', 'ad', 'after', 'again', 'against',\n",
       "                            'agl', 'agli', 'ah', 'ai', 'ain', 'al', 'alcuni',\n",
       "                            'all', 'alla', 'alle', 'allo', 'allora', 'altre',\n",
       "                            'altri', 'altro', 'am', 'an', 'anche', 'ancora', ...})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#contains the vocabulary\n",
    "cv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T 4] Stenght: 10539.78, words:  stato,obbligo,medici,draghi,vogliono,governo,nessuno,mesi,casa,gente\n",
      "[T 1] Stenght: 8450.46, words:  covid,morto,muore,positivi,vaccinoanticovid,gennaio,muori,decessi,positivo,vaccinocovid\n",
      "[T 2] Stenght: 6475.15, words:  astrazeneca,ema,aifa,trombosi,somministrazione,sospeso,sicuro,effetti_collaterali,lotto,sospensione\n",
      "[T 17] Stenght: 5476.49, words:  virus,lockdown,varianti,serve,malattia,anno,rna,efficacia,trasmissione,dati\n",
      "[T 18] Stenght: 5039.77, words:  campagna,anticovid,corso,mattina,roma,precedenti,istituto_spallanzani,presidente_mattarella,nati,coloro\n",
      "[T 7] Stenght: 4840.33, words:  anni,riflettete,antipolio,diversa,foto,potuto,morto,muore,due,mesi\n",
      "[T 11] Stenght: 4807.17, words:  prima,giorni,vaccinata,persona,giornalisti,volta,scanzi,anziani,secondo,notizia\n",
      "[T 5] Stenght: 4677.28, words:  pfizer,funziona,moderna,efficace,dati,efficacia,israele,ceo,effetti_collaterali,seconda_dose\n",
      "[T 3] Stenght: 4498.25, words:  milioni,dosi,ue,due,somministrate,italiani,mln,figliuolo,usa,arrivo\n",
      "[T 15] Stenght: 4414.87, words:  persone,morte,ricevuto,totale,muoiono,rischio,prima_dose,milioni,giorni,categoria\n",
      "[T 8] Stenght: 4373.21, words:  europa,primi,arcuri,primo,dicembre,governo,ritwittate,dobbiamo,vorrei_chiedere,iniziano\n",
      "[T 6] Stenght: 4110.59, words:  morti,anziani,dati,giorno,causa,vaiolo,vedete,stati,contagi,numero\n",
      "[T 13] Stenght: 3641.38, words:  lombardia,regione,lazio,bertolaso,moratti,caos,fontana,salvini,regioni,gallera\n",
      "[T 10] Stenght: 3369.43, words:  mila,giorno,germania,governo,spagna,mila_dosi,svezia,settimana,giorni,popolazione\n",
      "[T 14] Stenght: 2908.12, words:  coronavirus,somministrati,regione,gennaio,moderna,efficace,fino,dubbi,effettuate,primo\n",
      "[T 9] Stenght: 2867.87, words:  anti_covid,effettuate,gennaio,ansa,notizia,muore,somministrazione,ricevuto,giorni,prenotazioni\n",
      "[T 16] Stenght: 2398.95, words:  johnson,johnson_amp,ema,usa,funziona,aprile,johnsonandjohnson,libera,ansa,unica\n",
      "[T 19] Stenght: 2223.10, words:  cazzo,fatevi,iniziate,rompere_coglioni,serve,frega,vita,capito,devo,volete\n",
      "[T 12] Stenght: 1828.97, words:  paura,fuori,basta,abbiam,esempidelcazzo,motivazione,possa,gente,qui,morire\n",
      "[T 0] Stenght: 1823.68, words:  piano,arcuri,governo,draghi,conte,regioni,nuovo,priorità,nazionale,distribuzione\n"
     ]
    }
   ],
   "source": [
    "#printing topics\n",
    "feature_names = np.array(cv3.get_feature_names())\n",
    "topic_strength = W.sum(axis=0)\n",
    "for i in topic_strength.argsort()[::-1]:\n",
    "    topic_words = feature_names[np.argsort(H[i])[::-1][:10]]\n",
    "    print(\"[T %d] Stenght: %.2f, words: \" % (i, topic_strength[i]), \",\".join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "del C3, cv3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAX-drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "[C4, cv4] = joblib.load('/../data/counts_vocabulary_v.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 162 ms, sys: 23.8 ms, total: 186 ms\n",
      "Wall time: 184 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#apply the tfidf weighting\n",
    "tfidf = feature_extraction.text.TfidfTransformer()\n",
    "tfidf.fit(C4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.33 s, sys: 92.2 ms, total: 1.43 s\n",
      "Wall time: 1.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#tfidf normalization: to avoid the + 1 of the idf term (added to avoid division by zero)\n",
    "tfidf.idf_ = tfidf.idf_ - 1\n",
    "X = tfidf.transform(C4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 32s, sys: 1min 6s, total: 16min 39s\n",
      "Wall time: 15min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#fit the non-negative matrix factorization\n",
    "nr_topics = 20\n",
    "nmf = decomposition.NMF(nr_topics,\n",
    "                        beta_loss='frobenius', solver='cd',\n",
    "                        init='nndsvd', random_state=42)\n",
    "W = nmf.fit_transform(X)\n",
    "H = nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4160533, 20), (20, 85922))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape, H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/gcrupi/6_time_windows/sparse_matrices/top_model_timewindow/tfidf_VAXdrive.joblib']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save the fitted tfidf object\n",
    "joblib.dump(tfidf, '/../data/tfidf_v.joblib', compress=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/gcrupi/6_time_windows/sparse_matrices/top_model_timewindow/WHnmf_VAXdrive.joblib']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#saving nmf results\n",
    "joblib.dump([W,H,nmf], '/../data/WHnmf_v.joblib', compress=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv4' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-565024fdf404>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#contains the vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcv4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cv4' is not defined"
     ]
    }
   ],
   "source": [
    "#contains the vocabulary\n",
    "cv4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T 5] Stenght: 6250.78, words:  covid,ricoverati,bambini,ospedale,casi,giugno,decessi,positivi,preso,cura\n",
      "[T 0] Stenght: 5519.50, words:  greenpass,andare,vogliono,casa,gente,niente,pass,nessuno,draghi,basta\n",
      "[T 17] Stenght: 4943.84, words:  persone,giorno,mila,morte,decessi,ricevuto,contagiose,migliaia,completamente,possono\n",
      "[T 16] Stenght: 4665.76, words:  virus,variante_delta,varianti,serve,circolazione,rischio,contrarre,mattarella,fauci,dovere_morale\n",
      "[T 12] Stenght: 4102.39, words:  pfizer,seconda_dose,israele,efficacia,dosi,giorni,variante_delta,prima_dose,muore,dose\n",
      "[T 18] Stenght: 3974.52, words:  stato,dose,almeno,vaiolo,emergenza,diritto,sperimentale,popolazione,completamente,muori\n",
      "[T 13] Stenght: 3853.36, words:  prima,seconda_dose,giorno,vaccinatevi,tampone,volta,adesso,nomask,nopass,pragmatica\n",
      "[T 9] Stenght: 3692.78, words:  milioni,italiani,dosi,dose,stati,coronavirus,ricevuto_almeno,figliuolo,dati_aggiornati,colpiti\n",
      "[T 4] Stenght: 3636.65, words:  anni,muore,morta,due,foto,antipolio,riflettete,potuto,diversa,morto\n",
      "[T 6] Stenght: 3504.86, words:  morti,numero,contagi,dati,israele,contagiati,reazioni_avverse,maggio,luglio,casi\n",
      "[T 8] Stenght: 3486.92, words:  astrazeneca,trombosi,prima_dose,enne,mix,speranza,pfizer_moderna,aifa,figliuolo,morta\n",
      "[T 14] Stenght: 3183.78, words:  libertà,macron,passaporto,francia,parigi,rivolta,francesi,scelta,chiesta,presa\n",
      "[T 11] Stenght: 3070.73, words:  green_pass,tampone,entrare,pare,scherziamo,obbligatorio,bar_ristoranti,serve,chiedo,resta\n",
      "[T 3] Stenght: 3016.34, words:  obbligo,sanitari,votato,medici,personale_sanitario,passaporto_sanitario,legge,costituzione,noobbligovaccinale,favore\n",
      "[T 15] Stenght: 2852.82, words:  giovani,cazzo,rischio,mese,gianfranco,campagna,colpa,aperto,anziani,miocardite\n",
      "[T 10] Stenght: 2186.39, words:  morire,appello_morire,appello,mario_draghi,draghi,salvini,presidente,ammali_muori,muori,draghi_appello\n",
      "[T 19] Stenght: 2009.58, words:  prenotato,giugno,salvini,data,massa,finalmente,prima_dose,ragazzi_ragazze,genere_umano,credere\n",
      "[T 1] Stenght: 1161.43, words:  enga,actor_siddharth,vaccine_enga_da_dei,bjpbetrayingtnpeople,narayanan,narendramodi,covidvaccine,bjp_india,coimbatore,mkstalin\n",
      "[T 2] Stenght: 965.43, words:  procrastinate,mr_modi,novaccinenovacancy,rahulgandhi,modi,incindia,ndtv,narendramodi_pmoindia,india,srinivasiyc\n",
      "[T 7] Stenght: 748.66, words:  vaccination,indonesia,china,certificate,imagine,dia,affidi,bambini,scienza,data\n"
     ]
    }
   ],
   "source": [
    "#printing topics\n",
    "feature_names = np.array(cv4.get_feature_names())\n",
    "topic_strength = W.sum(axis=0)\n",
    "for i in topic_strength.argsort()[::-1]:\n",
    "    topic_words = feature_names[np.argsort(H[i])[::-1][:10]]\n",
    "    print(\"[T %d] Stenght: %.2f, words: \" % (i, topic_strength[i]), \",\".join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del C4, cv4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## late-VAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "[C5, cv5] = joblib.load('/../data/counts_vocabulary_vi.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 239 ms, sys: 32.1 ms, total: 271 ms\n",
      "Wall time: 269 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#apply the tfidf weighting\n",
    "tfidf = feature_extraction.text.TfidfTransformer()\n",
    "tfidf.fit(C5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.79 s, sys: 148 ms, total: 1.94 s\n",
      "Wall time: 1.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#tfidf normalization: to avoid the + 1 of the idf term (added to avoid division by zero)\n",
    "tfidf.idf_ = tfidf.idf_ - 1\n",
    "X = tfidf.transform(C5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gcrupi/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:1076: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26min 42s, sys: 1min 50s, total: 28min 32s\n",
      "Wall time: 26min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#fit the non-negative matrix factorization\n",
    "nr_topics = 20\n",
    "nmf = decomposition.NMF(nr_topics,\n",
    "                        beta_loss='frobenius', solver='cd',\n",
    "                        init='nndsvd', random_state=42)\n",
    "W = nmf.fit_transform(X)\n",
    "H = nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5450244, 20), (20, 100285))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape, H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/gcrupi/6_time_windows/sparse_matrices/top_model_timewindow/tfidf_lateVAX.joblib']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save the fitted tfidf object\n",
    "joblib.dump(tfidf, '/../data/tfidf_vi.joblib', compress=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/gcrupi/6_time_windows/sparse_matrices/top_model_timewindow/WHnmf_lateVAX.joblib']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#saving nmf results\n",
    "joblib.dump([W,H,nmf], '/../data/WHnmf_vi.joblib', compress=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=<function phrase_analyzer at 0x7fbea8286c10>,\n",
       "                max_df=0.5, min_df=10,\n",
       "                stop_words={'a', 'abbia', 'abbiamo', 'abbiano', 'abbiate',\n",
       "                            'about', 'above', 'ad', 'after', 'again', 'against',\n",
       "                            'agl', 'agli', 'ah', 'ai', 'ain', 'al', 'alcuni',\n",
       "                            'all', 'alla', 'alle', 'allo', 'allora', 'altre',\n",
       "                            'altri', 'altro', 'am', 'an', 'anche', 'ancora', ...})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#contains the vocabulary\n",
    "cv5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T 18] Stenght: 14124.08, words:  virus,serve,gente,paura,cazzo,capito,medico,nessuno,funziona,vero\n",
      "[T 15] Stenght: 6507.83, words:  tamponi,tampone,gratis,lavoro,lavoratori,pagare,vogliono,settimana,lavorare,ore\n",
      "[T 5] Stenght: 5847.54, words:  green_pass,obbligatorio,governo,giorno,nogreenpass,trieste,contagi,possesso,strumento,nulla\n",
      "[T 3] Stenght: 5735.92, words:  obbligo,passaporto_sanitario,draghi,manifestazione,governo,grande,operatori_sanitari,legge,massa,sanitari\n",
      "[T 13] Stenght: 5229.42, words:  prima,dose,almeno,ricevuto,morto,seconda_dose,volta,enne,seconda,popolazione\n",
      "[T 17] Stenght: 5147.79, words:  mesi,due,protezione,efficacia,anno,casa,immunità,durata,dosi,richiamo\n",
      "[T 9] Stenght: 4757.94, words:  greenpass,nogreenpass,greenpassobbligatorio,governo,obbligatorio,draghi,serve,gran_bretagna,contrari,ricatto\n",
      "[T 19] Stenght: 4594.20, words:  persone,morte,migliaia,muoiono,causa,manifestazioni,vogliono,possono,decessi,miliardi\n",
      "[T 4] Stenght: 4395.50, words:  anni,muore,medico,morto,giorno,due,morta,giorni,seconda_dose,morire\n",
      "[T 12] Stenght: 4227.89, words:  pfizer,fda,protegge,dati,usa,giorni,seconda_dose,studio,sicuro,enne\n",
      "[T 16] Stenght: 3950.64, words:  ricoverati,ospedale,israele,doppia_dose,positivi,reparti,casi,terapia_intensiva,finisce,vanno\n",
      "[T 2] Stenght: 3831.90, words:  morti,giorno,contagi,numero,aifa,post,taiwan,giovani,anno,casi\n",
      "[T 14] Stenght: 3739.81, words:  milioni,italiani,dosi,euro,mese,draghi,lavoratori,mila,campagna,governo\n",
      "[T 11] Stenght: 3639.14, words:  bambini,vogliono,rischio,genitori,giovani,direttore_spallanzani,stop,pendendo_verso,scuola,passaporto_sanitario\n",
      "[T 1] Stenght: 3529.02, words:  stato,emergenza,diritto,responsabilità,consiglio,ricatto,legge,stesso,poliziotto,migranti\n",
      "[T 10] Stenght: 3370.90, words:  libertà,invochi,mattarella,sottrarsi,pericolo,vita_altrui,scelta,licenza,mettere,caso\n",
      "[T 8] Stenght: 3328.51, words:  agosto,terapia_intensiva,ricoveri,dati,nessun,salute,dosi,ministero,settembre,decessi\n",
      "[T 0] Stenght: 3238.24, words:  covid,decessi,morto,muore,reparti,contagiati,casi,medici,positivo,influenza\n",
      "[T 6] Stenght: 2648.70, words:  terza_dose,draghi,israele,anti_covid,serve,funziona,tanto,report,fragili,aumenta\n",
      "[T 7] Stenght: 748.64, words:  mandates,american,anti,propaganda,fascist,works,pro,new_york,protest,covid_\n"
     ]
    }
   ],
   "source": [
    "#printing topics\n",
    "feature_names = np.array(cv5.get_feature_names())\n",
    "topic_strength = W.sum(axis=0)\n",
    "for i in topic_strength.argsort()[::-1]:\n",
    "    topic_words = feature_names[np.argsort(H[i])[::-1][:10]]\n",
    "    print(\"[T %d] Stenght: %.2f, words: \" % (i, topic_strength[i]), \",\".join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "del C5, cv5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
