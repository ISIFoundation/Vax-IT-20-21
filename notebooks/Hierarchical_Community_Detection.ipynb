{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Community Detection\n",
    "## In this notebook:\n",
    "* We run the following algorithm on each of the six networks. The algorithm:\n",
    "\n",
    "1. takes the graph G and samples it by removing half of the edges (or nodes) and extracting the GCC (let's call it wcc).\n",
    "2. runs hierarchical clustering algorithm paris on wcc and extract the dendrogram Z.\n",
    "3. cuts Z at 10 different heights (from 2 to 12 communities) and evaluates modularity for each of the 10 partitions.\n",
    "4. given the set of modularities it looks for the index of the 'jump'. We have a 'jump' in modularity when the value i+1 is at least 10% greater than value i.\n",
    "5. selects as best partition the partition of the index of the 'jump'.\n",
    "6. removes nodes belonging to small communities (i.e. communities with size < 100).\n",
    "7. At this point, what we have is: a set of nodes (wcc nodes. The ones sampled at the beginning) and a best_partition (a set of labels assigning each node to a community).\n",
    "8. steps from 1. to 6. are repeated 100 times. So we get 100 different set of nodes (from 10 independent samples of the same network) and 100 different best_partitions.\n",
    "\n",
    "* Then we plot experimental RMCA distribution. We define RMCA (Rate of the Most Common Assignment) as the normalized occurrence of the most common label of a certain node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.read_csv('/../github_alltime_edgelist.csv').drop(['Unnamed: 0'],axis=1)\n",
    "t[t['weight'] > 1] #applying filter on edges' weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usrs_net_df0 = t[t['time_window'] == 'i']\n",
    "usrs_net_df1 = t[t['time_window'] == 'ii']\n",
    "usrs_net_df2 = t[t['time_window'] == 'iii']\n",
    "usrs_net_df3 = t[t['time_window'] == 'iv']\n",
    "usrs_net_df4 = t[t['time_window'] == 'v']\n",
    "usrs_net_df5 = t[t['time_window'] == 'vi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I apply the filter on the weight. I discard links with weight = 1\n",
    "G0w = nx.from_pandas_edgelist(usrs_net_df0[usrs_net_df0['weight'] > 1], source='user1', target='user2', edge_attr=True, create_using=nx.DiGraph())\n",
    "G1w = nx.from_pandas_edgelist(usrs_net_df1[usrs_net_df1['weight'] > 1], source='user1', target='user2', edge_attr=True, create_using=nx.DiGraph())\n",
    "G2w = nx.from_pandas_edgelist(usrs_net_df2[usrs_net_df2['weight'] > 1], source='user1', target='user2', edge_attr=True, create_using=nx.DiGraph())\n",
    "G3w = nx.from_pandas_edgelist(usrs_net_df3[usrs_net_df3['weight'] > 1], source='user1', target='user2', edge_attr=True, create_using=nx.DiGraph())\n",
    "G4w = nx.from_pandas_edgelist(usrs_net_df4[usrs_net_df4['weight'] > 1], source='user1', target='user2', edge_attr=True, create_using=nx.DiGraph())\n",
    "G5w = nx.from_pandas_edgelist(usrs_net_df5[usrs_net_df5['weight'] > 1], source='user1', target='user2', edge_attr=True, create_using=nx.DiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del G0w,G1w,G2w,G3w,G4w,G5w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the GCC out of the network obtained by the application of the filter weight > 1\n",
    "wcc0_w = max(nx.weakly_connected_components(G0w), key=len) \n",
    "wcc0_w = G0w.subgraph(wcc0_w).copy()\n",
    "\n",
    "wcc1_w = max(nx.weakly_connected_components(G1w), key=len) \n",
    "wcc1_w = G1w.subgraph(wcc1_w).copy()\n",
    "\n",
    "wcc2_w = max(nx.weakly_connected_components(G2w), key=len) \n",
    "wcc2_w = G2w.subgraph(wcc2_w).copy()\n",
    "\n",
    "wcc3_w = max(nx.weakly_connected_components(G3w), key=len) \n",
    "wcc3_w = G3w.subgraph(wcc3_w).copy()\n",
    "\n",
    "wcc4_w = max(nx.weakly_connected_components(G4w), key=len) \n",
    "wcc4_w = G4w.subgraph(wcc4_w).copy()\n",
    "\n",
    "wcc5_w = max(nx.weakly_connected_components(G5w), key=len) \n",
    "wcc5_w = G5w.subgraph(wcc5_w).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N0wcc_w, N1wcc_w, N2wcc_w, N3wcc_w, N4wcc_w, N5wcc_w = wcc0_w.number_of_nodes(), wcc1_w.number_of_nodes(), wcc2_w.number_of_nodes(), wcc3_w.number_of_nodes(), wcc4_w.number_of_nodes(), wcc5_w.number_of_nodes()\n",
    "n0wcc_w, n1wcc_w, n2wcc_w, n3wcc_w, n4wcc_w, n5wcc_w = wcc0_w.number_of_edges(), wcc1_w.number_of_edges(), wcc2_w.number_of_edges(), wcc3_w.number_of_edges(), wcc4_w.number_of_edges(), wcc5_w.number_of_edges()\n",
    "\n",
    "print('i   -> Number of nodes:', N0wcc_w, ', number of edges', n0wcc_w)\n",
    "print('ii  -> Number of nodes:', N1wcc_w, ', number of edges', n1wcc_w)\n",
    "print('iii -> Number of nodes:', N2wcc_w, ', number of edges', n2wcc_w)\n",
    "print('iv  -> Number of nodes:', N3wcc_w, ', number of edges', n3wcc_w)\n",
    "print('v   -> Number of nodes:', N4wcc_w, ', number of edges', n4wcc_w)\n",
    "print('vi  -> Number of nodes:', N5wcc_w, ', number of edges', n5wcc_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total number of tweets in the preCOVID retweet network \n",
    "s = 0\n",
    "for e in wcc0_w.edges:\n",
    "    s+=wcc0_w.get_edge_data(*e)['weight']\n",
    "s    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "for e in wcc1_w.edges:\n",
    "    s+=wcc1_w.get_edge_data(*e)['weight']\n",
    "s    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "for e in wcc2_w.edges:\n",
    "    s+=wcc2_w.get_edge_data(*e)['weight']\n",
    "s    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "for e in wcc3_w.edges:\n",
    "    s+=wcc3_w.get_edge_data(*e)['weight']\n",
    "s    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "for e in wcc4_w.edges:\n",
    "    s+=wcc4_w.get_edge_data(*e)['weight']\n",
    "s    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "for e in wcc5_w.edges:\n",
    "    s+=wcc5_w.get_edge_data(*e)['weight']\n",
    "s    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcc1_w['3327831430']['882965185237065728']['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying natural log to the weights of all the links\n",
    "for e in wcc0_w.edges:\n",
    "    wcc0_w.get_edge_data(*e)['weight'] = np.log(wcc0_w.get_edge_data(*e)['weight'])\n",
    "for e in wcc1_w.edges:\n",
    "    wcc1_w.get_edge_data(*e)['weight'] = np.log(wcc1_w.get_edge_data(*e)['weight'])\n",
    "for e in wcc2_w.edges:\n",
    "    wcc2_w.get_edge_data(*e)['weight'] = np.log(wcc2_w.get_edge_data(*e)['weight'])\n",
    "for e in wcc3_w.edges:\n",
    "    wcc3_w.get_edge_data(*e)['weight'] = np.log(wcc3_w.get_edge_data(*e)['weight'])\n",
    "for e in wcc4_w.edges:\n",
    "    wcc4_w.get_edge_data(*e)['weight'] = np.log(wcc4_w.get_edge_data(*e)['weight'])\n",
    "for e in wcc5_w.edges:\n",
    "    wcc5_w.get_edge_data(*e)['weight'] = np.log(wcc5_w.get_edge_data(*e)['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcc1_w['3327831430']['882965185237065728']['weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sknetwork.hierarchy import Paris, Ward\n",
    "from scipy.cluster.hierarchy import dendrogram, fcluster\n",
    "import random\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "import networkx.algorithms.community as nx_comm\n",
    "from itertools import combinations\n",
    "from statistics import mean\n",
    "from statistics import stdev\n",
    "import collections\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partition(clustering_array, G): #clustering_array represents a certain partition of the network G\n",
    "    #clustering array is a numpy array containing the indices of the communities to which each node is associated. \n",
    "    #For example, if there are just 2 communities, clustering_array is going to be something \n",
    "    #like [1,1,2,1,2, ... ,2,2,1,2]. So the first node goes into community 1, so does the second, \n",
    "    #the third node goes into community 2, etc...\n",
    "    #G is the network of which I want to get a partition\n",
    "    series = pd.Series(clustering_array)\n",
    "    series.index = G.nodes\n",
    "    communities = np.unique(clustering_array) #gets all the different communities\n",
    "    partition = []\n",
    "    for c in communities:\n",
    "        ind = series[series == c].index #indices assigned to the community c\n",
    "        partition.append(ind)\n",
    "    \n",
    "    return partition #it is something like [indices1, indices2, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given a list, this function returns the index of the value after the 'jump'\n",
    "#by 'jump' we mean a sudden increase of the values of the list\n",
    "#the jump must be at least perc% to be considered jump\n",
    "\n",
    "def jump_detector(l, hc_alg, perc, min_val = 0.4):\n",
    "    s = pd.Series(l)\n",
    "    #print(s)\n",
    "    opt = 0\n",
    "    for i in s.index:\n",
    "        if s[i] < min_val: #sometimes modularity starts from zero, increases to ~0.4 and then has another jump to\n",
    "            #~0.5, so we want to keep the second jump and not the first one.\n",
    "            continue\n",
    "        if hc_alg == 'ward':\n",
    "            if s[i+1] < s[i]:\n",
    "                opt = i\n",
    "                return opt\n",
    "\n",
    "            elif (s[i+1]-s[i])/s[i] > perc: #if finds a suitable big enough jump the function returns the index of the jump\n",
    "                \n",
    "                opt = i + 1\n",
    "                return opt\n",
    "            \n",
    "        elif hc_alg == 'paris':\n",
    "            \n",
    "            if (s[i+1]-s[i])/s[i] > perc: #if finds a suitable big enough jump the function returns the index of the jump\n",
    "                \n",
    "                opt = i + 1\n",
    "                return opt\n",
    "\n",
    "        if i==len(s)-2 and opt==0:#if gets to the end without finding a jump it takes the index of the minimum \n",
    "            #value grater than the threshold min_val\n",
    "\n",
    "            s = s[s>min_val]\n",
    "\n",
    "            opt = min(s.index)\n",
    "            return opt\n",
    "            \n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the modularities associated to each partition present in clustering_arrays\n",
    "def get_modularities(clustering_arrays,G):\n",
    "    mod_array = []\n",
    "    for c_a in clustering_arrays:\n",
    "        partition = get_partition(c_a,G)\n",
    "        mod = nx_comm.modularity(G,partition)\n",
    "        mod_array.append(mod)\n",
    "    \n",
    "    return mod_array\n",
    "\n",
    "#here is where the hierarchical clustering algorithm runs on GCC of the sampled network\n",
    "#then the dendrogram is cut at 10 different heights and the corresponding clustering_arrays are put together\n",
    "#then we evaluate the modularity of each partition (ie clustering_array)\n",
    "#then we select the index of the jump\n",
    "def dendrogram_and_jumpidx(G,hc_alg, min_val, perc):\n",
    "    if hc_alg == 'paris':\n",
    "        paris_ward = Paris()\n",
    "    \n",
    "    elif hc_alg == 'ward':\n",
    "        paris_ward = Ward()\n",
    "        \n",
    "    else:\n",
    "        print(hc_alg,': unknown clustering algorithm')\n",
    "        return\n",
    "    \n",
    "    adj_m = nx.adjacency_matrix(G)\n",
    "    Z0 = paris_ward.fit_transform(adj_m)\n",
    "    \n",
    "\n",
    "    clust_arrays = []\n",
    "    for ti in range(2,11,1):    \n",
    "        clust_array = fcluster(Z=Z0,t=ti,criterion='maxclust')\n",
    "        clust_arrays.append(clust_array)\n",
    "\n",
    "    \n",
    "    mods = get_modularities(clust_arrays,G)\n",
    "    jd = jump_detector(mods,hc_alg=hc_alg, min_val=min_val, perc= perc)\n",
    "    \n",
    "\n",
    "    \n",
    "    cab = fcluster(Z=Z0,t= jd+2, criterion='maxclust') #clust_array_best\n",
    "    \n",
    "    return cab ,jd, mods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes half random links from a network, then returns the GCC of the remaning network\n",
    "def remove_edges(g, fraction): #with fraction=2 we remove half of the links\n",
    "    G = g.copy()\n",
    "    n = G.number_of_edges()\n",
    "    to_remove=random.sample(G.edges(), k=int(n/fraction))\n",
    "    G.remove_edges_from(to_remove)\n",
    "    \n",
    "    wcc = max(nx.weakly_connected_components(G), key=len) \n",
    "    wcc = G.subgraph(wcc).copy()\n",
    "\n",
    "    return wcc\n",
    "\n",
    "def remove_nodes(g, fraction): #with fraction=2 we remove half of the links\n",
    "    G = g.copy()\n",
    "    N = G.number_of_nodes()\n",
    "    to_remove=random.sample(G.nodes(), k=int(N/fraction))\n",
    "    G.remove_nodes_from(to_remove)\n",
    "    \n",
    "    wcc = max(nx.weakly_connected_components(G), key=len) \n",
    "    wcc = G.subgraph(wcc).copy()\n",
    "\n",
    "    return wcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes as keys the nodes of a certain GCC and as values the list containing the label of the community to which\n",
    "#each node belongs to. thr is the threshold for small communities. All the communities smaller than threshold are \n",
    "#removed (ie all the nodes belonging to the small community are removed).\n",
    "def remove_small_communities(keys, values, thr):\n",
    "    label_dict = dict(zip(keys, values)) #creates a dict with keys: nodes, values: community label\n",
    "    labels = label_dict.values() #takes the list of the labels\n",
    "    \n",
    "    bins = np.bincount(list(labels))[1:] #evaluates how many nodes there are in each community\n",
    "    \n",
    "    to_remove = []\n",
    "    for i in range(len(bins)): #append the labels to remove to a list\n",
    "        if bins[i] < thr:\n",
    "            to_remove.append(i+1)\n",
    "            \n",
    "    for l in to_remove: #removes the labels present in the list to_remove\n",
    "        label_dict = dict((k, v) for k, v in label_dict.items() if v != l)\n",
    "    \n",
    "    #after removing small communities\n",
    "    #we want the community to be ordered by size (ie the smallest community is always labeled 1 and the biggest\n",
    "    #is always labeled N, where N is the number of communities.)\n",
    "    ll = np.arange(1,len(set(label_dict.values())) + 1, 1) #it is just an array like [1,2, ... , N]\n",
    "    \n",
    "    #we want set(dict.values) to be equal to ll, because sometimes we have for example 4 communities and the third \n",
    "    #is 'small' then it is removed and so set(dict.values) is {1,2,4} when we want it to be {1,2,3}.\n",
    "    chs = changes_to_make(list(set(label_dict.values())),list(ll))#gets the changes to transform set(dict.values)\n",
    "    #into [1,2,...,N]\n",
    "    \n",
    "    s = pd.Series(data=label_dict.values(), index=label_dict.keys())#turns the dict into a series to use 'replace'\n",
    "    \n",
    "    for ch in chs: #renames communities to be renamed\n",
    "        s = s.replace(ch[0],'x')\n",
    "        s = s.replace(ch[1],'y')\n",
    "            \n",
    "        s = s.replace('x', ch[1])\n",
    "        s = s.replace('y', ch[0])\n",
    "            \n",
    "    \n",
    "    label_dict = s.to_dict() #back to dict\n",
    "    \n",
    "\n",
    "    return label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes to lists and returns the changes (ie the switchs of elements) to make l equal to true_l\n",
    "def changes_to_make(l,true_l):\n",
    "    s = pd.Series(l)\n",
    "    changes = []\n",
    "\n",
    "    while list(s) != true_l:\n",
    "\n",
    "        chs = [(i,j) for i,j in zip(list(s),true_l)]\n",
    "        for i in range(len(chs)):\n",
    "            if chs[i][0] == chs[i][1]:\n",
    "                continue\n",
    "            \n",
    "            change = chs[i]\n",
    "\n",
    "            s = s.replace(change[0],'x')\n",
    "            s = s.replace(change[1],'y')\n",
    "            \n",
    "            s = s.replace('x', change[1])\n",
    "            s = s.replace('y', change[0])\n",
    "            \n",
    "            changes.append(change)\n",
    "            break\n",
    "            \n",
    "    return changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes a list of labels [1,1,2,4,5,2,1,...] and a list of changes to make. For example we may want to switch\n",
    "#(2,1) or (1,3) etc.\n",
    "#returns the list of labels with the changes made.\n",
    "def lab_adjuster(labels,changes):\n",
    "    \n",
    "    s = pd.Series(labels) \n",
    "    \n",
    "    for change in changes: #renames community according to changes\n",
    "        s = s.replace(change[0],'x')\n",
    "        s = s.replace(change[1],'y')\n",
    "            \n",
    "        s = s.replace('x', change[1])\n",
    "        s = s.replace('y', change[0])\n",
    "    \n",
    "    return list(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#every tuple contains (nodes of the sample, ordered label). The labels are now ordered by community size.\n",
    "#so if a node belongs to the smallest community it is labeled 1, if it belongs to the biggest community it is \n",
    "#labeled N.\n",
    "#returns a dictionary like: keys: nodes, values: list containing all the labels of that nodes in different samples.\n",
    "def nodes_dictionary(tuples):\n",
    "    nodes_dict = {}\n",
    "    for tup in tuples:\n",
    "        for node, label in zip(tup[0],tup[1]):\n",
    "\n",
    "            nodes_dict.setdefault(node,[])\n",
    "            nodes_dict[node].append(label)\n",
    "            \n",
    "    return nodes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes two different sets of nodes of the same network which was sampled twice\n",
    "#and two clust_arrays containing the assignment to the community of the nodes in each set of nodes\n",
    "#finds common nodes and puts into a df each common node (index) and the two different assignment from the two \n",
    "#different partitions\n",
    "\n",
    "def common_nodes_label(nodes1,nodes2,clust_array1, clust_array2):\n",
    "    \n",
    "    zip_iterator1 = zip(nodes1, clust_array1)\n",
    "    zip_iterator2 = zip(nodes2, clust_array2)\n",
    "    \n",
    "    dict1 = dict(zip_iterator1)\n",
    "    dict2 = dict(zip_iterator2)\n",
    "    \n",
    "    common_nodes = list(set(nodes1).intersection(set(nodes2)))\n",
    "    \n",
    "    new_dict1 = {your_key: dict1[your_key] for your_key in common_nodes}\n",
    "    new_dict2 = {your_key: dict2[your_key] for your_key in common_nodes}\n",
    "    \n",
    "    df = pd.DataFrame(index=common_nodes)\n",
    "    df['d1'] = new_dict1.values()\n",
    "    df['d2'] = new_dict2.values()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the sum of non-diagonal elements of a matrix\n",
    "def sum_non_diagonal(mat):\n",
    "    rw, cl = mat.shape\n",
    "    dia = np.diag_indices(min(rw,cl)) #indices of diagonal elements\n",
    "    dia_sum = sum(mat[dia]) # sum of diagonal elements\n",
    "    off_dia_sum = np.sum(mat) - dia_sum # subtract the diagonal sum from total array sum\n",
    "    return off_dia_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#swaps two rows (x,y) of a given matrix a\n",
    "def swap_rows(a,x,y):\n",
    "    a[[x, y]] = a[[y, x]]\n",
    "    return a\n",
    "\n",
    "#takes a matrix and returns the couples of rows to be swapped in order to make that matrix diagonal\n",
    "def changes_to_make2(mx):\n",
    "    changes = []\n",
    "    \n",
    "    rows, columns = mx.shape\n",
    "    \n",
    "    if sum_non_diagonal(mx) == 0.:\n",
    "        return changes\n",
    "    \n",
    "    else:\n",
    "        while sum_non_diagonal(mx) != 0.:\n",
    "            for r in range(rows):\n",
    "                am = np.argmax(mx[r]) #argmax of the row r\n",
    "                if am != r:\n",
    "                    changes.append((r+1,am+1))\n",
    "                    mx = swap_rows(mx,am,r)\n",
    "                    if sum_non_diagonal(mx) == 0.:\n",
    "                        return changes\n",
    "                \n",
    "                else:\n",
    "                    continue\n",
    "                        \n",
    "    return changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes a confusion matrix and returns the changes to make in order to make the matrix have the larger value of each \n",
    "#row on the diagonal\n",
    "def go_through_cm(cm):\n",
    "    \n",
    "    if cm.shape != cm[~np.all(cm == 0, axis=1)].shape:\n",
    "        print('!')\n",
    "    \n",
    "    cm = cm[~np.all(cm == 0, axis=1)] #removing rows (or columns with all zeroes)\n",
    "    \n",
    "    cm = cm.transpose()\n",
    "    \n",
    "    if cm.shape != cm[~np.all(cm == 0, axis=1)].shape:\n",
    "        print('!!')\n",
    "    \n",
    "    cm = cm[~np.all(cm == 0, axis=1)]\n",
    "    \n",
    "    cm = cm.transpose()\n",
    "    \n",
    "    rows, columns = cm.shape\n",
    "    \n",
    "    I = np.zeros(rows*columns).reshape(rows,columns)\n",
    "    max_ind = cm.argmax(1) #list saying what is the index of the max in each row\n",
    "    \n",
    "    for i in range(len(max_ind)): #creates I with 1 in the argmax position and zero elsewhere\n",
    "        \n",
    "        ind = max_ind[i]\n",
    "        \n",
    "        I[i][ind] = 1\n",
    "\n",
    "    I = I.transpose() #so that I have to switch rows and not columns\n",
    "    return changes_to_make2(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given a network it samples the network, does Paris hierarchical clustering, assigns a label to the nodes \n",
    "#of the sample. Repeats this for N times and return a dictionary having keys: nodes' id and values: a list\n",
    "#containing the labels of the nodes. The length of the lists is going to be <= N.\n",
    "def partitions_dict2(g, fraction=2, edges=True, N=100, hc_alg = 'paris', min_val = .4, perc=0.1, thr=100):\n",
    "    \n",
    "    tuples = []\n",
    "    \n",
    "    for i in range(N): #we want N different samples of the network\n",
    "        sys.stdout.write(f'\\r{i+1}/{N}')\n",
    "        \n",
    "        if edges:\n",
    "            wcc = remove_edges(g,fraction)\n",
    "        else:\n",
    "            wcc = remove_nodes(g,fraction)\n",
    "        \n",
    "        \n",
    "        clust_array_best, _, _ = dendrogram_and_jumpidx(wcc, hc_alg, min_val, perc=perc)#for each of the samples \n",
    "        #we run paris and obtain the dendrogram (Z0). We obtain the clust_array_best (ie the partition after the \n",
    "        #jump in modularity) \n",
    "        #we also get the jump_index to keep track of it\n",
    "        \n",
    "        #removes the nodes belonging to small communities (ie community size < 100)\n",
    "        \n",
    "        dic = remove_small_communities(wcc.nodes(),clust_array_best, thr=thr)\n",
    "        \n",
    "        nodes, labels = list(dic.keys()), list(dic.values())\n",
    "        \n",
    "        if i == 0:\n",
    "            ref_dict = dic\n",
    "        print(np.bincount(list(dic.values()))[1:], len(np.bincount(list(dic.values()))[1:]))\n",
    "        if len(np.bincount(list(dic.values()))[1:]) < 2:\n",
    "            print('continue')\n",
    "            continue\n",
    "\n",
    "        \n",
    "        df = common_nodes_label(ref_dict.keys(),dic.keys(),ref_dict.values(),dic.values())\n",
    "        \n",
    "        cm = confusion_matrix(df['d1'],df['d2'])\n",
    "        \n",
    "        changes = go_through_cm(cm)\n",
    "        labels_adj = lab_adjuster(labels,changes)\n",
    "        \n",
    "        #we add to tuples the tuple (nodes of the sampled network,best assignment to communities of these nodes)\n",
    "        tuples.append((nodes,labels_adj))\n",
    "        \n",
    "    return nodes_dictionary(tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given a network it samples the nwtwork, does Paris hierarchical clustering, assigns a label to the nodes \n",
    "#of the sample. Repeats this for N times and return a dictionary having keys: nodes' id and values: a list\n",
    "#containing the labels of the nodes. The length of the lists is going to be <= N.\n",
    "def partitions_dict5(g, fraction=2, edges=True, N=100, hc_alg = 'paris', min_val = .4, perc=0.1, thr=100):\n",
    "    \n",
    "    tuples = []\n",
    "    i=0\n",
    "    while i < N: #we want N different samples of the network\n",
    "        sys.stdout.write(f'\\r{i+1}/{N}')\n",
    "        \n",
    "        if edges:\n",
    "            wcc = remove_edges(g,fraction)\n",
    "        else:\n",
    "            wcc = remove_nodes(g,fraction)\n",
    "        \n",
    "        \n",
    "        clust_array_best, _, _ = dendrogram_and_jumpidx(wcc, hc_alg, min_val, perc=perc)#for each of the samples \n",
    "        #we run paris and obtain the dendrogram (Z0). We obtain the clust_array_best (ie the partition after the \n",
    "        #jump in modularity) \n",
    "        #we also get the jump_index to keep track of it\n",
    "        \n",
    "        #removes the nodes belonging to small communities (ie community size < 100)\n",
    "        \n",
    "        dic = remove_small_communities(wcc.nodes(),clust_array_best, thr=thr)\n",
    "        \n",
    "        nodes, labels = list(dic.keys()), list(dic.values())\n",
    "        \n",
    "        if i == 0:\n",
    "            ref_dict = dic\n",
    "        print(np.bincount(list(dic.values()))[1:], len(np.bincount(list(dic.values()))[1:]))\n",
    "        if len(np.bincount(list(dic.values()))[1:]) < 2:\n",
    "            print('continue')\n",
    "            continue\n",
    "\n",
    "        \n",
    "        df = common_nodes_label(ref_dict.keys(),dic.keys(),ref_dict.values(),dic.values())\n",
    "        \n",
    "        cm = confusion_matrix(df['d1'],df['d2'])\n",
    "        \n",
    "        changes = go_through_cm(cm)\n",
    "        labels_adj = lab_adjuster(labels,changes)\n",
    "        \n",
    "        #we add to tuples the tuple (nodes of the sampled network,best assignment to communities of these nodes)\n",
    "        tuples.append((nodes,labels_adj))\n",
    "        \n",
    "        i+=1\n",
    "        \n",
    "    return nodes_dictionary(tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict(dic, path=''):\n",
    "    with open(path, 'w') as csv_file:  \n",
    "        writer = csv.writer(csv_file)\n",
    "        for key, value in dic.items():\n",
    "            writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preCOVID network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Modularity jump percentage (perc) = 0.1\n",
    "part_dict0 = partitions_dict2(wcc0_w, N=100, perc=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('preCOVID network number of nodes:', wcc0_w.number_of_nodes())\n",
    "print('preCOVID network covered nodes:', len(part_dict0))\n",
    "print('preCOVID fraction of covered nodes:',len(part_dict0)/wcc0_w.number_of_nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## earlyCOVID network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "part_dict1 = partitions_dict2(wcc1_w, N=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('earlyCOVID network number of nodes:', wcc1_w.number_of_nodes())\n",
    "print('earlyCOVID network covered nodes:', len(part_dict1))\n",
    "print('earlyCOVID fraction of covered nodes:',len(part_dict1)/wcc1_w.number_of_nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preVAX network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Modularity jump percentage (perc) = 0.1\n",
    "part_dict2 = partitions_dict2(wcc2_w, N=10, perc=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('preVAX network number of nodes:', wcc2_w.number_of_nodes())\n",
    "print('preVAX network covered nodes:', len(part_dict2))\n",
    "print('preVAX fraction of covered nodes:',len(part_dict2)/wcc2_w.number_of_nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## earlyVAX network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "part_dict3 = partitions_dict2(wcc3_w, N=100, thr=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('earlyVAX network number of nodes:', wcc3_w.number_of_nodes())\n",
    "print('earlyVAX network covered nodes:', len(part_dict3))\n",
    "print('earlyVAX fraction of covered nodes:',len(part_dict3)/wcc3_w.number_of_nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAXdrive network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "part_dict4 = partitions_dict2(wcc4_w, N=100, thr=100, min_val=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('VAXdrive network number of nodes:', wcc4_w.number_of_nodes())\n",
    "print('VAXdrive network covered nodes:', len(part_dict4))\n",
    "print('VAXdrive fraction of covered nodes:',len(part_dict4)/wcc4_w.number_of_nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lateVAX network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "part_dict5 = partitions_dict5(wcc5_w, N=100, thr=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('lateVAX network number of nodes:', wcc5_w.number_of_nodes())\n",
    "print('lateVAX network covered nodes:', len(part_dict5))\n",
    "print('lateVAX fraction of covered nodes:',len(part_dict5)/wcc5_w.number_of_nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a list containing the number of times that each node was sampled (number of appereances of each node)\n",
    "def number_of_appearances(dic):\n",
    "    new_dict = {}\n",
    "    for k in dic.keys():\n",
    "        new_dict.setdefault(k,len(dic[k]))\n",
    "    return list(new_dict.values())\n",
    "\n",
    "def number_of_appearances2(dic):\n",
    "    new_dict = {}\n",
    "    for k in dic.keys():\n",
    "        f = dic[k].split(',')\n",
    "        new_dict.setdefault(k,len(f))\n",
    "    return list(new_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each node ia associated with a list of labels.\n",
    "#this function returns the list of the percentage most common label of each node\n",
    "#so if a node has a list like [1,1,3,1], the percentage associated will be 0.75\n",
    "def rmc(dic): #RMCA\n",
    "    new_dict = {}\n",
    "    for k in dic.keys():\n",
    "        e = len(dic[k])\n",
    "        f = dic[k][1:e-1]\n",
    "        \n",
    "        f = f.split(', ')\n",
    "        \n",
    "        c = collections.Counter(f)\n",
    "        x = c.most_common()[0][1]\n",
    "        \n",
    "        new_dict.setdefault(k,float(\"{0:.3f}\".format(x/len(f))))\n",
    "    return list(new_dict.values()), new_dict\n",
    "\n",
    "#returns a dict with keys->nodes and values-> most common community attributed to the node\n",
    "def most_common_label(dic):\n",
    "    new_dict = {}\n",
    "    for k in dic.keys():\n",
    "        c = collections.Counter(dic[k][1:-1].split(', '))\n",
    "        x = c.most_common()[0][0]\n",
    "        new_dict.setdefault(k,x)\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, sharey=True, sharex=True, figsize=(6,4))\n",
    "axs_l = axs.flatten()\n",
    "ax = 10\n",
    "tit= 12\n",
    "hist_bins = np.linspace(0.5, 1.0, 21)\n",
    "\n",
    "l, _ = rmc(part_dict0)\n",
    "axs_l[0].hist(l, hist_bins, color='grey')\n",
    "axs_l[0].set_yscale('log')\n",
    "axs_l[0].grid()\n",
    "\n",
    "l, _ = rmc(part_dict1)\n",
    "axs_l[1].hist(l, hist_bins, color='grey')\n",
    "axs_l[1].set_yscale('log')\n",
    "axs_l[1].grid()\n",
    "\n",
    "l, _ = rmc(part_dict2)\n",
    "axs_l[2].hist(l, hist_bins, color='grey')                   \n",
    "axs_l[2].set_yscale('log')\n",
    "axs_l[2].grid()\n",
    "\n",
    "l, _ = rmc(part_dict3)\n",
    "axs_l[3].grid()\n",
    "axs_l[3].hist(l, hist_bins, color='grey')\n",
    "axs_l[3].set_yscale('log')\n",
    "\n",
    "l, _ = rmc(part_dict4)\n",
    "axs_l[4].grid()\n",
    "axs_l[4].hist(l, hist_bins, color='grey')                                     \n",
    "axs_l[4].set_yscale('log')\n",
    "\n",
    "l, _ = rmc(part_dict5)\n",
    "axs_l[5].grid()\n",
    "axs_l[5].hist(l, hist_bins, color='grey')\n",
    "axs_l[5].set_yscale('log')\n",
    "\n",
    "axs_l[0].set_ylabel('number of nodes',fontsize=ax)\n",
    "axs_l[3].set_ylabel('number of nodes',fontsize=ax)\n",
    "\n",
    "for ax_idx in [3,4,5]:\n",
    "    axs_l[ax_idx].set_xlabel('RMCA',fontsize=ax)\n",
    "\n",
    "titles = ['i', 'ii', 'iii', 'iv', 'v', 'vi']\n",
    "for i in range(len(titles)):\n",
    "    axs_l[i].text(0.58, 1e4, titles[i], \n",
    "                 bbox=dict(boxstyle='round', facecolor='white', alpha=1.0))\n",
    "\n",
    "plt.ylim(10,)\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig('/figures/RMCA_distribution.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
