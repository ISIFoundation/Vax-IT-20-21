{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GHS (General hesitant supporters) \n",
    "## In this notebook:\n",
    "* We import the whole dataset and split it in 6 dataframes, one per time segment.\n",
    "* We import the sets of supporters and hesitant users for every time window and obtain the indices of the tweets from supporters and hesitant indices.\n",
    "* For every time window we run the following steps:\n",
    "    * we import the raw count matrix $C$, the vocabulary $cv$ (of length $|V|$), the fitted $tfidf$ and the fitted $NMF$ (that contains $W$ and $H$ as well).\n",
    "    * we transform $C$ with the $tfidf$ and obtain $X$.\n",
    "    * we extract supporters and hesitant tweets (rows) to form $X_P$ and $X_A$ ($|P|$ and $|A|$ are the respective number of supporters and hesitant tweets).\n",
    "    * we run topic discovery on both $X_P$ and $X_A$, to obtain $NMF_P$ and $NMF_A$ (that contain $W_P$, $H_P$ and $W_A$, $H_A$ as well).\n",
    "    * we transform $X_P$ with $NMF_A$ to obtain $W^P_A$ and we transform $X_A$ with $NMF_P$ to obtain $W^A_P$. Note that $W^P_A$ has shape $|P| \\times |V|$ and that $W^A_P$ has shape $|A| \\times |V|$.\n",
    "    * we define the hesitant strength of general topics as $S^A = W[hesitant].sumcols()$ and the supporters strength of general topics as $S^P = W[supporters].sumcols()$ and compute the hesitant share of **general** topics as $\\frac{|P| \\times S^A}{|P| \\times S^A + |A| \\times S^P}$.\n",
    "    * we define the hesitant strength of hesitant topics as $S_A^A = W_A.sumcols()$ and the supporters strength of hesitant topics as $S_A^P = W_A^P.sumcols()$ and compute the hesitant share of **hesitant** topics as $\\frac{|P| \\times S_A^A}{|P| \\times S_A^A + |A| \\times S_A^P}$.\n",
    "    * we define the hesitant strength of supporters topics as $S_P^A = W_P^A.sumcols()$ and the supporters strength of supporters topics as $S_P^P = W_P.sumcols()$ and compute the hesitant share of **supporters** topics as $\\frac{|P| \\times S_P^A}{|P| \\times S_P^A + |A| \\times S_P^P}$.\n",
    "    * in the end we have 60 topics per time window: 20 general topics, 20 hesitant and 20 supporters topics.\n",
    "    * we compute the strength of the topics like $NMF.transform(X).sumcols()$, $NMF_A.transform(X).sumcols()$, $NMF_P.transform(X).sumcols()$ where $X$ contains all the tweets of the time window.\n",
    "    * we save all these results to file in dataframes like: **topic number, topic words, strength ,hesitant share**.\n",
    "    \n",
    "* **NB: vaccine hesitant group is represented by an \"A\", whereas vaccine supporters group is represented by an \"P\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "import scipy.sparse\n",
    "from scipy.sparse import hstack, coo_matrix, vstack\n",
    "from sklearn import feature_extraction\n",
    "import joblib\n",
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import regex as re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_analyzer(text):\n",
    "    words = [w for w in token_pattern.findall(text.lower()) if w not in stop_words]\n",
    "    return bigram[words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#import the tweets_df\n",
    "tweets_df = pd.read_csv('/../data/tweets_example.csv').drop(['Unnamed: 0'],axis=1)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = tweets_df[['id','id_usr','id_usr_rt','created_at']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming 'created_at' fields into datetime objects and sorting tweets by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#to_datetime transforms strings containing dates into datetime objects. to_datetime returns a pd Series with indices\n",
    "#the same indices of the rt_id_df and with values datetime objects\n",
    "#So I first get two pd Series containing infos of 'created_at' and 'created_at_rt' fields\n",
    "cr_at_series = pd.to_datetime(tweets_df['created_at'], format = '%a %b %d %H:%M:%S +0000 %Y')\n",
    "\n",
    "#then I turn the two series into two temporary dataframes.\n",
    "temp_df1 = cr_at_series.to_frame()\n",
    "temp_df1.columns = ['created_at_datetime']\n",
    "\n",
    "#substituting the old string-form fields with new datetime-form fields\n",
    "tweets_df = tweets_df.drop(['created_at'],axis=1)\n",
    "\n",
    "tweets_df.insert(loc=2, column='created_at',value=temp_df1['created_at_datetime'],allow_duplicates=True)\n",
    "\n",
    "del temp_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_tweets_df = tweets_df.sort_values(by=['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sort_tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing self loops\n",
    "self_index = sort_tweets_df[sort_tweets_df['id_usr']==sort_tweets_df['id_usr_rt']].index\n",
    "sort_tweets_df = sort_tweets_df.drop(self_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sort_tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the indices of the sorted df so that they go from 0 to len(sort_tweets_df)-1\n",
    "ni = np.arange(len(sort_tweets_df)) #new indices     \n",
    "s = pd.Series(ni) #I turn my 'new indices' numpy array into a pandas series\n",
    "sort_tweets_df = sort_tweets_df.set_index([s]) #and use this series to change the indices of the dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#removing the 'id_usr_rt' field\n",
    "sort_tweets_df = sort_tweets_df.drop(['id_usr_rt'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating dataframes and importing NMFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the splitting dates\n",
    "sep_5th_19 = pd.Timestamp(2019,9,5)\n",
    "jan_1st_20 = pd.Timestamp(2020,1,1)\n",
    "mar_9th_20 = pd.Timestamp(2020,3,9)\n",
    "nov_1st_20 = pd.Timestamp(2020,11,1)\n",
    "apr_17_21 = pd.Timestamp(2021,4,17)\n",
    "aug_1st_21 = pd.Timestamp(2021,8,1)\n",
    "nov_8th_21 = pd.Timestamp(2021,11,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw0_df = sort_tweets_df[sort_tweets_df['created_at'] < jan_1st_20].reset_index()\n",
    "tw5_df = sort_tweets_df[sort_tweets_df['created_at'] >= aug_1st_21].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx1 = np.where((sort_tweets_df['created_at'] >= jan_1st_20) & (sort_tweets_df['created_at'] < mar_9th_20))\n",
    "idx2 = np.where((sort_tweets_df['created_at'] >= mar_9th_20) & (sort_tweets_df['created_at'] < nov_1st_20))\n",
    "idx3 = np.where((sort_tweets_df['created_at'] >= nov_1st_20) & (sort_tweets_df['created_at'] < apr_17_21))\n",
    "idx4 = np.where((sort_tweets_df['created_at'] >= apr_17_21) & (sort_tweets_df['created_at'] < aug_1st_21))\n",
    "\n",
    "tw1_df = sort_tweets_df.loc[idx1].reset_index()\n",
    "tw2_df = sort_tweets_df.loc[idx2].reset_index()\n",
    "tw3_df = sort_tweets_df.loc[idx3].reset_index()\n",
    "tw4_df = sort_tweets_df.loc[idx4].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing users dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n is a dataframe like: id_usr, RMC, community, time_window \n",
    "n = pd.read_csv('/../data/nodes_example_i_vi.csv',dtype=str).drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "n0 = n[n['time_window'] == 'i']\n",
    "n1 = n[n['time_window'] == 'ii']\n",
    "n2 = n[n['time_window'] == 'iii']\n",
    "n3 = n[n['time_window'] == 'iv']\n",
    "n4 = n[n['time_window'] == 'v']\n",
    "n5 = n[n['time_window'] == 'vi']\n",
    "\n",
    "#vaccine hesitant community i: 1\n",
    "#vaccine hesitant community ii: 3\n",
    "#vaccine hesitant community iii: 1\n",
    "#vaccine hesitant community iv: 1\n",
    "#vaccine hesitant community v: 1\n",
    "#vaccine hesitant community vi: 1\n",
    "\n",
    "av0 = set(n0[n0['community'] == '1'].id_usr)\n",
    "av1 = set(n1[n1['community'] == '3'].id_usr)\n",
    "av2 = set(n2[n2['community'] == '1'].id_usr)\n",
    "av3 = set(n3[n3['community'] == '1'].id_usr)\n",
    "av4 = set(n4[n4['community'] == '1'].id_usr)\n",
    "av5 = set(n5[n5['community'] == '1'].id_usr)\n",
    "\n",
    "\n",
    "#vaccine supporters community i: 3\n",
    "#vaccine supporters community ii: 1\n",
    "#vaccine supporters community iii: 2\n",
    "#vaccine supporters community iv: 2\n",
    "#vaccine supporters community v: 2\n",
    "#vaccine supporters community vi: 2\n",
    "\n",
    "\n",
    "pv0 = set(n0[n0['community'] == '3'].id_usr)\n",
    "pv1 = set(n1[n1['community'] == '1'].id_usr)\n",
    "pv2 = set(n2[n2['community'] == '2'].id_usr)\n",
    "pv3 = set(n3[n3['community'] == '2'].id_usr)\n",
    "pv4 = set(n4[n4['community'] == '2'].id_usr)\n",
    "pv5 = set(n5[n5['community'] == '2'].id_usr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw0_df = tw0_df.drop(['index'],axis=1).reset_index()\n",
    "tw1_df = tw1_df.drop(['index'],axis=1).reset_index()\n",
    "tw2_df = tw2_df.drop(['index'],axis=1).reset_index()\n",
    "tw3_df = tw3_df.drop(['index'],axis=1).reset_index()\n",
    "tw4_df = tw4_df.drop(['index'],axis=1).reset_index()\n",
    "tw5_df = tw5_df.drop(['index'],axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw0_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw5_df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#I want to store the indices of the tweets from supporters/hesitant users for every time window.\n",
    "#I store them in supporters_/hesitant_idx\n",
    "\n",
    "hesitant = [av0,av1,av2,av3,av4,av5]\n",
    "supporters = [pv0,pv1,pv2,pv3,pv4,pv5]\n",
    "dfs = [tw0_df,tw1_df,tw2_df,tw3_df,tw4_df,tw5_df]\n",
    "hesitant_idx, supporters_idx = [], []\n",
    "\n",
    "for av,pv,df in zip(hesitant,supporters,dfs):\n",
    "\n",
    "    m_av = pd.merge(df, av, on='id_usr')\n",
    "        \n",
    "    m_pv = pd.merge(df, pv, on='id_usr')\n",
    "\n",
    "    print('# hesitant tweets:',len(m_av),'# supporters tweets:',len(m_pv),'# total tweets:',len(df))\n",
    "\n",
    "    hesitant_idx.append(m_av['index'])\n",
    "    supporters_idx.append(m_pv['index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_share_df(cv, nmf, X, H, nr_topics, sa_norm):\n",
    "    #sa_norm is the vector containing the hesitant share of the topics\n",
    "    \n",
    "    feature_names = np.array(cv.get_feature_names()) #vocabulary\n",
    "    topic_strength = nmf.transform(X).sum(axis=0) #strength of the topics computed over the total dataset X\n",
    "    \n",
    "    topwords = np.array([])\n",
    "    for i in range(nr_topics):\n",
    "        topic_words = feature_names[np.argsort(H[i])[::-1][:10]]\n",
    "\n",
    "        s = ' '.join(topic_words) #string containing the top 10 words of the topic\n",
    "\n",
    "        topwords = np.append(topwords, s)\n",
    "    \n",
    "    #create a df like topic number, topic words, strength ,hesitant share\n",
    "    df = pd.concat([pd.Series(np.arange(nr_topics)),pd.Series(topwords), pd.Series(topic_strength),pd.Series(sa_norm)],axis=1)\n",
    "    df.columns = ['topic_index','topic_words','strength','hesitant_s']\n",
    "    df['strength'] = df['strength'].apply(lambda x: float(x))\n",
    "    return df.sort_values(by=['strength'],ascending=False)\n",
    "    \n",
    "\n",
    "#G: general\n",
    "#H: hesitant \n",
    "#S: supporters \n",
    "def GHS(C0, cv0, tfidf0, W0, H0, nmf0, tw_idx, time_window, nr_topics = 20):\n",
    "    \n",
    "    X0 = tfidf0.transform(C0) #general tfidf matrix\n",
    "    X0A = X0[hesitant_idx[tw_idx]] #tfidf matrix of the hesitant tweets only\n",
    "    X0P = X0[supporters_idx[tw_idx]] #tfidf matrix of the supporters tweets only\n",
    "    \n",
    "    print('Running hesitant Topic Discovery...') #topic discovery on the hesitant tweets\n",
    "    nmf0A = decomposition.NMF(nr_topics,\n",
    "                            beta_loss='frobenius', solver='cd',\n",
    "                            init='nndsvd', random_state=42)\n",
    "    W0A = nmf0A.fit_transform(X0A)\n",
    "    H0A = nmf0A.components_\n",
    "    clear_output()\n",
    "\n",
    "    print('Running supporters Topic Discovery...') #topic discovery on the supporters tweets\n",
    "    nmf0P = decomposition.NMF(nr_topics,\n",
    "                        beta_loss='frobenius', solver='cd',\n",
    "                        init='nndsvd', random_state=42)\n",
    "    W0P = nmf0P.fit_transform(X0P)\n",
    "    H0P = nmf0P.components_\n",
    "    clear_output()\n",
    "    \n",
    "    #cross W\n",
    "    W0A_p = nmf0A.transform(X0P) #supporters matrix transformed with the hesitant fitted nmf\n",
    "    W0P_a = nmf0P.transform(X0A) #hesitant matrix transformed with the supporters fitted nmf\n",
    "\n",
    "    \n",
    "    #ratio of nr_hesitant_tweets/nr_supporters_tweets\n",
    "    #r can be smaller or larger than 1\n",
    "    r_A = W0[hesitant_idx[tw_idx]].shape[0] \n",
    "    r_P = W0[supporters_idx[tw_idx]].shape[0] \n",
    "    \n",
    "    #print(r_A, r_P)\n",
    "    \n",
    "    #####################################|G|####################################\n",
    "\n",
    "    s0_a = W0[hesitant_idx[tw_idx]].sum(axis=0)*r_P #general strength of hesitant tweets \n",
    "    s0_p = W0[supporters_idx[tw_idx]].sum(axis=0)*r_A #general strength of supporters tweets\n",
    "    norm = s0_a + s0_p #general normalization\n",
    "    \n",
    "    #print(s0_a/norm)\n",
    "\n",
    "\n",
    "    df_G = topic_share_df(cv0, nmf0, X0, H0, nr_topics, s0_a/norm)\n",
    "    \n",
    "    \n",
    "    #####################################|H|####################################\n",
    "    s0A_a = W0A.sum(axis=0)*r_P #hesitant strength of hesitant tweets \n",
    "    s0A_p = W0A_p.sum(axis=0)*r_A #hesitant (normalized by r) strength of supporters tweets\n",
    "    norm = s0A_p + s0A_a #hesitant normalization\n",
    "\n",
    "\n",
    "    df_A = topic_share_df(cv0, nmf0A, X0, H0A, nr_topics, s0A_a/norm)\n",
    "    \n",
    "    \n",
    "    #####################################|S|####################################\n",
    "    s0P_a = W0P_a.sum(axis=0)*r_P #supporters strength of hesitant tweets \n",
    "    s0P_p = W0P.sum(axis=0)*r_A #supporters (normalized by r) strength of supporters tweets\n",
    "    norm = s0P_a + s0P_p #supporters normalization\n",
    "\n",
    "    \n",
    "    df_P = topic_share_df(cv0, nmf0P, X0, H0P, nr_topics, s0P_a/norm)\n",
    "    \n",
    "    return df_G, df_A, df_P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing NMFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the vocabularies and the NMFs\n",
    "[C0, cv0] = joblib.load('/../data/counts_vocabulary_i.joblib')\n",
    "[W0,H0,nmf0] = joblib.load('/../data/WHnmf_i.joblib')\n",
    "tfidf0 = joblib.load('/../data/tfidf_i.joblib')\n",
    "\n",
    "[C1, cv1] = joblib.load('/../data/counts_vocabulary_ii.joblib')\n",
    "[W1,H1,nmf1] = joblib.load('/../data/WHnmf_ii.joblib')\n",
    "tfidf1 = joblib.load('/../data/tfidf_ii.joblib')\n",
    "\n",
    "[C2, cv2] = joblib.load('/../data/counts_vocabulary_iii.joblib')\n",
    "[W2,H2,nmf2] = joblib.load('/../data/WHnmf_iii.joblib')\n",
    "tfidf2 = joblib.load('/../data/tfidf_iii.joblib')\n",
    "\n",
    "[C3, cv3] = joblib.load('/../data/counts_vocabulary_iv.joblib')\n",
    "[W3,H3,nmf3] = joblib.load('/../data/WHnmf_iv.joblib')\n",
    "tfidf3 = joblib.load('/../data/tfidf_iv.joblib')\n",
    "\n",
    "[C4, cv4] = joblib.load('/../data/counts_vocabulary_v.joblib')\n",
    "[W4,H4,nmf4] = joblib.load('/../data/WHnmf_v.joblib')\n",
    "tfidf4 = joblib.load('/../data/tfidf_v.joblib')\n",
    "\n",
    "[C5, cv5] = joblib.load('/../data/counts_vocabulary_vi.joblib')\n",
    "[W5,H5,nmf5] = joblib.load('/../data/WHnmf_vi.joblib')\n",
    "tfidf5 = joblib.load('/../data/tfidf_vi.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preCOVID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "g0,h0,s0 = GHS(C0, cv0, tfidf0, W0, H0, nmf0, tw_idx=0, time_window='preCOVID',save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g0.to_csv('/../data/general_i.csv')\n",
    "h0.to_csv('/../data/hesitant_i.csv')\n",
    "s0.to_csv('/../data/supporters_i.csv')\n",
    "\n",
    "del g0, h0, s0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## earlyCOVID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "g1,h1,s1 = GHS(C1, cv1, tfidf1, W1, H1, nmf1, tw_idx=1, time_window='earlyCOVID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1.to_csv('/../data/general_ii.csv')\n",
    "h1.to_csv('/../data/hesitant_ii.csv')\n",
    "s1.to_csv('/../data/supporters_ii.csv')\n",
    "\n",
    "del g1, h1, s1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preVAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "g2,h2,s2 = GHS(C2, cv2, tfidf2, W2, H2, nmf2, tw_idx=2, time_window='preVAX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2.to_csv('/../data/general_iii.csv')\n",
    "h2.to_csv('/../data/hesitant_iii.csv')\n",
    "s2.to_csv('/../data/supporters_iii.csv')\n",
    "\n",
    "del g2, h2, s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## earlyVAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "g3,h3,s3 = GHS(C3, cv3, tfidf3, W3, H3, nmf3, tw_idx=3, time_window='earlyVAX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g3.to_csv('/../data/general_iv.csv')\n",
    "h3.to_csv('/../data/hesitant_iv.csv')\n",
    "s3.to_csv('/../data/supporters_iv.csv')\n",
    "\n",
    "del g3, h3, s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAXdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "g4,h4,s4 = GHS(C4, cv4, tfidf4, W4, H4, nmf4, tw_idx=4, time_window='VAXdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g4.to_csv('/../data/general_v.csv')\n",
    "h4.to_csv('/../data/hesitant_v.csv')\n",
    "s4.to_csv('/../data/supporters_v.csv')\n",
    "\n",
    "del g4, h4, s4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lateVAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "g5,h5,s5 = GHS(C5, cv5, tfidf5, W5, H5, nmf5, tw_idx=5, time_window='lateVAX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g5.to_csv('/../data/general_vi.csv')\n",
    "h5.to_csv('/../data/hesitant_vi.csv')\n",
    "s5.to_csv('/../data/supporters_vi.csv')\n",
    "\n",
    "del g5, h5, s5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
