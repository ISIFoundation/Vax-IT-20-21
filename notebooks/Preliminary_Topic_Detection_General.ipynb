{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Topic Detection General \n",
    "## In this notebook:\n",
    "* We import and manipulate the tweets dataframe by adding the master text and sorting tweets by date;\n",
    "* We clean the text and add the respective field to the dataframe (we do not remove hashtags and mentions);\n",
    "* For each time window:\n",
    "    * We build the vocabulary by fitting the count vectorizer on the clean_text field (WITHOUT TEXT DUPLICATES). We also include bigrams in the vocabulary;\n",
    "    * We obtain the Tweet-Term-Matrix (C2) with all the tweets (not just the ones with unique piece of text) and we save it to file;\n",
    "    * We save the count vectorizer (vocabulary) to file;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "import scipy.sparse\n",
    "from scipy.sparse import hstack, coo_matrix, vstack\n",
    "from sklearn import feature_extraction\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import regex as re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phraser, Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#import the tweets_df\n",
    "\n",
    "tweets_df = pd.read_csv('/home/gcrupi/6_time_windows/github/tweets_example.csv').drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming 'created_at' fields into datetime objects and sorting tweets by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#to_datetime transforms strings containing dates into datetime objects. to_datetime returns a pd Series with indices\n",
    "#the same indices of the rt_id_df and with values datetime objects\n",
    "#So I first get two pd Series containing infos of 'created_at' and 'created_at_rt' fields\n",
    "cr_at_series = pd.to_datetime(tweets_df['created_at'], format = '%a %b %d %H:%M:%S +0000 %Y')\n",
    "\n",
    "#then I turn the two series into two temporary dataframes.\n",
    "temp_df1 = cr_at_series.to_frame()\n",
    "temp_df1.columns = ['created_at_datetime']\n",
    "\n",
    "#substituting the old string-form fields with new datetime-form fields\n",
    "tweets_df = tweets_df.drop(['created_at'],axis=1)\n",
    "\n",
    "tweets_df.insert(loc=2, column='created_at',value=temp_df1['created_at_datetime'],allow_duplicates=True)\n",
    "\n",
    "del temp_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_tweets_df = tweets_df.sort_values(by=['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the splitting dates\n",
    "sep_5th_19 = pd.Timestamp(2019,9,5)\n",
    "jan_1st_20 = pd.Timestamp(2020,1,1)\n",
    "mar_9th_20 = pd.Timestamp(2020,3,9)\n",
    "nov_1st_20 = pd.Timestamp(2020,11,1)\n",
    "apr_17_21 = pd.Timestamp(2021,4,17)\n",
    "aug_1st_21 = pd.Timestamp(2021,8,1)\n",
    "nov_8th_21 = pd.Timestamp(2021,11,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sort_tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing self loops\n",
    "self_index = sort_tweets_df[sort_tweets_df['id_usr']==sort_tweets_df['id_usr_rt']].index\n",
    "sort_tweets_df = sort_tweets_df.drop(self_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sort_tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the indices of the sorted df so that they go from 0 to len(sort_tweets_df)-1\n",
    "ni = np.arange(len(sort_tweets_df)) #new indices     \n",
    "s = pd.Series(ni) #I turn my 'new indices' numpy array into a pandas series\n",
    "sort_tweets_df = sort_tweets_df.set_index([s]) #and use this series to change the indices of the dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#removing the 'id_usr_rt' field\n",
    "sort_tweets_df = sort_tweets_df.drop(['id_usr_rt'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopwords.words('italian') #'forse', 'qualche', 'qualcosa', 'chissà', 'po', 'stata', 'fatta', 'fatto', 'alcuni', \n",
    "#'quasi', 'oltre', 'fate', 'to', 'farne', 'far', 'ecco', 'però', 'sì', 'circa', 'state', 'ok', 'magari', 'so', \n",
    "#'ieri', 'oggi', 'stare', 'perchè', 'eh', 'ah', 'vabbè', 'ce', 'fra', 'proprio', 'te', 'pensa', 'vuoi', 'sai', \n",
    "#'puoi', 'devi', 'vai', 'fatti', 'guarda', 'dico', 'sa', 'sti', 'allora', 'tutte','altre', 'comunque', 'avere', 'deve'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('italian')\n",
    "en_stop = stopwords.words('english')\n",
    "query = ['vacc', 'vaccinale', 'vaccinali', 'vaccinano', 'vaccinarci', 'vaccinare', 'vaccinarsi',\n",
    "               'vaccinate', 'vaccinati', 'vaccinato', 'vaccinaz', 'vaccinazione', 'vaccinazioni', 'vaccines','vax','vaccine',\n",
    "               'vaccini', 'vaccinista', 'vaccinisti', 'vaccino', 'antivaccinisti', 'freevax', 'iovaccino', \n",
    "               'nonvaccinato', 'novax', 'obbligovaccinale', 'provax', 'ridacciilvaccino','vaccine']\n",
    "\n",
    "re_url = re.compile(r'https?:\\/\\/.*[\\r\\n]*', flags=re.U)\n",
    "#re_rtw = re.compile(r'RT', flags=re.U)\n",
    "re_htg = re.compile(r'#', flags=re.U) # remove hashtag sign\n",
    "#re_htg = re.compile(r'#[\\w]+ ?', flags=re.U)   # remove hashtags\n",
    "re_hnd = re.compile(r'@', flags=re.U)\n",
    "#re_hnd = re.compile(r'@\\w+ ?', flags=re.U)\n",
    "re_wrd = re.compile(r'[^\\w]+ ', flags=re.U)\n",
    "re_num = re.compile(r'[0-9]+', flags=re.U)\n",
    "\n",
    "def cleantext(txt):\n",
    "    t = txt\n",
    "    t = re_url.sub('', t)\n",
    "    #t = re_htg.sub('', t)\n",
    "    #t = re_rtw.sub(' ', t)\n",
    "    t = re_hnd.sub(' ', t)\n",
    "    t = re_wrd.sub(' ', t)\n",
    "    t = re_num.sub(' ', t)\n",
    "    return t.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining stop words\n",
    "more_stop = ['già','poi','solo','no','fa','può','quindi','quando','x','ogni','altro','così','mai','tutta','ancora',\n",
    "            'ora', 'molto','d', 'via','sempre','rt','co','https','dopo','fare','fatto','italia','essere','cosa',\n",
    "            'oggi','bene','dire','dice','vuole','vaccinati','vaccino','vaccini','vaccinato','senza','altri','me',\n",
    "             'detto','meno','invece','va','grazie']\n",
    "            \n",
    "more_more = ['forse', 'qualche', 'qualcosa', 'chissà', 'po', 'stata', 'fatta', 'fatto', 'alcuni', \n",
    "            'quasi', 'oltre', 'fate', 'to', 'farne', 'far', 'ecco', 'però', 'sì', 'circa', 'state', 'ok', 'magari', 'so', \n",
    "            'ieri', 'oggi', 'stare', 'perchè', 'eh', 'ah', 'vabbè', 'ce', 'fra', 'proprio', 'te', 'pensa', 'vuoi', 'sai', \n",
    "            'puoi', 'devi', 'vai', 'fatti', 'guarda', 'dico', 'sa', 'sti', 'allora', 'tutte','altre', 'comunque', 'avere', 'deve']\n",
    "\n",
    "stop_words = set(stop+query+en_stop+more_stop+more_more)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-COVID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw0_df = sort_tweets_df[sort_tweets_df['created_at'] < jan_1st_20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw0_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#cleaning the text and adding a 'clean_text' field\n",
    "tw0_df['clean_text'] = tw0_df['master_text'].apply(lambda txt: cleantext(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tw0_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tw0_df), len(tw0_df.drop_duplicates(subset=['master_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phraser\n",
    "In this section we add bigrams to the vocabulary: namely couple of words which often go together (i.e. \"green_pass\" or \"new_york\" etc...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_analyzer(text):\n",
    "    words = [w for w in token_pattern.findall(text.lower()) if w not in stop_words]\n",
    "    return bigram[words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "token_pattern = re.compile(r'(?u)\\b[A-Za-z]\\w+\\b')\n",
    "text_sentences = []\n",
    "#building the dictionary with unique pieces of text (in other words dropping duplicated on the clean_text column)\n",
    "for doc in tw0_df.clean_text.drop_duplicates():\n",
    "    text_sentences.extend([token_pattern.findall(sent.lower()) for sent in doc.split('\\n') if len(sent) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#min_count is the minimal nuimber of times that a single bigram has to appear in order to be considered a real bigram\n",
    "#threshold is linked to the probability of observing the words of the bigram together and the probability of \n",
    "#observing them separately\n",
    "phrases = Phrases(text_sentences, min_count=10, threshold=20., common_terms=stop_words) #, scoring='npmi')\n",
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the count vectorizer on the clean_text field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "cv2 = feature_extraction.text.CountVectorizer(min_df=10, max_df=0.5, stop_words=stop_words, analyzer=phrase_analyzer)\n",
    "#building the vocabulary with unique pieces of text (in other words dropping duplicates in the clean_text column)\n",
    "cv2.fit(tw0_df.clean_text.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#here I just obtain the matrix of counts of all the tweets, but with the vocabulary built with the unique \n",
    "#pieces of text only\n",
    "C2 = cv2.transform(tw0_df.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tw0_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the raw count matrix C2 and the vocabulary\n",
    "\n",
    "joblib.dump([C2,cv2], '/../data/counts_vocabulary_i.joblib', compress=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# early-COVID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.where((sort_tweets_df['created_at'] >= jan_1st_20) & (sort_tweets_df['created_at'] < mar_9th_20))\n",
    "\n",
    "tw1_df = sort_tweets_df.loc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw1_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#cleaning the text and adding a 'clean_text' field\n",
    "tw1_df['clean_text'] = tw1_df['master_text'].apply(lambda txt: cleantext(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tw1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tw1_df), len(tw1_df.drop_duplicates(subset=['master_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phraser\n",
    "In this section we add bigrams to the vocabulary: namely couple of words which often go together (i.e. \"green_pass\" or \"new_york\" etc...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "token_pattern = re.compile(r'(?u)\\b[A-Za-z]\\w+\\b')\n",
    "text_sentences = []\n",
    "#building the dictionary with unique pieces of text (in other words dropping duplicated on the clean_text column)\n",
    "for doc in tw1_df.clean_text.drop_duplicates():\n",
    "    text_sentences.extend([token_pattern.findall(sent.lower()) for sent in doc.split('\\n') if len(sent) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#min_count is the minimal nuimber of times that a single bigram has to appear in order to be considered a real bigram\n",
    "#threshold is linked to the probability of observing the words of the bigram together and the probability of \n",
    "#observing them separately\n",
    "phrases = Phrases(text_sentences, min_count=10, threshold=20., common_terms=stop_words) #, scoring='npmi')\n",
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the count vectorizer on the clean_text field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "cv2 = feature_extraction.text.CountVectorizer(min_df=10, max_df=0.5, stop_words=stop_words, analyzer=phrase_analyzer)\n",
    "#building the vocabulary with unique pieces of text (in other words dropping duplicates in the clean_text column)\n",
    "cv2.fit(tw1_df.clean_text.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#here I just obtain the matrix of counts of all the tweets, but with the vocabulary built with the unique \n",
    "#pieces of text only\n",
    "C2 = cv2.transform(tw1_df.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tw1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the raw count matrix C2 and the vocabulary\n",
    "\n",
    "joblib.dump([C2,cv2], '/../data/counts_vocabulary_ii.joblib', compress=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-VAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.where((sort_tweets_df['created_at'] >= mar_9th_20) & (sort_tweets_df['created_at'] < nov_1st_20))\n",
    "\n",
    "tw2_df = sort_tweets_df.loc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw2_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#cleaning the text and adding a 'clean_text' field\n",
    "tw2_df['clean_text'] = tw2_df['master_text'].apply(lambda txt: cleantext(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tw2_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tw2_df), len(tw2_df.drop_duplicates(subset=['master_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phraser\n",
    "In this section we add bigrams to the vocabulary: namely couple of words which often go together (i.e. \"green_pass\" or \"new_york\" etc...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "token_pattern = re.compile(r'(?u)\\b[A-Za-z]\\w+\\b')\n",
    "text_sentences = []\n",
    "#building the dictionary with unique pieces of text (in other words dropping duplicated on the clean_text column)\n",
    "for doc in tw2_df.clean_text.drop_duplicates():\n",
    "    text_sentences.extend([token_pattern.findall(sent.lower()) for sent in doc.split('\\n') if len(sent) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#min_count is the minimal nuimber of times that a single bigram has to appear in order to be considered a real bigram\n",
    "#threshold is linked to the probability of observing the words of the bigram together and the probability of \n",
    "#observing them separately\n",
    "phrases = Phrases(text_sentences, min_count=10, threshold=20., common_terms=stop_words) #, scoring='npmi')\n",
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the count vectorizer on the clean_text field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "cv2 = feature_extraction.text.CountVectorizer(min_df=10, max_df=0.5, stop_words=stop_words, analyzer=phrase_analyzer)\n",
    "#building the vocabulary with unique pieces of text (in other words dropping duplicates in the clean_text column)\n",
    "cv2.fit(tw2_df.clean_text.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#here I just obtain the matrix of counts of all the tweets, but with the vocabulary built with the unique \n",
    "#pieces of text only\n",
    "C2 = cv2.transform(tw2_df.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tw2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the raw count matrix C2 and the vocabulary\n",
    "\n",
    "joblib.dump([C2,cv2], '/../data/counts_vocabulary_iii.joblib', compress=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# early-VAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.where((sort_tweets_df['created_at'] >= nov_1st_20) & (sort_tweets_df['created_at'] < apr_17_21))\n",
    "\n",
    "tw3_df = sort_tweets_df.loc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw3_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#cleaning the text and adding a 'clean_text' field\n",
    "tw3_df['clean_text'] = tw3_df['master_text'].apply(lambda txt: cleantext(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tw3_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tw3_df), len(tw3_df.drop_duplicates(subset=['master_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phraser\n",
    "In this section we add bigrams to the vocabulary: namely couple of words which often go together (i.e. \"green_pass\" or \"new_york\" etc...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "token_pattern = re.compile(r'(?u)\\b[A-Za-z]\\w+\\b')\n",
    "text_sentences = []\n",
    "#building the dictionary with unique pieces of text (in other words dropping duplicated on the clean_text column)\n",
    "for doc in tw3_df.clean_text.drop_duplicates():\n",
    "    text_sentences.extend([token_pattern.findall(sent.lower()) for sent in doc.split('\\n') if len(sent) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#min_count is the minimal nuimber of times that a single bigram has to appear in order to be considered a real bigram\n",
    "#threshold is linked to the probability of observing the words of the bigram together and the probability of \n",
    "#observing them separately\n",
    "phrases = Phrases(text_sentences, min_count=10, threshold=20., common_terms=stop_words) #, scoring='npmi')\n",
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the count vectorizer on the clean_text field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "cv2 = feature_extraction.text.CountVectorizer(min_df=10, max_df=0.5, stop_words=stop_words, analyzer=phrase_analyzer)\n",
    "#building the vocabulary with unique pieces of text (in other words dropping duplicates in the clean_text column)\n",
    "cv2.fit(tw3_df.clean_text.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#here I just obtain the matrix of counts of all the tweets, but with the vocabulary built with the unique \n",
    "#pieces of text only\n",
    "C2 = cv2.transform(tw3_df.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tw3_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the raw count matrix C2 and the vocabulary\n",
    "\n",
    "joblib.dump([C2,cv2], '/../data/counts_vocabulary_iv.joblib', compress=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAX-drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.where((sort_tweets_df['created_at'] >= apr_17_21) & (sort_tweets_df['created_at'] < aug_1st_21))\n",
    "\n",
    "tw4_df = sort_tweets_df.loc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw4_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#cleaning the text and adding a 'clean_text' field\n",
    "tw4_df['clean_text'] = tw4_df['master_text'].apply(lambda txt: cleantext(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tw4_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tw4_df), len(tw4_df.drop_duplicates(subset=['master_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phraser\n",
    "In this section we add bigrams to the vocabulary: namely couple of words which often go together (i.e. \"green_pass\" or \"new_york\" etc...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "token_pattern = re.compile(r'(?u)\\b[A-Za-z]\\w+\\b')\n",
    "text_sentences = []\n",
    "#building the dictionary with unique pieces of text (in other words dropping duplicated on the clean_text column)\n",
    "for doc in tw4_df.clean_text.drop_duplicates():\n",
    "    text_sentences.extend([token_pattern.findall(sent.lower()) for sent in doc.split('\\n') if len(sent) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#min_count is the minimal nuimber of times that a single bigram has to appear in order to be considered a real bigram\n",
    "#threshold is linked to the probability of observing the words of the bigram together and the probability of \n",
    "#observing them separately\n",
    "phrases = Phrases(text_sentences, min_count=10, threshold=20., common_terms=stop_words) #, scoring='npmi')\n",
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the count vectorizer on the clean_text field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "cv2 = feature_extraction.text.CountVectorizer(min_df=10, max_df=0.5, stop_words=stop_words, analyzer=phrase_analyzer)\n",
    "#building the vocabulary with unique pieces of text (in other words dropping duplicates in the clean_text column)\n",
    "cv2.fit(tw4_df.clean_text.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#here I just obtain the matrix of counts of all the tweets, but with the vocabulary built with the unique \n",
    "#pieces of text only\n",
    "C2 = cv2.transform(tw4_df.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tw4_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the raw count matrix C2 and the vocabulary\n",
    "\n",
    "joblib.dump([C2,cv2], '/../data/counts_vocabulary_v.joblib', compress=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# late-VAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw5_df = sort_tweets_df[sort_tweets_df['created_at'] >= aug_1st_21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw5_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#cleaning the text and adding a 'clean_text' field\n",
    "tw5_df['clean_text'] = tw5_df['master_text'].apply(lambda txt: cleantext(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tw5_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tw5_df), len(tw5_df.drop_duplicates(subset=['master_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phraser\n",
    "In this section we add bigrams to the vocabulary: namely couple of words which often go together (i.e. \"green_pass\" or \"new_york\" etc...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "token_pattern = re.compile(r'(?u)\\b[A-Za-z]\\w+\\b')\n",
    "text_sentences = []\n",
    "#building the dictionary with unique pieces of text (in other words dropping duplicated on the clean_text column)\n",
    "for doc in tw5_df.clean_text.drop_duplicates():\n",
    "    text_sentences.extend([token_pattern.findall(sent.lower()) for sent in doc.split('\\n') if len(sent) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#min_count is the minimal nuimber of times that a single bigram has to appear in order to be considered a real bigram\n",
    "#threshold is linked to the probability of observing the words of the bigram together and the probability of \n",
    "#observing them separately\n",
    "phrases = Phrases(text_sentences, min_count=10, threshold=20., common_terms=stop_words) #, scoring='npmi')\n",
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the count vectorizer on the clean_text field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "cv2 = feature_extraction.text.CountVectorizer(min_df=10, max_df=0.5, stop_words=stop_words, analyzer=phrase_analyzer)\n",
    "#building the vocabulary with unique pieces of text (in other words dropping duplicates in the clean_text column)\n",
    "cv2.fit(tw5_df.clean_text.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#here I just obtain the matrix of counts of all the tweets, but with the vocabulary built with the unique \n",
    "#pieces of text only\n",
    "C2 = cv2.transform(tw5_df.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tw5_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the raw count matrix C2 and the vocabulary\n",
    "\n",
    "joblib.dump([C2,cv2], '/../data/counts_vocabulary_vi.joblib', compress=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
