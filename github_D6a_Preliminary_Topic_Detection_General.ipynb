{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Topic Detection General \n",
    "## In this notebook:\n",
    "* We import and manipulate the tweets dataframe by adding the master text and sorting tweets by date;\n",
    "* We clean the text and add the respective field to the dataframe (we do not remove hashtags and mentions);\n",
    "* For each time window:\n",
    "    * We build the vocabulary by fitting the count vectorizer on the clean_text field (WITHOUT TEXT DUPLICATES). We also include bigrams in the vocabulary;\n",
    "    * We obtain the Tweet-Term-Matrix (C2) with all the tweets (not just the ones with unique piece of text) and we save it to file;\n",
    "    * We save the count vectorizer (vocabulary) to file;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "import scipy.sparse\n",
    "from scipy.sparse import hstack, coo_matrix, vstack\n",
    "from sklearn import feature_extraction\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import regex as re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phraser, Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 36.7 ms, total: 36.7 ms\n",
      "Wall time: 581 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#import the tweets_df\n",
    "\n",
    "tweets_df = pd.read_csv('/home/gcrupi/6_time_windows/github/tweets_example.csv').drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             100\n",
       "id_usr         100\n",
       "id_usr_rt       55\n",
       "created_at     100\n",
       "master_text    100\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming 'created_at' fields into datetime objects and sorting tweets by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#to_datetime transforms strings containing dates into datetime objects. to_datetime returns a pd Series with indices\n",
    "#the same indices of the rt_id_df and with values datetime objects\n",
    "#So I first get two pd Series containing infos of 'created_at' and 'created_at_rt' fields\n",
    "cr_at_series = pd.to_datetime(tweets_df['created_at'], format = '%a %b %d %H:%M:%S +0000 %Y')\n",
    "\n",
    "#then I turn the two series into two temporary dataframes.\n",
    "temp_df1 = cr_at_series.to_frame()\n",
    "temp_df1.columns = ['created_at_datetime']\n",
    "\n",
    "#substituting the old string-form fields with new datetime-form fields\n",
    "tweets_df = tweets_df.drop(['created_at'],axis=1)\n",
    "\n",
    "tweets_df.insert(loc=2, column='created_at',value=temp_df1['created_at_datetime'],allow_duplicates=True)\n",
    "\n",
    "del temp_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_tweets_df = tweets_df.sort_values(by=['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the splitting dates\n",
    "sep_5th_19 = pd.Timestamp(2019,9,5)\n",
    "jan_1st_20 = pd.Timestamp(2020,1,1)\n",
    "mar_9th_20 = pd.Timestamp(2020,3,9)\n",
    "nov_1st_20 = pd.Timestamp(2020,11,1)\n",
    "apr_17_21 = pd.Timestamp(2021,4,17)\n",
    "aug_1st_21 = pd.Timestamp(2021,8,1)\n",
    "nov_8th_21 = pd.Timestamp(2021,11,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sort_tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing self loops\n",
    "self_index = sort_tweets_df[sort_tweets_df['id_usr']==sort_tweets_df['id_usr_rt']].index\n",
    "sort_tweets_df = sort_tweets_df.drop(self_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sort_tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the indices of the sorted df so that they go from 0 to len(sort_tweets_df)-1\n",
    "ni = np.arange(len(sort_tweets_df)) #new indices     \n",
    "s = pd.Series(ni) #I turn my 'new indices' numpy array into a pandas series\n",
    "sort_tweets_df = sort_tweets_df.set_index([s]) #and use this series to change the indices of the dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#removing the 'id_usr_rt' field\n",
    "sort_tweets_df = sort_tweets_df.drop(['id_usr_rt'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ad',\n",
       " 'al',\n",
       " 'allo',\n",
       " 'ai',\n",
       " 'agli',\n",
       " 'all',\n",
       " 'agl',\n",
       " 'alla',\n",
       " 'alle',\n",
       " 'con',\n",
       " 'col',\n",
       " 'coi',\n",
       " 'da',\n",
       " 'dal',\n",
       " 'dallo',\n",
       " 'dai',\n",
       " 'dagli',\n",
       " 'dall',\n",
       " 'dagl',\n",
       " 'dalla',\n",
       " 'dalle',\n",
       " 'di',\n",
       " 'del',\n",
       " 'dello',\n",
       " 'dei',\n",
       " 'degli',\n",
       " 'dell',\n",
       " 'degl',\n",
       " 'della',\n",
       " 'delle',\n",
       " 'in',\n",
       " 'nel',\n",
       " 'nello',\n",
       " 'nei',\n",
       " 'negli',\n",
       " 'nell',\n",
       " 'negl',\n",
       " 'nella',\n",
       " 'nelle',\n",
       " 'su',\n",
       " 'sul',\n",
       " 'sullo',\n",
       " 'sui',\n",
       " 'sugli',\n",
       " 'sull',\n",
       " 'sugl',\n",
       " 'sulla',\n",
       " 'sulle',\n",
       " 'per',\n",
       " 'tra',\n",
       " 'contro',\n",
       " 'io',\n",
       " 'tu',\n",
       " 'lui',\n",
       " 'lei',\n",
       " 'noi',\n",
       " 'voi',\n",
       " 'loro',\n",
       " 'mio',\n",
       " 'mia',\n",
       " 'miei',\n",
       " 'mie',\n",
       " 'tuo',\n",
       " 'tua',\n",
       " 'tuoi',\n",
       " 'tue',\n",
       " 'suo',\n",
       " 'sua',\n",
       " 'suoi',\n",
       " 'sue',\n",
       " 'nostro',\n",
       " 'nostra',\n",
       " 'nostri',\n",
       " 'nostre',\n",
       " 'vostro',\n",
       " 'vostra',\n",
       " 'vostri',\n",
       " 'vostre',\n",
       " 'mi',\n",
       " 'ti',\n",
       " 'ci',\n",
       " 'vi',\n",
       " 'lo',\n",
       " 'la',\n",
       " 'li',\n",
       " 'le',\n",
       " 'gli',\n",
       " 'ne',\n",
       " 'il',\n",
       " 'un',\n",
       " 'uno',\n",
       " 'una',\n",
       " 'ma',\n",
       " 'ed',\n",
       " 'se',\n",
       " 'perch√©',\n",
       " 'anche',\n",
       " 'come',\n",
       " 'dov',\n",
       " 'dove',\n",
       " 'che',\n",
       " 'chi',\n",
       " 'cui',\n",
       " 'non',\n",
       " 'pi√π',\n",
       " 'quale',\n",
       " 'quanto',\n",
       " 'quanti',\n",
       " 'quanta',\n",
       " 'quante',\n",
       " 'quello',\n",
       " 'quelli',\n",
       " 'quella',\n",
       " 'quelle',\n",
       " 'questo',\n",
       " 'questi',\n",
       " 'questa',\n",
       " 'queste',\n",
       " 'si',\n",
       " 'tutto',\n",
       " 'tutti',\n",
       " 'a',\n",
       " 'c',\n",
       " 'e',\n",
       " 'i',\n",
       " 'l',\n",
       " 'o',\n",
       " 'ho',\n",
       " 'hai',\n",
       " 'ha',\n",
       " 'abbiamo',\n",
       " 'avete',\n",
       " 'hanno',\n",
       " 'abbia',\n",
       " 'abbiate',\n",
       " 'abbiano',\n",
       " 'avr√≤',\n",
       " 'avrai',\n",
       " 'avr√†',\n",
       " 'avremo',\n",
       " 'avrete',\n",
       " 'avranno',\n",
       " 'avrei',\n",
       " 'avresti',\n",
       " 'avrebbe',\n",
       " 'avremmo',\n",
       " 'avreste',\n",
       " 'avrebbero',\n",
       " 'avevo',\n",
       " 'avevi',\n",
       " 'aveva',\n",
       " 'avevamo',\n",
       " 'avevate',\n",
       " 'avevano',\n",
       " 'ebbi',\n",
       " 'avesti',\n",
       " 'ebbe',\n",
       " 'avemmo',\n",
       " 'aveste',\n",
       " 'ebbero',\n",
       " 'avessi',\n",
       " 'avesse',\n",
       " 'avessimo',\n",
       " 'avessero',\n",
       " 'avendo',\n",
       " 'avuto',\n",
       " 'avuta',\n",
       " 'avuti',\n",
       " 'avute',\n",
       " 'sono',\n",
       " 'sei',\n",
       " '√®',\n",
       " 'siamo',\n",
       " 'siete',\n",
       " 'sia',\n",
       " 'siate',\n",
       " 'siano',\n",
       " 'sar√≤',\n",
       " 'sarai',\n",
       " 'sar√†',\n",
       " 'saremo',\n",
       " 'sarete',\n",
       " 'saranno',\n",
       " 'sarei',\n",
       " 'saresti',\n",
       " 'sarebbe',\n",
       " 'saremmo',\n",
       " 'sareste',\n",
       " 'sarebbero',\n",
       " 'ero',\n",
       " 'eri',\n",
       " 'era',\n",
       " 'eravamo',\n",
       " 'eravate',\n",
       " 'erano',\n",
       " 'fui',\n",
       " 'fosti',\n",
       " 'fu',\n",
       " 'fummo',\n",
       " 'foste',\n",
       " 'furono',\n",
       " 'fossi',\n",
       " 'fosse',\n",
       " 'fossimo',\n",
       " 'fossero',\n",
       " 'essendo',\n",
       " 'faccio',\n",
       " 'fai',\n",
       " 'facciamo',\n",
       " 'fanno',\n",
       " 'faccia',\n",
       " 'facciate',\n",
       " 'facciano',\n",
       " 'far√≤',\n",
       " 'farai',\n",
       " 'far√†',\n",
       " 'faremo',\n",
       " 'farete',\n",
       " 'faranno',\n",
       " 'farei',\n",
       " 'faresti',\n",
       " 'farebbe',\n",
       " 'faremmo',\n",
       " 'fareste',\n",
       " 'farebbero',\n",
       " 'facevo',\n",
       " 'facevi',\n",
       " 'faceva',\n",
       " 'facevamo',\n",
       " 'facevate',\n",
       " 'facevano',\n",
       " 'feci',\n",
       " 'facesti',\n",
       " 'fece',\n",
       " 'facemmo',\n",
       " 'faceste',\n",
       " 'fecero',\n",
       " 'facessi',\n",
       " 'facesse',\n",
       " 'facessimo',\n",
       " 'facessero',\n",
       " 'facendo',\n",
       " 'sto',\n",
       " 'stai',\n",
       " 'sta',\n",
       " 'stiamo',\n",
       " 'stanno',\n",
       " 'stia',\n",
       " 'stiate',\n",
       " 'stiano',\n",
       " 'star√≤',\n",
       " 'starai',\n",
       " 'star√†',\n",
       " 'staremo',\n",
       " 'starete',\n",
       " 'staranno',\n",
       " 'starei',\n",
       " 'staresti',\n",
       " 'starebbe',\n",
       " 'staremmo',\n",
       " 'stareste',\n",
       " 'starebbero',\n",
       " 'stavo',\n",
       " 'stavi',\n",
       " 'stava',\n",
       " 'stavamo',\n",
       " 'stavate',\n",
       " 'stavano',\n",
       " 'stetti',\n",
       " 'stesti',\n",
       " 'stette',\n",
       " 'stemmo',\n",
       " 'steste',\n",
       " 'stettero',\n",
       " 'stessi',\n",
       " 'stesse',\n",
       " 'stessimo',\n",
       " 'stessero',\n",
       " 'stando']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('italian') #'forse', 'qualche', 'qualcosa', 'chiss√†', 'po', 'stata', 'fatta', 'fatto', 'alcuni', \n",
    "#'quasi', 'oltre', 'fate', 'to', 'farne', 'far', 'ecco', 'per√≤', 's√¨', 'circa', 'state', 'ok', 'magari', 'so', \n",
    "#'ieri', 'oggi', 'stare', 'perch√®', 'eh', 'ah', 'vabb√®', 'ce', 'fra', 'proprio', 'te', 'pensa', 'vuoi', 'sai', \n",
    "#'puoi', 'devi', 'vai', 'fatti', 'guarda', 'dico', 'sa', 'sti', 'allora', 'tutte','altre', 'comunque', 'avere', 'deve'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('italian')\n",
    "en_stop = stopwords.words('english')\n",
    "query = ['vacc', 'vaccinale', 'vaccinali', 'vaccinano', 'vaccinarci', 'vaccinare', 'vaccinarsi',\n",
    "               'vaccinate', 'vaccinati', 'vaccinato', 'vaccinaz', 'vaccinazione', 'vaccinazioni', 'vaccines','vax','vaccine',\n",
    "               'vaccini', 'vaccinista', 'vaccinisti', 'vaccino', 'antivaccinisti', 'freevax', 'iovaccino', \n",
    "               'nonvaccinato', 'novax', 'obbligovaccinale', 'provax', 'ridacciilvaccino','vaccine']\n",
    "\n",
    "re_url = re.compile(r'https?:\\/\\/.*[\\r\\n]*', flags=re.U)\n",
    "#re_rtw = re.compile(r'RT', flags=re.U)\n",
    "re_htg = re.compile(r'#', flags=re.U) # remove hashtag sign\n",
    "#re_htg = re.compile(r'#[\\w]+ ?', flags=re.U)   # remove hashtags\n",
    "re_hnd = re.compile(r'@', flags=re.U)\n",
    "#re_hnd = re.compile(r'@\\w+ ?', flags=re.U)\n",
    "re_wrd = re.compile(r'[^\\w]+ ', flags=re.U)\n",
    "re_num = re.compile(r'[0-9]+', flags=re.U)\n",
    "\n",
    "def cleantext(txt):\n",
    "    t = txt\n",
    "    t = re_url.sub('', t)\n",
    "    #t = re_htg.sub('', t)\n",
    "    #t = re_rtw.sub(' ', t)\n",
    "    t = re_hnd.sub(' ', t)\n",
    "    t = re_wrd.sub(' ', t)\n",
    "    t = re_num.sub(' ', t)\n",
    "    return t.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining stop words\n",
    "more_stop = ['gi√†','poi','solo','no','fa','pu√≤','quindi','quando','x','ogni','altro','cos√¨','mai','tutta','ancora',\n",
    "            'ora', 'molto','d', 'via','sempre','rt','co','https','dopo','fare','fatto','italia','essere','cosa',\n",
    "            'oggi','bene','dire','dice','vuole','vaccinati','vaccino','vaccini','vaccinato','senza','altri','me',\n",
    "             'detto','meno','invece','va','grazie']\n",
    "            \n",
    "more_more = ['forse', 'qualche', 'qualcosa', 'chiss√†', 'po', 'stata', 'fatta', 'fatto', 'alcuni', \n",
    "            'quasi', 'oltre', 'fate', 'to', 'farne', 'far', 'ecco', 'per√≤', 's√¨', 'circa', 'state', 'ok', 'magari', 'so', \n",
    "            'ieri', 'oggi', 'stare', 'perch√®', 'eh', 'ah', 'vabb√®', 'ce', 'fra', 'proprio', 'te', 'pensa', 'vuoi', 'sai', \n",
    "            'puoi', 'devi', 'vai', 'fatti', 'guarda', 'dico', 'sa', 'sti', 'allora', 'tutte','altre', 'comunque', 'avere', 'deve']\n",
    "\n",
    "stop_words = set(stop+query+en_stop+more_stop+more_more)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-COVID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw0_df = sort_tweets_df[sort_tweets_df['created_at'] < jan_1st_20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             204275\n",
       "id_usr         204275\n",
       "created_at     204275\n",
       "master_text    204275\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw0_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.09 s, sys: 1.17 s, total: 4.26 s\n",
      "Wall time: 4.67 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#cleaning the text and adding a 'clean_text' field\n",
    "tw0_df['clean_text'] = tw0_df['master_text'].apply(lambda txt: cleantext(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>id_usr</th>\n",
       "      <th>created_at</th>\n",
       "      <th>master_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1169560932198039552</td>\n",
       "      <td>939822944</td>\n",
       "      <td>2019-09-05 10:40:30</td>\n",
       "      <td>@carlosibilia @GiuseppeConteIT QUINDI  SETTIMA...</td>\n",
       "      <td>carlosibilia giuseppeconteit quindi settimana ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1169561004151382016</td>\n",
       "      <td>939822944</td>\n",
       "      <td>2019-09-05 10:40:47</td>\n",
       "      <td>@SkyTG24 POTRESTE CHIEDERE X CORTESIA   SE SET...</td>\n",
       "      <td>skytg  potreste chiedere x cortesia se settima...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1169561087475421184</td>\n",
       "      <td>939822944</td>\n",
       "      <td>2019-09-05 10:41:07</td>\n",
       "      <td>@F_Boccia @pdnetwork POTRESTE CHIEDERE X CORTE...</td>\n",
       "      <td>f_boccia pdnetwork potreste chiedere x cortesi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1169561110388850688</td>\n",
       "      <td>80200885</td>\n",
       "      <td>2019-09-05 10:41:12</td>\n",
       "      <td>Nel 2018 votai 5 stelle per due motivi: 1) abo...</td>\n",
       "      <td>nel   votai   stelle per due motivi   abolizio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1169561453701083136</td>\n",
       "      <td>939822944</td>\n",
       "      <td>2019-09-05 10:42:34</td>\n",
       "      <td>@RepubblicaTv POTRESTE CHIEDERE X CORTESIA   S...</td>\n",
       "      <td>repubblicatv potreste chiedere x cortesia se s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id     id_usr          created_at  \\\n",
       "0  1169560932198039552  939822944 2019-09-05 10:40:30   \n",
       "1  1169561004151382016  939822944 2019-09-05 10:40:47   \n",
       "2  1169561087475421184  939822944 2019-09-05 10:41:07   \n",
       "3  1169561110388850688   80200885 2019-09-05 10:41:12   \n",
       "4  1169561453701083136  939822944 2019-09-05 10:42:34   \n",
       "\n",
       "                                         master_text  \\\n",
       "0  @carlosibilia @GiuseppeConteIT QUINDI  SETTIMA...   \n",
       "1  @SkyTG24 POTRESTE CHIEDERE X CORTESIA   SE SET...   \n",
       "2  @F_Boccia @pdnetwork POTRESTE CHIEDERE X CORTE...   \n",
       "3  Nel 2018 votai 5 stelle per due motivi: 1) abo...   \n",
       "4  @RepubblicaTv POTRESTE CHIEDERE X CORTESIA   S...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  carlosibilia giuseppeconteit quindi settimana ...  \n",
       "1  skytg  potreste chiedere x cortesia se settima...  \n",
       "2  f_boccia pdnetwork potreste chiedere x cortesi...  \n",
       "3  nel   votai   stelle per due motivi   abolizio...  \n",
       "4  repubblicatv potreste chiedere x cortesia se s...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw0_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(204275, 43001)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tw0_df), len(tw0_df.drop_duplicates(subset=['master_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phraser\n",
    "In this section we add bigrams to the vocabulary: namely couple of words which often go together (i.e. \"green_pass\" or \"new_york\" etc...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_analyzer(text):\n",
    "    words = [w for w in token_pattern.findall(text.lower()) if w not in stop_words]\n",
    "    return bigram[words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 744 ms, sys: 116 ms, total: 861 ms\n",
      "Wall time: 859 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "token_pattern = re.compile(r'(?u)\\b[A-Za-z]\\w+\\b')\n",
    "text_sentences = []\n",
    "#building the dictionary with unique pieces of text (in other words dropping duplicated on the clean_text column)\n",
    "for doc in tw0_df.clean_text.drop_duplicates():\n",
    "    text_sentences.extend([token_pattern.findall(sent.lower()) for sent in doc.split('\\n') if len(sent) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.02 s, sys: 11.5 ms, total: 7.04 s\n",
      "Wall time: 7.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#min_count is the minimal nuimber of times that a single bigram has to appear in order to be considered a real bigram\n",
    "#threshold is linked to the probability of observing the words of the bigram together and the probability of \n",
    "#observing them separately\n",
    "phrases = Phrases(text_sentences, min_count=10, threshold=20., common_terms=stop_words) #, scoring='npmi')\n",
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the count vectorizer on the clean_text field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gcrupi/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:497: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'stop_words' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.97 s, sys: 99.6 ms, total: 3.07 s\n",
      "Wall time: 3.27 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=<function phrase_analyzer at 0x7f3f30319550>,\n",
       "                max_df=0.5, min_df=10,\n",
       "                stop_words={'a', 'abbia', 'abbiamo', 'abbiano', 'abbiate',\n",
       "                            'about', 'above', 'ad', 'after', 'again', 'against',\n",
       "                            'agl', 'agli', 'ah', 'ai', 'ain', 'al', 'alcuni',\n",
       "                            'all', 'alla', 'alle', 'allo', 'allora', 'altre',\n",
       "                            'altri', 'altro', 'am', 'an', 'anche', 'ancora', ...})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cv2 = feature_extraction.text.CountVectorizer(min_df=10, max_df=0.5, stop_words=stop_words, analyzer=phrase_analyzer)\n",
    "#building the vocabulary with unique pieces of text (in other words dropping duplicates in the clean_text column)\n",
    "cv2.fit(tw0_df.clean_text.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.1 s, sys: 592 ms, total: 16.7 s\n",
      "Wall time: 16.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#here I just obtain the matrix of counts of all the tweets, but with the vocabulary built with the unique \n",
    "#pieces of text only\n",
    "C2 = cv2.transform(tw0_df.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tw0_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<204275x7458 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2399096 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/gcrupi/6_time_windows/sparse_matrices/top_model_timewindow/counts_vocabulary_preCOVID.joblib']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#saving the raw count matrix C2 and the vocabulary\n",
    "\n",
    "joblib.dump([C2,cv2], '/../data/counts_vocabulary_i.joblib', compress=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# early-COVID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.where((sort_tweets_df['created_at'] >= jan_1st_20) & (sort_tweets_df['created_at'] < mar_9th_20))\n",
    "\n",
    "tw1_df = sort_tweets_df.loc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             125887\n",
       "id_usr         125887\n",
       "created_at     125887\n",
       "master_text    125887\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw1_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.96 s, sys: 724 ms, total: 2.68 s\n",
      "Wall time: 2.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#cleaning the text and adding a 'clean_text' field\n",
    "tw1_df['clean_text'] = tw1_df['master_text'].apply(lambda txt: cleantext(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>id_usr</th>\n",
       "      <th>created_at</th>\n",
       "      <th>master_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>204275</th>\n",
       "      <td>1212161556798152706</td>\n",
       "      <td>1045707647440302086</td>\n",
       "      <td>2020-01-01 00:00:10</td>\n",
       "      <td>@RobertoBurioni üßênon riesce ad ammettere i rea...</td>\n",
       "      <td>robertoburioni üßênon riesce ad ammettere i real...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204276</th>\n",
       "      <td>1212162656574066688</td>\n",
       "      <td>4160203479</td>\n",
       "      <td>2020-01-01 00:04:32</td>\n",
       "      <td>Quando ho iniziato a dissentire sull'obbligo v...</td>\n",
       "      <td>quando ho iniziato a dissentire sull'obbligo v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204277</th>\n",
       "      <td>1212162722198151168</td>\n",
       "      <td>1087376422140817409</td>\n",
       "      <td>2020-01-01 00:04:48</td>\n",
       "      <td>‚ùå‚ùå‚ù§Ô∏è‚ù§Ô∏èüíîüíî I CUCCIOLI IN CANILE NON DOVREBBERO M...</td>\n",
       "      <td>‚ùå‚ùå‚ù§Ô∏è‚ù§Ô∏è i cuccioli in canile non dovrebbero mai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204278</th>\n",
       "      <td>1212162901433339904</td>\n",
       "      <td>884873425</td>\n",
       "      <td>2020-01-01 00:05:31</td>\n",
       "      <td>Il Messaggero: Alzheimer e demenza, vaccino vi...</td>\n",
       "      <td>il messaggero alzheimer e demenza vaccino vici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204279</th>\n",
       "      <td>1212164483432493056</td>\n",
       "      <td>700901262</td>\n",
       "      <td>2020-01-01 00:11:48</td>\n",
       "      <td>@nopost3b @osvaldoluci @intuslegens Ma i vacci...</td>\n",
       "      <td>nopost b osvaldoluci intuslegens ma i vaccini ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id               id_usr          created_at  \\\n",
       "204275  1212161556798152706  1045707647440302086 2020-01-01 00:00:10   \n",
       "204276  1212162656574066688           4160203479 2020-01-01 00:04:32   \n",
       "204277  1212162722198151168  1087376422140817409 2020-01-01 00:04:48   \n",
       "204278  1212162901433339904            884873425 2020-01-01 00:05:31   \n",
       "204279  1212164483432493056            700901262 2020-01-01 00:11:48   \n",
       "\n",
       "                                              master_text  \\\n",
       "204275  @RobertoBurioni üßênon riesce ad ammettere i rea...   \n",
       "204276  Quando ho iniziato a dissentire sull'obbligo v...   \n",
       "204277  ‚ùå‚ùå‚ù§Ô∏è‚ù§Ô∏èüíîüíî I CUCCIOLI IN CANILE NON DOVREBBERO M...   \n",
       "204278  Il Messaggero: Alzheimer e demenza, vaccino vi...   \n",
       "204279  @nopost3b @osvaldoluci @intuslegens Ma i vacci...   \n",
       "\n",
       "                                               clean_text  \n",
       "204275  robertoburioni üßênon riesce ad ammettere i real...  \n",
       "204276  quando ho iniziato a dissentire sull'obbligo v...  \n",
       "204277  ‚ùå‚ùå‚ù§Ô∏è‚ù§Ô∏è i cuccioli in canile non dovrebbero mai...  \n",
       "204278  il messaggero alzheimer e demenza vaccino vici...  \n",
       "204279  nopost b osvaldoluci intuslegens ma i vaccini ...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125887, 35181)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tw1_df), len(tw1_df.drop_duplicates(subset=['master_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phraser\n",
    "In this section we add bigrams to the vocabulary: namely couple of words which often go together (i.e. \"green_pass\" or \"new_york\" etc...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 775 ms, sys: 91.6 ms, total: 867 ms\n",
      "Wall time: 865 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "token_pattern = re.compile(r'(?u)\\b[A-Za-z]\\w+\\b')\n",
    "text_sentences = []\n",
    "#building the dictionary with unique pieces of text (in other words dropping duplicated on the clean_text column)\n",
    "for doc in tw1_df.clean_text.drop_duplicates():\n",
    "    text_sentences.extend([token_pattern.findall(sent.lower()) for sent in doc.split('\\n') if len(sent) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.06 s, sys: 7.14 ms, total: 6.06 s\n",
      "Wall time: 6.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#min_count is the minimal nuimber of times that a single bigram has to appear in order to be considered a real bigram\n",
    "#threshold is linked to the probability of observing the words of the bigram together and the probability of \n",
    "#observing them separately\n",
    "phrases = Phrases(text_sentences, min_count=10, threshold=20., common_terms=stop_words) #, scoring='npmi')\n",
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the count vectorizer on the clean_text field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gcrupi/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:497: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'stop_words' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.43 s, sys: 88.1 ms, total: 2.52 s\n",
      "Wall time: 2.52 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=<function phrase_analyzer at 0x7f3f30319550>,\n",
       "                max_df=0.5, min_df=10,\n",
       "                stop_words={'a', 'abbia', 'abbiamo', 'abbiano', 'abbiate',\n",
       "                            'about', 'above', 'ad', 'after', 'again', 'against',\n",
       "                            'agl', 'agli', 'ah', 'ai', 'ain', 'al', 'alcuni',\n",
       "                            'all', 'alla', 'alle', 'allo', 'allora', 'altre',\n",
       "                            'altri', 'altro', 'am', 'an', 'anche', 'ancora', ...})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cv2 = feature_extraction.text.CountVectorizer(min_df=10, max_df=0.5, stop_words=stop_words, analyzer=phrase_analyzer)\n",
    "#building the vocabulary with unique pieces of text (in other words dropping duplicates in the clean_text column)\n",
    "cv2.fit(tw1_df.clean_text.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.73 s, sys: 291 ms, total: 10 s\n",
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#here I just obtain the matrix of counts of all the tweets, but with the vocabulary built with the unique \n",
    "#pieces of text only\n",
    "C2 = cv2.transform(tw1_df.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tw1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<125887x6171 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1332630 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/gcrupi/6_time_windows/sparse_matrices/top_model_timewindow/counts_vocabulary_preCOVID.joblib']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#saving the raw count matrix C2 and the vocabulary\n",
    "\n",
    "joblib.dump([C2,cv2], '/../data/counts_vocabulary_ii.joblib', compress=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-VAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.where((sort_tweets_df['created_at'] >= mar_9th_20) & (sort_tweets_df['created_at'] < nov_1st_20))\n",
    "\n",
    "tw2_df = sort_tweets_df.loc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             1036177\n",
       "id_usr         1036177\n",
       "created_at     1036177\n",
       "master_text    1036177\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw2_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 s, sys: 5.71 s, total: 21.7 s\n",
      "Wall time: 21.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#cleaning the text and adding a 'clean_text' field\n",
    "tw2_df['clean_text'] = tw2_df['master_text'].apply(lambda txt: cleantext(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>id_usr</th>\n",
       "      <th>created_at</th>\n",
       "      <th>master_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>330162</th>\n",
       "      <td>1236804065906110465</td>\n",
       "      <td>128242325</td>\n",
       "      <td>2020-03-09 00:00:42</td>\n",
       "      <td>Sarebbe interessante sapere se quelli deceduti...</td>\n",
       "      <td>sarebbe interessante sapere se quelli deceduti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330163</th>\n",
       "      <td>1236804086521106434</td>\n",
       "      <td>1004943776199028738</td>\n",
       "      <td>2020-03-09 00:00:47</td>\n",
       "      <td>I medici cinesi lavorano a pieno ritmo per un ...</td>\n",
       "      <td>i medici cinesi lavorano a pieno ritmo per un ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330164</th>\n",
       "      <td>1236804171799633926</td>\n",
       "      <td>955107625393418241</td>\n",
       "      <td>2020-03-09 00:01:07</td>\n",
       "      <td>Italia. Il Paese il cui Premier ha un comunica...</td>\n",
       "      <td>italia il paese il cui premier ha un comunicat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330165</th>\n",
       "      <td>1236804401148432384</td>\n",
       "      <td>1093535050757476355</td>\n",
       "      <td>2020-03-09 00:02:02</td>\n",
       "      <td>@MinervaMcGrani1 Vaccini non ne faccio assolut...</td>\n",
       "      <td>minervamcgrani  vaccini non ne faccio assoluta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330166</th>\n",
       "      <td>1236804433595465729</td>\n",
       "      <td>3010677376</td>\n",
       "      <td>2020-03-09 00:02:10</td>\n",
       "      <td>Scoppia un‚Äôepidemia nazionale quando al govern...</td>\n",
       "      <td>scoppia un‚Äôepidemia nazionale quando al govern...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id               id_usr          created_at  \\\n",
       "330162  1236804065906110465            128242325 2020-03-09 00:00:42   \n",
       "330163  1236804086521106434  1004943776199028738 2020-03-09 00:00:47   \n",
       "330164  1236804171799633926   955107625393418241 2020-03-09 00:01:07   \n",
       "330165  1236804401148432384  1093535050757476355 2020-03-09 00:02:02   \n",
       "330166  1236804433595465729           3010677376 2020-03-09 00:02:10   \n",
       "\n",
       "                                              master_text  \\\n",
       "330162  Sarebbe interessante sapere se quelli deceduti...   \n",
       "330163  I medici cinesi lavorano a pieno ritmo per un ...   \n",
       "330164  Italia. Il Paese il cui Premier ha un comunica...   \n",
       "330165  @MinervaMcGrani1 Vaccini non ne faccio assolut...   \n",
       "330166  Scoppia un‚Äôepidemia nazionale quando al govern...   \n",
       "\n",
       "                                               clean_text  \n",
       "330162  sarebbe interessante sapere se quelli deceduti...  \n",
       "330163  i medici cinesi lavorano a pieno ritmo per un ...  \n",
       "330164  italia il paese il cui premier ha un comunicat...  \n",
       "330165  minervamcgrani  vaccini non ne faccio assoluta...  \n",
       "330166  scoppia un‚Äôepidemia nazionale quando al govern...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw2_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1036177, 349401)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tw2_df), len(tw2_df.drop_duplicates(subset=['master_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phraser\n",
    "In this section we add bigrams to the vocabulary: namely couple of words which often go together (i.e. \"green_pass\" or \"new_york\" etc...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.29 s, sys: 812 ms, total: 7.1 s\n",
      "Wall time: 7.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "token_pattern = re.compile(r'(?u)\\b[A-Za-z]\\w+\\b')\n",
    "text_sentences = []\n",
    "#building the dictionary with unique pieces of text (in other words dropping duplicated on the clean_text column)\n",
    "for doc in tw2_df.clean_text.drop_duplicates():\n",
    "    text_sentences.extend([token_pattern.findall(sent.lower()) for sent in doc.split('\\n') if len(sent) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50.1 s, sys: 31.6 ms, total: 50.1 s\n",
      "Wall time: 50.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#min_count is the minimal nuimber of times that a single bigram has to appear in order to be considered a real bigram\n",
    "#threshold is linked to the probability of observing the words of the bigram together and the probability of \n",
    "#observing them separately\n",
    "phrases = Phrases(text_sentences, min_count=10, threshold=20., common_terms=stop_words) #, scoring='npmi')\n",
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the count vectorizer on the clean_text field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gcrupi/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:497: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'stop_words' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.3 s, sys: 791 ms, total: 23.1 s\n",
      "Wall time: 23.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=<function phrase_analyzer at 0x7f3f30319550>,\n",
       "                max_df=0.5, min_df=10,\n",
       "                stop_words={'a', 'abbia', 'abbiamo', 'abbiano', 'abbiate',\n",
       "                            'about', 'above', 'ad', 'after', 'again', 'against',\n",
       "                            'agl', 'agli', 'ah', 'ai', 'ain', 'al', 'alcuni',\n",
       "                            'all', 'alla', 'alle', 'allo', 'allora', 'altre',\n",
       "                            'altri', 'altro', 'am', 'an', 'anche', 'ancora', ...})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cv2 = feature_extraction.text.CountVectorizer(min_df=10, max_df=0.5, stop_words=stop_words, analyzer=phrase_analyzer)\n",
    "#building the vocabulary with unique pieces of text (in other words dropping duplicates in the clean_text column)\n",
    "cv2.fit(tw2_df.clean_text.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 17s, sys: 2.71 s, total: 1min 19s\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#here I just obtain the matrix of counts of all the tweets, but with the vocabulary built with the unique \n",
    "#pieces of text only\n",
    "C2 = cv2.transform(tw2_df.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tw2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1036177x32842 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 12215954 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/gcrupi/6_time_windows/sparse_matrices/top_model_timewindow/counts_vocabulary_preCOVID.joblib']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#saving the raw count matrix C2 and the vocabulary\n",
    "\n",
    "joblib.dump([C2,cv2], '/../data/counts_vocabulary_iii.joblib', compress=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# early-VAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.where((sort_tweets_df['created_at'] >= nov_1st_20) & (sort_tweets_df['created_at'] < apr_17_21))\n",
    "\n",
    "tw3_df = sort_tweets_df.loc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             5137559\n",
       "id_usr         5137559\n",
       "created_at     5137559\n",
       "master_text    5137559\n",
       "dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw3_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 13s, sys: 26.9 s, total: 1min 40s\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#cleaning the text and adding a 'clean_text' field\n",
    "tw3_df['clean_text'] = tw3_df['master_text'].apply(lambda txt: cleantext(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>id_usr</th>\n",
       "      <th>created_at</th>\n",
       "      <th>master_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1366339</th>\n",
       "      <td>1322689824499916801</td>\n",
       "      <td>28414121</td>\n",
       "      <td>2020-11-01 00:00:03</td>\n",
       "      <td>Non ho detto di no a un cocktail fluorescente ...</td>\n",
       "      <td>non ho detto di no a un cocktail fluorescente ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1366340</th>\n",
       "      <td>1322689942624063491</td>\n",
       "      <td>1088147570961068034</td>\n",
       "      <td>2020-11-01 00:00:31</td>\n",
       "      <td>VALIGIA E CORREDINO PRONTI MA LA FAMIGLIA HA R...</td>\n",
       "      <td>valigia e corredino pronti ma la famiglia ha r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1366341</th>\n",
       "      <td>1322690040020013056</td>\n",
       "      <td>805913988240474115</td>\n",
       "      <td>2020-11-01 00:00:54</td>\n",
       "      <td>Provincia di Lecce\\n\\nQuesto cucciolone di 1 a...</td>\n",
       "      <td>provincia di lecce\\n\\nquesto cucciolone di   a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1366342</th>\n",
       "      <td>1322690740267425792</td>\n",
       "      <td>790152050948575233</td>\n",
       "      <td>2020-11-01 00:03:41</td>\n",
       "      <td>@Clubchakama @MDragonil @MassimoGalli51 Quello...</td>\n",
       "      <td>clubchakama mdragonil massimogalli  quello che...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1366343</th>\n",
       "      <td>1322691045549813763</td>\n",
       "      <td>121443333</td>\n",
       "      <td>2020-11-01 00:04:54</td>\n",
       "      <td>Urbano #Cairo, proprietario de #la7, positivo ...</td>\n",
       "      <td>urbano #cairo proprietario de #la  positivo al...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id               id_usr          created_at  \\\n",
       "1366339  1322689824499916801             28414121 2020-11-01 00:00:03   \n",
       "1366340  1322689942624063491  1088147570961068034 2020-11-01 00:00:31   \n",
       "1366341  1322690040020013056   805913988240474115 2020-11-01 00:00:54   \n",
       "1366342  1322690740267425792   790152050948575233 2020-11-01 00:03:41   \n",
       "1366343  1322691045549813763            121443333 2020-11-01 00:04:54   \n",
       "\n",
       "                                               master_text  \\\n",
       "1366339  Non ho detto di no a un cocktail fluorescente ...   \n",
       "1366340  VALIGIA E CORREDINO PRONTI MA LA FAMIGLIA HA R...   \n",
       "1366341  Provincia di Lecce\\n\\nQuesto cucciolone di 1 a...   \n",
       "1366342  @Clubchakama @MDragonil @MassimoGalli51 Quello...   \n",
       "1366343  Urbano #Cairo, proprietario de #la7, positivo ...   \n",
       "\n",
       "                                                clean_text  \n",
       "1366339  non ho detto di no a un cocktail fluorescente ...  \n",
       "1366340  valigia e corredino pronti ma la famiglia ha r...  \n",
       "1366341  provincia di lecce\\n\\nquesto cucciolone di   a...  \n",
       "1366342  clubchakama mdragonil massimogalli  quello che...  \n",
       "1366343  urbano #cairo proprietario de #la  positivo al...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw3_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5137559, 1868058)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tw3_df), len(tw3_df.drop_duplicates(subset=['master_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phraser\n",
    "In this section we add bigrams to the vocabulary: namely couple of words which often go together (i.e. \"green_pass\" or \"new_york\" etc...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.2 s, sys: 4.46 s, total: 40.6 s\n",
      "Wall time: 40.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "token_pattern = re.compile(r'(?u)\\b[A-Za-z]\\w+\\b')\n",
    "text_sentences = []\n",
    "#building the dictionary with unique pieces of text (in other words dropping duplicated on the clean_text column)\n",
    "for doc in tw3_df.clean_text.drop_duplicates():\n",
    "    text_sentences.extend([token_pattern.findall(sent.lower()) for sent in doc.split('\\n') if len(sent) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 1s, sys: 812 ms, total: 4min 1s\n",
      "Wall time: 4min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#min_count is the minimal nuimber of times that a single bigram has to appear in order to be considered a real bigram\n",
    "#threshold is linked to the probability of observing the words of the bigram together and the probability of \n",
    "#observing them separately\n",
    "phrases = Phrases(text_sentences, min_count=10, threshold=20., common_terms=stop_words) #, scoring='npmi')\n",
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the count vectorizer on the clean_text field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gcrupi/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:497: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'stop_words' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 59s, sys: 4.85 s, total: 2min 4s\n",
      "Wall time: 2min 4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=<function phrase_analyzer at 0x7f3f30319550>,\n",
       "                max_df=0.5, min_df=10,\n",
       "                stop_words={'a', 'abbia', 'abbiamo', 'abbiano', 'abbiate',\n",
       "                            'about', 'above', 'ad', 'after', 'again', 'against',\n",
       "                            'agl', 'agli', 'ah', 'ai', 'ain', 'al', 'alcuni',\n",
       "                            'all', 'alla', 'alle', 'allo', 'allora', 'altre',\n",
       "                            'altri', 'altro', 'am', 'an', 'anche', 'ancora', ...})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cv2 = feature_extraction.text.CountVectorizer(min_df=10, max_df=0.5, stop_words=stop_words, analyzer=phrase_analyzer)\n",
    "#building the vocabulary with unique pieces of text (in other words dropping duplicates in the clean_text column)\n",
    "cv2.fit(tw3_df.clean_text.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 14s, sys: 13.4 s, total: 6min 27s\n",
      "Wall time: 6min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#here I just obtain the matrix of counts of all the tweets, but with the vocabulary built with the unique \n",
    "#pieces of text only\n",
    "C2 = cv2.transform(tw3_df.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tw3_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5137559x95428 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 57800490 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/gcrupi/6_time_windows/sparse_matrices/top_model_timewindow/counts_vocabulary_preCOVID.joblib']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#saving the raw count matrix C2 and the vocabulary\n",
    "\n",
    "joblib.dump([C2,cv2], '/../data/counts_vocabulary_iv.joblib', compress=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAX-drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.where((sort_tweets_df['created_at'] >= apr_17_21) & (sort_tweets_df['created_at'] < aug_1st_21))\n",
    "\n",
    "tw4_df = sort_tweets_df.loc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             4160533\n",
       "id_usr         4160533\n",
       "created_at     4160533\n",
       "master_text    4160533\n",
       "dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw4_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59.9 s, sys: 25.7 s, total: 1min 25s\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#cleaning the text and adding a 'clean_text' field\n",
    "tw4_df['clean_text'] = tw4_df['master_text'].apply(lambda txt: cleantext(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>id_usr</th>\n",
       "      <th>created_at</th>\n",
       "      <th>master_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6503898</th>\n",
       "      <td>1383208730687311878</td>\n",
       "      <td>884460743453798401</td>\n",
       "      <td>2021-04-17 00:00:34</td>\n",
       "      <td>Caro @demagistris, nella scuola di mia figlia,...</td>\n",
       "      <td>caro demagistris nella scuola di mia figlia ie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6503899</th>\n",
       "      <td>1383208742301360134</td>\n",
       "      <td>2175887381</td>\n",
       "      <td>2021-04-17 00:00:37</td>\n",
       "      <td>Lorenzo Costa, consigliere comunale a Catanzar...</td>\n",
       "      <td>lorenzo costa consigliere comunale a catanzaro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6503900</th>\n",
       "      <td>1383208810320367625</td>\n",
       "      <td>1241210117657432064</td>\n",
       "      <td>2021-04-17 00:00:53</td>\n",
       "      <td>@luigidimaio Attenzione perche i vaccinati son...</td>\n",
       "      <td>luigidimaio attenzione perche i vaccinati sono...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6503901</th>\n",
       "      <td>1383208820718120960</td>\n",
       "      <td>1024702799215509504</td>\n",
       "      <td>2021-04-17 00:00:56</td>\n",
       "      <td>Se non ve ne foste accorti\\nnessuno dice che i...</td>\n",
       "      <td>se non ve ne foste accorti\\nnessuno dice che i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6503902</th>\n",
       "      <td>1383208847054098432</td>\n",
       "      <td>1699728120</td>\n",
       "      <td>2021-04-17 00:01:02</td>\n",
       "      <td>Sputnik V, \"nessun caso di trombosi\". Scienzia...</td>\n",
       "      <td>sputnik v \"nessun caso di trombosi scienziati ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id               id_usr          created_at  \\\n",
       "6503898  1383208730687311878   884460743453798401 2021-04-17 00:00:34   \n",
       "6503899  1383208742301360134           2175887381 2021-04-17 00:00:37   \n",
       "6503900  1383208810320367625  1241210117657432064 2021-04-17 00:00:53   \n",
       "6503901  1383208820718120960  1024702799215509504 2021-04-17 00:00:56   \n",
       "6503902  1383208847054098432           1699728120 2021-04-17 00:01:02   \n",
       "\n",
       "                                               master_text  \\\n",
       "6503898  Caro @demagistris, nella scuola di mia figlia,...   \n",
       "6503899  Lorenzo Costa, consigliere comunale a Catanzar...   \n",
       "6503900  @luigidimaio Attenzione perche i vaccinati son...   \n",
       "6503901  Se non ve ne foste accorti\\nnessuno dice che i...   \n",
       "6503902  Sputnik V, \"nessun caso di trombosi\". Scienzia...   \n",
       "\n",
       "                                                clean_text  \n",
       "6503898  caro demagistris nella scuola di mia figlia ie...  \n",
       "6503899  lorenzo costa consigliere comunale a catanzaro...  \n",
       "6503900  luigidimaio attenzione perche i vaccinati sono...  \n",
       "6503901  se non ve ne foste accorti\\nnessuno dice che i...  \n",
       "6503902  sputnik v \"nessun caso di trombosi scienziati ...  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw4_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4160533, 1507488)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tw4_df), len(tw4_df.drop_duplicates(subset=['master_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phraser\n",
    "In this section we add bigrams to the vocabulary: namely couple of words which often go together (i.e. \"green_pass\" or \"new_york\" etc...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.6 s, sys: 4.18 s, total: 38.8 s\n",
      "Wall time: 40 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "token_pattern = re.compile(r'(?u)\\b[A-Za-z]\\w+\\b')\n",
    "text_sentences = []\n",
    "#building the dictionary with unique pieces of text (in other words dropping duplicated on the clean_text column)\n",
    "for doc in tw4_df.clean_text.drop_duplicates():\n",
    "    text_sentences.extend([token_pattern.findall(sent.lower()) for sent in doc.split('\\n') if len(sent) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 27s, sys: 698 ms, total: 3min 27s\n",
      "Wall time: 3min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#min_count is the minimal nuimber of times that a single bigram has to appear in order to be considered a real bigram\n",
    "#threshold is linked to the probability of observing the words of the bigram together and the probability of \n",
    "#observing them separately\n",
    "phrases = Phrases(text_sentences, min_count=10, threshold=20., common_terms=stop_words) #, scoring='npmi')\n",
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the count vectorizer on the clean_text field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gcrupi/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:497: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'stop_words' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 41s, sys: 4.07 s, total: 1min 45s\n",
      "Wall time: 1min 45s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=<function phrase_analyzer at 0x7f3f30319550>,\n",
       "                max_df=0.5, min_df=10,\n",
       "                stop_words={'a', 'abbia', 'abbiamo', 'abbiano', 'abbiate',\n",
       "                            'about', 'above', 'ad', 'after', 'again', 'against',\n",
       "                            'agl', 'agli', 'ah', 'ai', 'ain', 'al', 'alcuni',\n",
       "                            'all', 'alla', 'alle', 'allo', 'allora', 'altre',\n",
       "                            'altri', 'altro', 'am', 'an', 'anche', 'ancora', ...})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cv2 = feature_extraction.text.CountVectorizer(min_df=10, max_df=0.5, stop_words=stop_words, analyzer=phrase_analyzer)\n",
    "#building the vocabulary with unique pieces of text (in other words dropping duplicates in the clean_text column)\n",
    "cv2.fit(tw4_df.clean_text.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 3s, sys: 11.1 s, total: 5min 14s\n",
      "Wall time: 5min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#here I just obtain the matrix of counts of all the tweets, but with the vocabulary built with the unique \n",
    "#pieces of text only\n",
    "C2 = cv2.transform(tw4_df.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tw4_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4160533x85922 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 45843757 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/gcrupi/6_time_windows/sparse_matrices/top_model_timewindow/counts_vocabulary_preCOVID.joblib']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#saving the raw count matrix C2 and the vocabulary\n",
    "\n",
    "joblib.dump([C2,cv2], '/../data/counts_vocabulary_v.joblib', compress=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# late-VAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw5_df = sort_tweets_df[sort_tweets_df['created_at'] >= aug_1st_21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             5450244\n",
       "id_usr         5450244\n",
       "created_at     5450244\n",
       "master_text    5450244\n",
       "dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw5_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 22s, sys: 30.2 s, total: 1min 52s\n",
      "Wall time: 1min 52s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#cleaning the text and adding a 'clean_text' field\n",
    "tw5_df['clean_text'] = tw5_df['master_text'].apply(lambda txt: cleantext(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>id_usr</th>\n",
       "      <th>created_at</th>\n",
       "      <th>master_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10664431</th>\n",
       "      <td>1421621702924988418</td>\n",
       "      <td>2691538351</td>\n",
       "      <td>2021-08-01 00:00:01</td>\n",
       "      <td>@FoxNews Vaccinate</td>\n",
       "      <td>foxnews vaccinate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10664432</th>\n",
       "      <td>1421621729185595400</td>\n",
       "      <td>632537304</td>\n",
       "      <td>2021-08-01 00:00:07</td>\n",
       "      <td>Sono passati 5 giorni dopo la prima dose del v...</td>\n",
       "      <td>sono passati   giorni dopo la prima dose del v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10664433</th>\n",
       "      <td>1421621731769393153</td>\n",
       "      <td>1252333704996888576</td>\n",
       "      <td>2021-08-01 00:00:07</td>\n",
       "      <td>@AlfioKrancic Sito che lavora per nato e milit...</td>\n",
       "      <td>alfiokrancic sito che lavora per nato e milita...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10664434</th>\n",
       "      <td>1421621744251592706</td>\n",
       "      <td>1214181278334865408</td>\n",
       "      <td>2021-08-01 00:00:10</td>\n",
       "      <td>@Leonard48598239 S√¨ tu la fai parlare e non ri...</td>\n",
       "      <td>leonard  s√¨ tu la fai parlare e non rispondere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10664435</th>\n",
       "      <td>1421621745270902786</td>\n",
       "      <td>22602699</td>\n",
       "      <td>2021-08-01 00:00:11</td>\n",
       "      <td>Dico a voi, #novax: guardate che anche la vita...</td>\n",
       "      <td>dico a voi #novax guardate che anche la vita √®...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           id               id_usr          created_at  \\\n",
       "10664431  1421621702924988418           2691538351 2021-08-01 00:00:01   \n",
       "10664432  1421621729185595400            632537304 2021-08-01 00:00:07   \n",
       "10664433  1421621731769393153  1252333704996888576 2021-08-01 00:00:07   \n",
       "10664434  1421621744251592706  1214181278334865408 2021-08-01 00:00:10   \n",
       "10664435  1421621745270902786             22602699 2021-08-01 00:00:11   \n",
       "\n",
       "                                                master_text  \\\n",
       "10664431                                 @FoxNews Vaccinate   \n",
       "10664432  Sono passati 5 giorni dopo la prima dose del v...   \n",
       "10664433  @AlfioKrancic Sito che lavora per nato e milit...   \n",
       "10664434  @Leonard48598239 S√¨ tu la fai parlare e non ri...   \n",
       "10664435  Dico a voi, #novax: guardate che anche la vita...   \n",
       "\n",
       "                                                 clean_text  \n",
       "10664431                                  foxnews vaccinate  \n",
       "10664432  sono passati   giorni dopo la prima dose del v...  \n",
       "10664433  alfiokrancic sito che lavora per nato e milita...  \n",
       "10664434  leonard  s√¨ tu la fai parlare e non rispondere...  \n",
       "10664435  dico a voi #novax guardate che anche la vita √®...  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw5_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5450244, 1731499)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tw5_df), len(tw5_df.drop_duplicates(subset=['master_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phraser\n",
    "In this section we add bigrams to the vocabulary: namely couple of words which often go together (i.e. \"green_pass\" or \"new_york\" etc...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.3 s, sys: 4.99 s, total: 41.3 s\n",
      "Wall time: 41.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "token_pattern = re.compile(r'(?u)\\b[A-Za-z]\\w+\\b')\n",
    "text_sentences = []\n",
    "#building the dictionary with unique pieces of text (in other words dropping duplicated on the clean_text column)\n",
    "for doc in tw5_df.clean_text.drop_duplicates():\n",
    "    text_sentences.extend([token_pattern.findall(sent.lower()) for sent in doc.split('\\n') if len(sent) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 12s, sys: 903 ms, total: 4min 13s\n",
      "Wall time: 4min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#min_count is the minimal nuimber of times that a single bigram has to appear in order to be considered a real bigram\n",
    "#threshold is linked to the probability of observing the words of the bigram together and the probability of \n",
    "#observing them separately\n",
    "phrases = Phrases(text_sentences, min_count=10, threshold=20., common_terms=stop_words) #, scoring='npmi')\n",
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the count vectorizer on the clean_text field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gcrupi/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:497: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'stop_words' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 1s, sys: 4.5 s, total: 2min 6s\n",
      "Wall time: 2min 6s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=<function phrase_analyzer at 0x7f3f30319550>,\n",
       "                max_df=0.5, min_df=10,\n",
       "                stop_words={'a', 'abbia', 'abbiamo', 'abbiano', 'abbiate',\n",
       "                            'about', 'above', 'ad', 'after', 'again', 'against',\n",
       "                            'agl', 'agli', 'ah', 'ai', 'ain', 'al', 'alcuni',\n",
       "                            'all', 'alla', 'alle', 'allo', 'allora', 'altre',\n",
       "                            'altri', 'altro', 'am', 'an', 'anche', 'ancora', ...})"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cv2 = feature_extraction.text.CountVectorizer(min_df=10, max_df=0.5, stop_words=stop_words, analyzer=phrase_analyzer)\n",
    "#building the vocabulary with unique pieces of text (in other words dropping duplicates in the clean_text column)\n",
    "cv2.fit(tw5_df.clean_text.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 53s, sys: 17.9 s, total: 7min 11s\n",
      "Wall time: 7min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#here I just obtain the matrix of counts of all the tweets, but with the vocabulary built with the unique \n",
    "#pieces of text only\n",
    "C2 = cv2.transform(tw5_df.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tw5_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5450244x100285 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 63817315 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/gcrupi/6_time_windows/sparse_matrices/top_model_timewindow/counts_vocabulary_preCOVID.joblib']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#saving the raw count matrix C2 and the vocabulary\n",
    "\n",
    "joblib.dump([C2,cv2], '/../data/counts_vocabulary_vi.joblib', compress=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
