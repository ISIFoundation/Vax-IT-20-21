{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Community Detection\n",
    "## In this notebook:\n",
    "* We run the following algorithm on each of the six networks. The algorithm:\n",
    "\n",
    "1. takes the graph G and samples it by removing half of the edges (or nodes) and extracting the GCC (let's call it wcc).\n",
    "2. runs hierarchical clustering algorithm paris on wcc and extract the dendrogram Z.\n",
    "3. cuts Z at 10 different heights (from 2 to 12 communities) and evaluates modularity for each of the 10 partitions.\n",
    "4. given the set of modularities it looks for the index of the 'jump'. We have a 'jump' in modularity when the value i+1 is at least 10% greater than value i.\n",
    "5. selects as best partition the partition of the index of the 'jump'.\n",
    "6. removes nodes belonging to small communities (i.e. communities with size < 100).\n",
    "7. At this point, what we have is: a set of nodes (wcc nodes. The ones sampled at the beginning) and a best_partition (a set of labels assigning each node to a community).\n",
    "8. steps from 1. to 6. are repeated 100 times. So we get 100 different set of nodes (from 10 independent samples of the same network) and 100 different best_partitions.\n",
    "\n",
    "* Then we plot experimental RMCA distribution. We define RMCA (Rate of the Most Common Assignment) as the normalized occurrence of the most common label of a certain node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user1</th>\n",
       "      <th>user2</th>\n",
       "      <th>weight</th>\n",
       "      <th>time_window</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3327831430</td>\n",
       "      <td>882965185237065728</td>\n",
       "      <td>159</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2535751839</td>\n",
       "      <td>80200885</td>\n",
       "      <td>145</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3327831430</td>\n",
       "      <td>80200885</td>\n",
       "      <td>143</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>273897097</td>\n",
       "      <td>4085520207</td>\n",
       "      <td>138</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1021332450893541376</td>\n",
       "      <td>469212336</td>\n",
       "      <td>132</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4366529</th>\n",
       "      <td>1089658546055204864</td>\n",
       "      <td>922049383436300288</td>\n",
       "      <td>2</td>\n",
       "      <td>vi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4366530</th>\n",
       "      <td>3244161467</td>\n",
       "      <td>1245427772991975432</td>\n",
       "      <td>2</td>\n",
       "      <td>vi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4366531</th>\n",
       "      <td>2902601895</td>\n",
       "      <td>1364298062919794688</td>\n",
       "      <td>2</td>\n",
       "      <td>vi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4366532</th>\n",
       "      <td>452013510</td>\n",
       "      <td>1245427772991975432</td>\n",
       "      <td>2</td>\n",
       "      <td>vi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4366533</th>\n",
       "      <td>2288557992</td>\n",
       "      <td>1245427772991975432</td>\n",
       "      <td>2</td>\n",
       "      <td>vi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1292968 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       user1                user2  weight time_window\n",
       "0                 3327831430   882965185237065728     159           i\n",
       "1                 2535751839             80200885     145           i\n",
       "2                 3327831430             80200885     143           i\n",
       "3                  273897097           4085520207     138           i\n",
       "4        1021332450893541376            469212336     132           i\n",
       "...                      ...                  ...     ...         ...\n",
       "4366529  1089658546055204864   922049383436300288       2          vi\n",
       "4366530           3244161467  1245427772991975432       2          vi\n",
       "4366531           2902601895  1364298062919794688       2          vi\n",
       "4366532            452013510  1245427772991975432       2          vi\n",
       "4366533           2288557992  1245427772991975432       2          vi\n",
       "\n",
       "[1292968 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = pd.read_csv('/../github_alltime_edgelist.csv').drop(['Unnamed: 0'],axis=1)\n",
    "t[t['weight'] > 1] #applying filter on edges' weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "usrs_net_df0 = t[t['time_window'] == 'i']\n",
    "usrs_net_df1 = t[t['time_window'] == 'ii']\n",
    "usrs_net_df2 = t[t['time_window'] == 'iii']\n",
    "usrs_net_df3 = t[t['time_window'] == 'iv']\n",
    "usrs_net_df4 = t[t['time_window'] == 'v']\n",
    "usrs_net_df5 = t[t['time_window'] == 'vi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I apply the filter on the weight. I discard links with weight = 1\n",
    "G0w = nx.from_pandas_edgelist(usrs_net_df0[usrs_net_df0['weight'] > 1], source='user1', target='user2', edge_attr=True, create_using=nx.DiGraph())\n",
    "G1w = nx.from_pandas_edgelist(usrs_net_df1[usrs_net_df1['weight'] > 1], source='user1', target='user2', edge_attr=True, create_using=nx.DiGraph())\n",
    "G2w = nx.from_pandas_edgelist(usrs_net_df2[usrs_net_df2['weight'] > 1], source='user1', target='user2', edge_attr=True, create_using=nx.DiGraph())\n",
    "G3w = nx.from_pandas_edgelist(usrs_net_df3[usrs_net_df3['weight'] > 1], source='user1', target='user2', edge_attr=True, create_using=nx.DiGraph())\n",
    "G4w = nx.from_pandas_edgelist(usrs_net_df4[usrs_net_df4['weight'] > 1], source='user1', target='user2', edge_attr=True, create_using=nx.DiGraph())\n",
    "G5w = nx.from_pandas_edgelist(usrs_net_df5[usrs_net_df5['weight'] > 1], source='user1', target='user2', edge_attr=True, create_using=nx.DiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del G0w,G1w,G2w,G3w,G4w,G5w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the GCC out of the network obtained by the application of the filter weight > 1\n",
    "wcc0_w = max(nx.weakly_connected_components(G0w), key=len) \n",
    "wcc0_w = G0w.subgraph(wcc0_w).copy()\n",
    "\n",
    "wcc1_w = max(nx.weakly_connected_components(G1w), key=len) \n",
    "wcc1_w = G1w.subgraph(wcc1_w).copy()\n",
    "\n",
    "wcc2_w = max(nx.weakly_connected_components(G2w), key=len) \n",
    "wcc2_w = G2w.subgraph(wcc2_w).copy()\n",
    "\n",
    "wcc3_w = max(nx.weakly_connected_components(G3w), key=len) \n",
    "wcc3_w = G3w.subgraph(wcc3_w).copy()\n",
    "\n",
    "wcc4_w = max(nx.weakly_connected_components(G4w), key=len) \n",
    "wcc4_w = G4w.subgraph(wcc4_w).copy()\n",
    "\n",
    "wcc5_w = max(nx.weakly_connected_components(G5w), key=len) \n",
    "wcc5_w = G5w.subgraph(wcc5_w).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 5528 , number of edges 18409\n",
      "Number of nodes: 4247 , number of edges 9054\n",
      "Number of nodes: 18967 , number of edges 80234\n",
      "Number of nodes: 59398 , number of edges 410515\n",
      "Number of nodes: 43325 , number of edges 318284\n",
      "Number of nodes: 44840 , number of edges 451118\n"
     ]
    }
   ],
   "source": [
    "N0wcc_w, N1wcc_w, N2wcc_w, N3wcc_w, N4wcc_w, N5wcc_w = wcc0_w.number_of_nodes(), wcc1_w.number_of_nodes(), wcc2_w.number_of_nodes(), wcc3_w.number_of_nodes(), wcc4_w.number_of_nodes(), wcc5_w.number_of_nodes()\n",
    "n0wcc_w, n1wcc_w, n2wcc_w, n3wcc_w, n4wcc_w, n5wcc_w = wcc0_w.number_of_edges(), wcc1_w.number_of_edges(), wcc2_w.number_of_edges(), wcc3_w.number_of_edges(), wcc4_w.number_of_edges(), wcc5_w.number_of_edges()\n",
    "\n",
    "print('i   -> Number of nodes:', N0wcc_w, ', number of edges', n0wcc_w)\n",
    "print('ii  -> Number of nodes:', N1wcc_w, ', number of edges', n1wcc_w)\n",
    "print('iii -> Number of nodes:', N2wcc_w, ', number of edges', n2wcc_w)\n",
    "print('iv  -> Number of nodes:', N3wcc_w, ', number of edges', n3wcc_w)\n",
    "print('v   -> Number of nodes:', N4wcc_w, ', number of edges', n4wcc_w)\n",
    "print('vi  -> Number of nodes:', N5wcc_w, ', number of edges', n5wcc_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92560.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total number of tweets in the preCOVID retweet network \n",
    "s = 0\n",
    "for e in wcc0_w.edges:\n",
    "    s+=wcc0_w.get_edge_data(*e)['weight']\n",
    "s    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29237.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 0\n",
    "for e in wcc1_w.edges:\n",
    "    s+=wcc1_w.get_edge_data(*e)['weight']\n",
    "s    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "348887.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 0\n",
    "for e in wcc2_w.edges:\n",
    "    s+=wcc2_w.get_edge_data(*e)['weight']\n",
    "s    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1911784.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 0\n",
    "for e in wcc3_w.edges:\n",
    "    s+=wcc3_w.get_edge_data(*e)['weight']\n",
    "s    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1596052.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 0\n",
    "for e in wcc4_w.edges:\n",
    "    s+=wcc4_w.get_edge_data(*e)['weight']\n",
    "s    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2322020.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 0\n",
    "for e in wcc5_w.edges:\n",
    "    s+=wcc5_w.get_edge_data(*e)['weight']\n",
    "s    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcc1_w['3327831430']['882965185237065728']['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying natural log to the weights of all the links\n",
    "for e in wcc0_w.edges:\n",
    "    wcc0_w.get_edge_data(*e)['weight'] = np.log(wcc0_w.get_edge_data(*e)['weight'])\n",
    "for e in wcc1_w.edges:\n",
    "    wcc1_w.get_edge_data(*e)['weight'] = np.log(wcc1_w.get_edge_data(*e)['weight'])\n",
    "for e in wcc2_w.edges:\n",
    "    wcc2_w.get_edge_data(*e)['weight'] = np.log(wcc2_w.get_edge_data(*e)['weight'])\n",
    "for e in wcc3_w.edges:\n",
    "    wcc3_w.get_edge_data(*e)['weight'] = np.log(wcc3_w.get_edge_data(*e)['weight'])\n",
    "for e in wcc4_w.edges:\n",
    "    wcc4_w.get_edge_data(*e)['weight'] = np.log(wcc4_w.get_edge_data(*e)['weight'])\n",
    "for e in wcc5_w.edges:\n",
    "    wcc5_w.get_edge_data(*e)['weight'] = np.log(wcc5_w.get_edge_data(*e)['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7376696182833684"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcc1_w['3327831430']['882965185237065728']['weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sknetwork.hierarchy import Paris, Ward\n",
    "from scipy.cluster.hierarchy import dendrogram, fcluster\n",
    "import random\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "import networkx.algorithms.community as nx_comm\n",
    "from itertools import combinations\n",
    "from statistics import mean\n",
    "from statistics import stdev\n",
    "import collections\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partition(clustering_array, G): #clustering_array represents a certain partition of the network G\n",
    "    #clustering array is a numpy array containing the indices of the communities to which each node is associated. \n",
    "    #For example, if there are just 2 communities, clustering_array is going to be something \n",
    "    #like [1,1,2,1,2, ... ,2,2,1,2]. So the first node goes into community 1, so does the second, \n",
    "    #the third node goes into community 2, etc...\n",
    "    #G is the network of which I want to get a partition\n",
    "    series = pd.Series(clustering_array)\n",
    "    series.index = G.nodes\n",
    "    communities = np.unique(clustering_array) #gets all the different communities\n",
    "    partition = []\n",
    "    for c in communities:\n",
    "        ind = series[series == c].index #indices assigned to the community c\n",
    "        partition.append(ind)\n",
    "    \n",
    "    return partition #it is something like [indices1, indices2, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given a list, this function returns the index of the value after the 'jump'\n",
    "#by 'jump' we mean a sudden increase of the values of the list\n",
    "#the jump must be at least perc% to be considered jump\n",
    "\n",
    "def jump_detector(l, hc_alg, perc, min_val = 0.4):\n",
    "    s = pd.Series(l)\n",
    "    #print(s)\n",
    "    opt = 0\n",
    "    for i in s.index:\n",
    "        if s[i] < min_val: #sometimes modularity starts from zero, increases to ~0.4 and then has another jump to\n",
    "            #~0.5, so we want to keep the second jump and not the first one.\n",
    "            continue\n",
    "        if hc_alg == 'ward':\n",
    "            if s[i+1] < s[i]:\n",
    "                opt = i\n",
    "                return opt\n",
    "\n",
    "            elif (s[i+1]-s[i])/s[i] > perc: #if finds a suitable big enough jump the function returns the index of the jump\n",
    "                \n",
    "                opt = i + 1\n",
    "                return opt\n",
    "            \n",
    "        elif hc_alg == 'paris':\n",
    "            \n",
    "            if (s[i+1]-s[i])/s[i] > perc: #if finds a suitable big enough jump the function returns the index of the jump\n",
    "                \n",
    "                opt = i + 1\n",
    "                return opt\n",
    "\n",
    "        if i==len(s)-2 and opt==0:#if gets to the end without finding a jump it takes the index of the minimum \n",
    "            #value grater than the threshold min_val\n",
    "\n",
    "            s = s[s>min_val]\n",
    "\n",
    "            opt = min(s.index)\n",
    "            return opt\n",
    "            \n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the modularities associated to each partition present in clustering_arrays\n",
    "def get_modularities(clustering_arrays,G):\n",
    "    mod_array = []\n",
    "    for c_a in clustering_arrays:\n",
    "        partition = get_partition(c_a,G)\n",
    "        mod = nx_comm.modularity(G,partition)\n",
    "        mod_array.append(mod)\n",
    "    \n",
    "    return mod_array\n",
    "\n",
    "#here is where the hierarchical clustering algorithm runs on GCC of the sampled network\n",
    "#then the dendrogram is cut at 10 different heights and the corresponding clustering_arrays are put together\n",
    "#then we evaluate the modularity of each partition (ie clustering_array)\n",
    "#then we select the index of the jump\n",
    "def dendrogram_and_jumpidx(G,hc_alg, min_val, perc):\n",
    "    if hc_alg == 'paris':\n",
    "        paris_ward = Paris()\n",
    "    \n",
    "    elif hc_alg == 'ward':\n",
    "        paris_ward = Ward()\n",
    "        \n",
    "    else:\n",
    "        print(hc_alg,': unknown clustering algorithm')\n",
    "        return\n",
    "    \n",
    "    adj_m = nx.adjacency_matrix(G)\n",
    "    Z0 = paris_ward.fit_transform(adj_m)\n",
    "    \n",
    "\n",
    "    clust_arrays = []\n",
    "    for ti in range(2,11,1):    \n",
    "        clust_array = fcluster(Z=Z0,t=ti,criterion='maxclust')\n",
    "        clust_arrays.append(clust_array)\n",
    "\n",
    "    \n",
    "    mods = get_modularities(clust_arrays,G)\n",
    "    jd = jump_detector(mods,hc_alg=hc_alg, min_val=min_val, perc= perc)\n",
    "    \n",
    "\n",
    "    \n",
    "    cab = fcluster(Z=Z0,t= jd+2, criterion='maxclust') #clust_array_best\n",
    "    \n",
    "    return cab ,jd, mods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes half random links from a network, then returns the GCC of the remaning network\n",
    "def remove_edges(g, fraction): #with fraction=2 we remove half of the links\n",
    "    G = g.copy()\n",
    "    n = G.number_of_edges()\n",
    "    to_remove=random.sample(G.edges(), k=int(n/fraction))\n",
    "    G.remove_edges_from(to_remove)\n",
    "    \n",
    "    wcc = max(nx.weakly_connected_components(G), key=len) \n",
    "    wcc = G.subgraph(wcc).copy()\n",
    "\n",
    "    return wcc\n",
    "\n",
    "def remove_nodes(g, fraction): #with fraction=2 we remove half of the links\n",
    "    G = g.copy()\n",
    "    N = G.number_of_nodes()\n",
    "    to_remove=random.sample(G.nodes(), k=int(N/fraction))\n",
    "    G.remove_nodes_from(to_remove)\n",
    "    \n",
    "    wcc = max(nx.weakly_connected_components(G), key=len) \n",
    "    wcc = G.subgraph(wcc).copy()\n",
    "\n",
    "    return wcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes as keys the nodes of a certain GCC and as values the list containing the label of the community to which\n",
    "#each node belongs to. thr is the threshold for small communities. All the communities smaller than threshold are \n",
    "#removed (ie all the nodes belonging to the small community are removed).\n",
    "def remove_small_communities(keys, values, thr):\n",
    "    label_dict = dict(zip(keys, values)) #creates a dict with keys: nodes, values: community label\n",
    "    labels = label_dict.values() #takes the list of the labels\n",
    "    \n",
    "    bins = np.bincount(list(labels))[1:] #evaluates how many nodes there are in each community\n",
    "    \n",
    "    to_remove = []\n",
    "    for i in range(len(bins)): #append the labels to remove to a list\n",
    "        if bins[i] < thr:\n",
    "            to_remove.append(i+1)\n",
    "            \n",
    "    for l in to_remove: #removes the labels present in the list to_remove\n",
    "        label_dict = dict((k, v) for k, v in label_dict.items() if v != l)\n",
    "    \n",
    "    #after removing small communities\n",
    "    #we want the community to be ordered by size (ie the smallest community is always labeled 1 and the biggest\n",
    "    #is always labeled N, where N is the number of communities.)\n",
    "    ll = np.arange(1,len(set(label_dict.values())) + 1, 1) #it is just an array like [1,2, ... , N]\n",
    "    \n",
    "    #we want set(dict.values) to be equal to ll, because sometimes we have for example 4 communities and the third \n",
    "    #is 'small' then it is removed and so set(dict.values) is {1,2,4} when we want it to be {1,2,3}.\n",
    "    chs = changes_to_make(list(set(label_dict.values())),list(ll))#gets the changes to transform set(dict.values)\n",
    "    #into [1,2,...,N]\n",
    "    \n",
    "    s = pd.Series(data=label_dict.values(), index=label_dict.keys())#turns the dict into a series to use 'replace'\n",
    "    \n",
    "    for ch in chs: #renames communities to be renamed\n",
    "        s = s.replace(ch[0],'x')\n",
    "        s = s.replace(ch[1],'y')\n",
    "            \n",
    "        s = s.replace('x', ch[1])\n",
    "        s = s.replace('y', ch[0])\n",
    "            \n",
    "    \n",
    "    label_dict = s.to_dict() #back to dict\n",
    "    \n",
    "\n",
    "    return label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes to lists and returns the changes (ie the switchs of elements) to make l equal to true_l\n",
    "def changes_to_make(l,true_l):\n",
    "    s = pd.Series(l)\n",
    "    changes = []\n",
    "\n",
    "    while list(s) != true_l:\n",
    "\n",
    "        chs = [(i,j) for i,j in zip(list(s),true_l)]\n",
    "        for i in range(len(chs)):\n",
    "            if chs[i][0] == chs[i][1]:\n",
    "                continue\n",
    "            \n",
    "            change = chs[i]\n",
    "\n",
    "            s = s.replace(change[0],'x')\n",
    "            s = s.replace(change[1],'y')\n",
    "            \n",
    "            s = s.replace('x', change[1])\n",
    "            s = s.replace('y', change[0])\n",
    "            \n",
    "            changes.append(change)\n",
    "            break\n",
    "            \n",
    "    return changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes a list of labels [1,1,2,4,5,2,1,...] and a list of changes to make. For example we may want to switch\n",
    "#(2,1) or (1,3) etc.\n",
    "#returns the list of labels with the changes made.\n",
    "def lab_adjuster(labels,changes):\n",
    "    \n",
    "    s = pd.Series(labels) \n",
    "    \n",
    "    for change in changes: #renames community according to changes\n",
    "        s = s.replace(change[0],'x')\n",
    "        s = s.replace(change[1],'y')\n",
    "            \n",
    "        s = s.replace('x', change[1])\n",
    "        s = s.replace('y', change[0])\n",
    "    \n",
    "    return list(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#every tuple contains (nodes of the sample, ordered label). The labels are now ordered by community size.\n",
    "#so if a node belongs to the smallest community it is labeled 1, if it belongs to the biggest community it is \n",
    "#labeled N.\n",
    "#returns a dictionary like: keys: nodes, values: list containing all the labels of that nodes in different samples.\n",
    "def nodes_dictionary(tuples):\n",
    "    nodes_dict = {}\n",
    "    for tup in tuples:\n",
    "        for node, label in zip(tup[0],tup[1]):\n",
    "\n",
    "            nodes_dict.setdefault(node,[])\n",
    "            nodes_dict[node].append(label)\n",
    "            \n",
    "    return nodes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes two different sets of nodes of the same network which was sampled twice\n",
    "#and two clust_arrays containing the assignment to the community of the nodes in each set of nodes\n",
    "#finds common nodes and puts into a df each common node (index) and the two different assignment from the two \n",
    "#different partitions\n",
    "\n",
    "def common_nodes_label(nodes1,nodes2,clust_array1, clust_array2):\n",
    "    \n",
    "    zip_iterator1 = zip(nodes1, clust_array1)\n",
    "    zip_iterator2 = zip(nodes2, clust_array2)\n",
    "    \n",
    "    dict1 = dict(zip_iterator1)\n",
    "    dict2 = dict(zip_iterator2)\n",
    "    \n",
    "    common_nodes = list(set(nodes1).intersection(set(nodes2)))\n",
    "    \n",
    "    new_dict1 = {your_key: dict1[your_key] for your_key in common_nodes}\n",
    "    new_dict2 = {your_key: dict2[your_key] for your_key in common_nodes}\n",
    "    \n",
    "    df = pd.DataFrame(index=common_nodes)\n",
    "    df['d1'] = new_dict1.values()\n",
    "    df['d2'] = new_dict2.values()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the sum of non-diagonal elements of a matrix\n",
    "def sum_non_diagonal(mat):\n",
    "    rw, cl = mat.shape\n",
    "    dia = np.diag_indices(min(rw,cl)) #indices of diagonal elements\n",
    "    dia_sum = sum(mat[dia]) # sum of diagonal elements\n",
    "    off_dia_sum = np.sum(mat) - dia_sum # subtract the diagonal sum from total array sum\n",
    "    return off_dia_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#swaps two rows (x,y) of a given matrix a\n",
    "def swap_rows(a,x,y):\n",
    "    a[[x, y]] = a[[y, x]]\n",
    "    return a\n",
    "\n",
    "#takes a matrix and returns the couples of rows to be swapped in order to make that matrix diagonal\n",
    "def changes_to_make2(mx):\n",
    "    changes = []\n",
    "    \n",
    "    rows, columns = mx.shape\n",
    "    \n",
    "    if sum_non_diagonal(mx) == 0.:\n",
    "        return changes\n",
    "    \n",
    "    else:\n",
    "        while sum_non_diagonal(mx) != 0.:\n",
    "            for r in range(rows):\n",
    "                am = np.argmax(mx[r]) #argmax of the row r\n",
    "                if am != r:\n",
    "                    changes.append((r+1,am+1))\n",
    "                    mx = swap_rows(mx,am,r)\n",
    "                    if sum_non_diagonal(mx) == 0.:\n",
    "                        return changes\n",
    "                \n",
    "                else:\n",
    "                    continue\n",
    "                        \n",
    "    return changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes a confusion matrix and returns the changes to make in order to make the matrix have the larger value of each \n",
    "#row on the diagonal\n",
    "def go_through_cm(cm):\n",
    "    \n",
    "    if cm.shape != cm[~np.all(cm == 0, axis=1)].shape:\n",
    "        print('!')\n",
    "    \n",
    "    cm = cm[~np.all(cm == 0, axis=1)] #removing rows (or columns with all zeroes)\n",
    "    \n",
    "    cm = cm.transpose()\n",
    "    \n",
    "    if cm.shape != cm[~np.all(cm == 0, axis=1)].shape:\n",
    "        print('!!')\n",
    "    \n",
    "    cm = cm[~np.all(cm == 0, axis=1)]\n",
    "    \n",
    "    cm = cm.transpose()\n",
    "    \n",
    "    rows, columns = cm.shape\n",
    "    \n",
    "    I = np.zeros(rows*columns).reshape(rows,columns)\n",
    "    max_ind = cm.argmax(1) #list saying what is the index of the max in each row\n",
    "    \n",
    "    for i in range(len(max_ind)): #creates I with 1 in the argmax position and zero elsewhere\n",
    "        \n",
    "        ind = max_ind[i]\n",
    "        \n",
    "        I[i][ind] = 1\n",
    "\n",
    "    I = I.transpose() #so that I have to switch rows and not columns\n",
    "    return changes_to_make2(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given a network it samples the network, does Paris hierarchical clustering, assigns a label to the nodes \n",
    "#of the sample. Repeats this for N times and return a dictionary having keys: nodes' id and values: a list\n",
    "#containing the labels of the nodes. The length of the lists is going to be <= N.\n",
    "def partitions_dict2(g, fraction=2, edges=True, N=100, hc_alg = 'paris', min_val = .4, perc=0.1, thr=100):\n",
    "    \n",
    "    tuples = []\n",
    "    \n",
    "    for i in range(N): #we want N different samples of the network\n",
    "        sys.stdout.write(f'\\r{i+1}/{N}')\n",
    "        \n",
    "        if edges:\n",
    "            wcc = remove_edges(g,fraction)\n",
    "        else:\n",
    "            wcc = remove_nodes(g,fraction)\n",
    "        \n",
    "        \n",
    "        clust_array_best, _, _ = dendrogram_and_jumpidx(wcc, hc_alg, min_val, perc=perc)#for each of the samples \n",
    "        #we run paris and obtain the dendrogram (Z0). We obtain the clust_array_best (ie the partition after the \n",
    "        #jump in modularity) \n",
    "        #we also get the jump_index to keep track of it\n",
    "        \n",
    "        #removes the nodes belonging to small communities (ie community size < 100)\n",
    "        \n",
    "        dic = remove_small_communities(wcc.nodes(),clust_array_best, thr=thr)\n",
    "        \n",
    "        nodes, labels = list(dic.keys()), list(dic.values())\n",
    "        \n",
    "        if i == 0:\n",
    "            ref_dict = dic\n",
    "        print(np.bincount(list(dic.values()))[1:], len(np.bincount(list(dic.values()))[1:]))\n",
    "        if len(np.bincount(list(dic.values()))[1:]) < 2:\n",
    "            print('continue')\n",
    "            continue\n",
    "\n",
    "        \n",
    "        df = common_nodes_label(ref_dict.keys(),dic.keys(),ref_dict.values(),dic.values())\n",
    "        \n",
    "        cm = confusion_matrix(df['d1'],df['d2'])\n",
    "        \n",
    "        changes = go_through_cm(cm)\n",
    "        labels_adj = lab_adjuster(labels,changes)\n",
    "        \n",
    "        #we add to tuples the tuple (nodes of the sampled network,best assignment to communities of these nodes)\n",
    "        tuples.append((nodes,labels_adj))\n",
    "        \n",
    "    return nodes_dictionary(tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given a network it samples the nwtwork, does Paris hierarchical clustering, assigns a label to the nodes \n",
    "#of the sample. Repeats this for N times and return a dictionary having keys: nodes' id and values: a list\n",
    "#containing the labels of the nodes. The length of the lists is going to be <= N.\n",
    "def partitions_dict5(g, fraction=2, edges=True, N=100, hc_alg = 'paris', min_val = .4, perc=0.1, thr=100):\n",
    "    \n",
    "    tuples = []\n",
    "    i=0\n",
    "    while i < N: #we want N different samples of the network\n",
    "        sys.stdout.write(f'\\r{i+1}/{N}')\n",
    "        \n",
    "        if edges:\n",
    "            wcc = remove_edges(g,fraction)\n",
    "        else:\n",
    "            wcc = remove_nodes(g,fraction)\n",
    "        \n",
    "        \n",
    "        clust_array_best, _, _ = dendrogram_and_jumpidx(wcc, hc_alg, min_val, perc=perc)#for each of the samples \n",
    "        #we run paris and obtain the dendrogram (Z0). We obtain the clust_array_best (ie the partition after the \n",
    "        #jump in modularity) \n",
    "        #we also get the jump_index to keep track of it\n",
    "        \n",
    "        #removes the nodes belonging to small communities (ie community size < 100)\n",
    "        \n",
    "        dic = remove_small_communities(wcc.nodes(),clust_array_best, thr=thr)\n",
    "        \n",
    "        nodes, labels = list(dic.keys()), list(dic.values())\n",
    "        \n",
    "        if i == 0:\n",
    "            ref_dict = dic\n",
    "        print(np.bincount(list(dic.values()))[1:], len(np.bincount(list(dic.values()))[1:]))\n",
    "        if len(np.bincount(list(dic.values()))[1:]) < 2:\n",
    "            print('continue')\n",
    "            continue\n",
    "\n",
    "        \n",
    "        df = common_nodes_label(ref_dict.keys(),dic.keys(),ref_dict.values(),dic.values())\n",
    "        \n",
    "        cm = confusion_matrix(df['d1'],df['d2'])\n",
    "        \n",
    "        changes = go_through_cm(cm)\n",
    "        labels_adj = lab_adjuster(labels,changes)\n",
    "        \n",
    "        #we add to tuples the tuple (nodes of the sampled network,best assignment to communities of these nodes)\n",
    "        tuples.append((nodes,labels_adj))\n",
    "        \n",
    "        i+=1\n",
    "        \n",
    "    return nodes_dictionary(tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict(dic, path=''):\n",
    "    with open(path, 'w') as csv_file:  \n",
    "        writer = csv.writer(csv_file)\n",
    "        for key, value in dic.items():\n",
    "            writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preCOVID network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Modularity jump percentage (perc) = 0.1\n",
    "part_dict0 = partitions_dict2(wcc0_w, N=100, perc=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('preCOVID network number of nodes:', wcc0_w.number_of_nodes())\n",
    "print('preCOVID network covered nodes:', len(part_dict0))\n",
    "print('preCOVID fraction of covered nodes:',len(part_dict0)/wcc0_w.number_of_nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## earlyCOVID network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/100[903 828 841] 3\n",
      "2/100[811 804 981] 3\n",
      "3/100[872 846 905] 3\n",
      "4/100[924 804 885] 3\n",
      "5/100[965 850 832] 3\n",
      "6/100[820 811 956] 3\n",
      "7/100[926 814 814] 3\n",
      "8/100[ 815 1018  797] 3\n",
      "9/100[958 811 832] 3\n",
      "10/100[789 981 834] 3\n",
      "11/100[926 810 834] 3\n",
      "12/100[938 841 795] 3\n",
      "13/100[879 831 880] 3\n",
      "14/100[951 803 834] 3\n",
      "15/100[945 797 830] 3\n",
      "16/100[976 789 829] 3\n",
      "17/100[940 820 822] 3\n",
      "18/100[927 801 834] 3\n",
      "19/100[920 842 846] 3\n",
      "20/100[952 845 821] 3\n",
      "21/100[952 812 821] 3\n",
      "22/100[922 840 812] 3\n",
      "23/100[808 919 879] 3\n",
      "24/100[895 818 832] 3\n",
      "25/100[861 786 865] 3\n",
      "26/100[954 829 829] 3\n",
      "27/100[957 820 847] 3\n",
      "28/100"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-fe8ad80326cc>\u001b[0m in \u001b[0;36mpartitions_dict2\u001b[0;34m(g, fraction, edges, N, hc_alg, min_val, perc, thr)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mclust_array_best\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdendrogram_and_jumpidx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhc_alg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mperc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#for each of the samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;31m#we run paris and obtain the dendrogram (Z0). We obtain the clust_array_best (ie the partition after the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m#jump in modularity)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-9b07e2d37387>\u001b[0m in \u001b[0;36mdendrogram_and_jumpidx\u001b[0;34m(G, hc_alg, min_val, perc)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m#partitions.append(np.bincount(clust_array)[1:])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mmods\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_modularities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclust_arrays\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mjd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjump_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmods\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhc_alg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhc_alg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperc\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mperc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-9b07e2d37387>\u001b[0m in \u001b[0;36mget_modularities\u001b[0;34m(clustering_arrays, G)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc_a\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclustering_arrays\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mpartition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_partition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_a\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx_comm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodularity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mmod_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/networkx/algorithms/community/quality.py\u001b[0m in \u001b[0;36mmodularity\u001b[0;34m(G, communities, weight)\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mL_c\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mout_degree_sum\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0min_degree_sum\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommunity_contribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommunities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/networkx/algorithms/community/quality.py\u001b[0m in \u001b[0;36mcommunity_contribution\u001b[0;34m(community)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcommunity_contribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommunity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mcomm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommunity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0mL_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0mout_degree_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_degree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/networkx/algorithms/community/quality.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcommunity_contribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommunity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mcomm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommunity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0mL_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0mout_degree_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_degree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "part_dict1 = partitions_dict2(wcc1_w, N=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('earlyCOVID network number of nodes:', wcc1_w.number_of_nodes())\n",
    "print('earlyCOVID network covered nodes:', len(part_dict1))\n",
    "print('earlyCOVID fraction of covered nodes:',len(part_dict1)/wcc1_w.number_of_nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preVAX network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Modularity jump percentage (perc) = 0.1\n",
    "part_dict2 = partitions_dict2(wcc2_w, N=10, perc=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('preVAX network number of nodes:', wcc2_w.number_of_nodes())\n",
    "print('preVAX network covered nodes:', len(part_dict2))\n",
    "print('preVAX fraction of covered nodes:',len(part_dict2)/wcc2_w.number_of_nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## earlyVAX network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/100[12551 30504] 2\n",
      "2/100[30802 12242] 2\n",
      "3/100[12344 30560] 2\n",
      "4/100[12477 30046] 2\n",
      "5/100[29826 12772] 2\n",
      "6/100[30465 12398] 2\n",
      "7/100[12392 30554] 2\n",
      "8/100[12495 30539] 2\n",
      "9/100[30537 12327] 2\n",
      "10/100[11915 31063] 2\n",
      "11/100[30424 12571] 2\n",
      "12/100[30035 12939] 2\n",
      "13/100[30743 12120] 2\n",
      "14/100[12688 29978] 2\n",
      "15/100[30058 12983] 2\n",
      "16/100[30631 12505] 2\n",
      "17/100[12325 30702] 2\n",
      "18/100[30548 12346] 2\n",
      "19/100[12395 30497] 2\n",
      "20/100[13025 29893] 2\n",
      "21/100[12770 29654] 2\n",
      "22/100[30445 12486] 2\n",
      "23/100[12219 30761] 2\n",
      "24/100[30599 12483] 2\n",
      "25/100[12087 30975] 2\n",
      "26/100[30824 12098] 2\n",
      "27/100[12374 30186] 2\n",
      "28/100[30235 12897] 2\n",
      "29/100[12379 30321] 2\n",
      "30/100[30364 12485] 2\n",
      "31/100[30902 12245] 2\n",
      "32/100[13942 29032] 2\n",
      "33/100[31270 11729] 2\n",
      "34/100[12056 30914] 2\n",
      "35/100[30699 12279] 2\n",
      "36/100[12280 30509] 2\n",
      "37/100[30719 12073] 2\n",
      "38/100[30292 12483] 2\n",
      "39/100[29763 13196] 2\n",
      "40/100[31006 12030] 2\n",
      "41/100[30778 12212] 2\n",
      "42/100[12597 29914] 2\n",
      "43/100[30820 12413] 2\n",
      "44/100[30645 12210] 2\n",
      "45/100[30122 12801] 2\n",
      "46/100[29935 13123] 2\n",
      "47/100[30277 12739] 2\n",
      "48/100[30887 12008] 2\n",
      "49/100[12457 30552] 2\n",
      "50/100[12995 29555] 2\n",
      "51/100[12798 30225] 2\n",
      "52/100[13553 29490] 2\n",
      "53/100[12359 30668] 2\n",
      "54/100[30485 12735] 2\n",
      "55/100[31209 11793] 2\n",
      "56/100[30607 12471] 2\n",
      "57/100[12392 30538] 2\n",
      "58/100[11810 30692] 2\n",
      "59/100[31018 12105] 2\n",
      "60/100[30339 12616] 2\n",
      "61/100[30812 12080] 2\n",
      "62/100[30075 12904] 2\n",
      "63/100[11955 31004] 2\n",
      "64/100[30868 12299] 2\n",
      "65/100[30054 12845] 2\n",
      "66/100[30480 12576] 2\n",
      "67/100[30532 12493] 2\n",
      "68/100[12868 30134] 2\n",
      "69/100[30159 12817] 2\n",
      "70/100[31261 11907] 2\n",
      "71/100[11752 31092] 2\n",
      "72/100[12798 30264] 2\n",
      "73/100[12294 30570] 2\n",
      "74/100[30280 12619] 2\n",
      "75/100[30192 12766] 2\n",
      "76/100[30499 12550] 2\n",
      "77/100[31054 12033] 2\n",
      "78/100[30458 12565] 2\n",
      "79/100[12171 30717] 2\n",
      "80/100[11859 31003] 2\n",
      "81/100[13890 29176] 2\n",
      "82/100[31119 11947] 2\n",
      "83/100[30585 12400] 2\n",
      "84/100[30699 12456] 2\n",
      "85/100[12671 30377] 2\n",
      "86/100[30802 12322] 2\n",
      "87/100[30372 12519] 2\n",
      "88/100[30354 12620] 2\n",
      "89/100[30862 12137] 2\n",
      "90/100[30741 12388] 2\n",
      "91/100[30930 11901] 2\n",
      "92/100[30098 12953] 2\n",
      "93/100[30739 12200] 2\n",
      "94/100[30643 12340] 2\n",
      "95/100[30325 12659] 2\n",
      "96/100[30596 12391] 2\n",
      "97/100[31159 11920] 2\n",
      "98/100[30796 12235] 2\n",
      "99/100[30226 12776] 2\n",
      "100/100[12080 30866] 2\n",
      "CPU times: user 55min 52s, sys: 4.05 s, total: 55min 56s\n",
      "Wall time: 55min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "part_dict3 = partitions_dict2(wcc3_w, N=100, thr=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "earlyVAX network number of nodes: 59398\n",
      "earlyVAX network covered nodes: 59374\n",
      "earlyVAX fraction of covered nodes: 0.9995959459914475\n"
     ]
    }
   ],
   "source": [
    "print('earlyVAX network number of nodes:', wcc3_w.number_of_nodes())\n",
    "print('earlyVAX network covered nodes:', len(part_dict3))\n",
    "print('earlyVAX fraction of covered nodes:',len(part_dict3)/wcc3_w.number_of_nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAXdrive network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/100[12137 18835] 2\n",
      "2/100[19438 11455] 2\n",
      "3/100[12443 18514] 2\n",
      "4/100[18691 12312] 2\n",
      "5/100[11662 19282] 2\n",
      "6/100[19343 11553] 2\n",
      "7/100[18722 12170] 2\n",
      "8/100[19357 11696] 2\n",
      "9/100[12551 18450] 2\n",
      "10/100[19020 12004] 2\n",
      "11/100[12197 18779] 2\n",
      "12/100[11629 19574] 2\n",
      "13/100[19453 11713] 2\n",
      "14/100[18846 12111] 2\n",
      "15/100[18965 12019] 2\n",
      "16/100[18500 12414] 2\n",
      "17/100[18983 11984] 2\n",
      "18/100[19138 11792] 2\n",
      "19/100[19043 11962] 2\n",
      "20/100[18655 12367] 2\n",
      "21/100[18679 12271] 2\n",
      "22/100[18839 12094] 2\n",
      "23/100[18954 12103] 2\n",
      "24/100[18606 12246] 2\n",
      "25/100[18986 12120] 2\n",
      "26/100[18571 12432] 2\n",
      "27/100[18473 12448] 2\n",
      "28/100[19178 11804] 2\n",
      "29/100[18946 12121] 2\n",
      "30/100[18857 12088] 2\n",
      "31/100[18812 12191] 2\n",
      "32/100[18912 12023] 2\n",
      "33/100[18666 12220] 2\n",
      "34/100[18997 12036] 2\n",
      "35/100[19156 11813] 2\n",
      "36/100[12249 18594] 2\n",
      "37/100[18861 12169] 2\n",
      "38/100[18664 12320] 2\n",
      "39/100[19068 12008] 2\n",
      "40/100[  147 18577 12245] 3\n",
      "!\n",
      "41/100[18582 12307] 2\n",
      "42/100[18634 12365] 2\n",
      "43/100[12677 18390] 2\n",
      "44/100[12136 18813] 2\n",
      "45/100[19024 12056] 2\n",
      "46/100[18799 12088] 2\n",
      "47/100[18635 12327] 2\n",
      "48/100[12111 19015] 2\n",
      "49/100[18626 12332] 2\n",
      "50/100[18773 12221] 2\n",
      "51/100[18985 12058] 2\n",
      "52/100[19031 11781] 2\n",
      "53/100[12198 18805] 2\n",
      "54/100[18968 12086] 2\n",
      "55/100[19265 11688] 2\n",
      "56/100[19108 11915] 2\n",
      "57/100[19017 12014] 2\n",
      "58/100[19297 11767] 2\n",
      "59/100[19103 12014] 2\n",
      "60/100[18832 12151] 2\n",
      "61/100[18713 12142] 2\n",
      "62/100[18908 12095] 2\n",
      "63/100[19149 11975] 2\n",
      "64/100[11626 19333] 2\n",
      "65/100[18785 12148] 2\n",
      "66/100[13231 17770] 2\n",
      "67/100[18929 11905] 2\n",
      "68/100[19356 11793] 2\n",
      "69/100[18453 12564] 2\n",
      "70/100[18975 11966] 2\n",
      "71/100[18895 12187] 2\n",
      "72/100[18396 12742] 2\n",
      "73/100[18881 12229] 2\n",
      "74/100[11700 19363] 2\n",
      "75/100[18761 12250] 2\n",
      "76/100[18972 12062] 2\n",
      "77/100[19306 11696] 2\n",
      "78/100[11761 19274] 2\n",
      "79/100[12080 18840] 2\n",
      "80/100[19013 11995] 2\n",
      "81/100[18164 12765] 2\n",
      "82/100[19035 12077] 2\n",
      "83/100[11793 19167] 2\n",
      "84/100[18903 12080] 2\n",
      "85/100[11845 19352] 2\n",
      "86/100[18522 12403] 2\n",
      "87/100[18853 12004] 2\n",
      "88/100[18860 12125] 2\n",
      "89/100[19138 12150] 2\n",
      "90/100[12164 18730] 2\n",
      "91/100[19400 11516] 2\n",
      "92/100[18952 12081] 2\n",
      "93/100[18745 12286] 2\n",
      "94/100[12249 18818] 2\n",
      "95/100[18992 11774] 2\n",
      "96/100[18625 12255] 2\n",
      "97/100[18802 12110] 2\n",
      "98/100[19099 11952] 2\n",
      "99/100[19160 11932] 2\n",
      "100/100[19450 11568] 2\n",
      "CPU times: user 36min 48s, sys: 2.13 s, total: 36min 50s\n",
      "Wall time: 36min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "part_dict4 = partitions_dict2(wcc4_w, N=100, thr=100, min_val=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAXdrive network number of nodes: 43325\n",
      "VAXdrive network covered nodes: 43325\n",
      "VAXdrive fraction of covered nodes: 1.0\n"
     ]
    }
   ],
   "source": [
    "print('VAXdrive network number of nodes:', wcc4_w.number_of_nodes())\n",
    "print('VAXdrive network covered nodes:', len(part_dict4))\n",
    "print('VAXdrive fraction of covered nodes:',len(part_dict4)/wcc4_w.number_of_nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lateVAX network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "part_dict5 = partitions_dict5(wcc5_w, N=100, thr=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('lateVAX network number of nodes:', wcc5_w.number_of_nodes())\n",
    "print('lateVAX network covered nodes:', len(part_dict5))\n",
    "print('lateVAX fraction of covered nodes:',len(part_dict5)/wcc5_w.number_of_nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a list containing the number of times that each node was sampled (number of appereances of each node)\n",
    "def number_of_appearances(dic):\n",
    "    new_dict = {}\n",
    "    for k in dic.keys():\n",
    "        new_dict.setdefault(k,len(dic[k]))\n",
    "    return list(new_dict.values())\n",
    "\n",
    "def number_of_appearances2(dic):\n",
    "    new_dict = {}\n",
    "    for k in dic.keys():\n",
    "        f = dic[k].split(',')\n",
    "        new_dict.setdefault(k,len(f))\n",
    "    return list(new_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each node ia associated with a list of labels.\n",
    "#this function returns the list of the percentage most common label of each node\n",
    "#so if a node has a list like [1,1,3,1], the percentage associated will be 0.75\n",
    "def rmc(dic): #RMCA\n",
    "    new_dict = {}\n",
    "    for k in dic.keys():\n",
    "        e = len(dic[k])\n",
    "        f = dic[k][1:e-1]\n",
    "        \n",
    "        f = f.split(', ')\n",
    "        \n",
    "        c = collections.Counter(f)\n",
    "        x = c.most_common()[0][1]\n",
    "        \n",
    "        new_dict.setdefault(k,float(\"{0:.3f}\".format(x/len(f))))\n",
    "    return list(new_dict.values()), new_dict\n",
    "\n",
    "#returns a dict with keys->nodes and values-> most common community attributed to the node\n",
    "def most_common_label(dic):\n",
    "    new_dict = {}\n",
    "    for k in dic.keys():\n",
    "        c = collections.Counter(dic[k][1:-1].split(', '))\n",
    "        x = c.most_common()[0][0]\n",
    "        new_dict.setdefault(k,x)\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c6d374f4ed77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0maxs_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtit\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhist_bins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m21\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2, 3, sharey=True, sharex=True, figsize=(6,4))\n",
    "axs_l = axs.flatten()\n",
    "ax = 10\n",
    "tit= 12\n",
    "hist_bins = np.linspace(0.5, 1.0, 21)\n",
    "\n",
    "l, _ = rmc(part_dict0)\n",
    "axs_l[0].hist(l, hist_bins, color='grey')\n",
    "axs_l[0].set_yscale('log')\n",
    "axs_l[0].grid()\n",
    "\n",
    "l, _ = rmc(part_dict1)\n",
    "axs_l[1].hist(l, hist_bins, color='grey')\n",
    "axs_l[1].set_yscale('log')\n",
    "axs_l[1].grid()\n",
    "\n",
    "l, _ = rmc(part_dict2)\n",
    "axs_l[2].hist(l, hist_bins, color='grey')                   \n",
    "axs_l[2].set_yscale('log')\n",
    "axs_l[2].grid()\n",
    "\n",
    "l, _ = rmc(part_dict3)\n",
    "axs_l[3].grid()\n",
    "axs_l[3].hist(l, hist_bins, color='grey')\n",
    "axs_l[3].set_yscale('log')\n",
    "\n",
    "l, _ = rmc(part_dict4)\n",
    "axs_l[4].grid()\n",
    "axs_l[4].hist(l, hist_bins, color='grey')                                     \n",
    "axs_l[4].set_yscale('log')\n",
    "\n",
    "l, _ = rmc(part_dict5)\n",
    "axs_l[5].grid()\n",
    "axs_l[5].hist(l, hist_bins, color='grey')\n",
    "axs_l[5].set_yscale('log')\n",
    "\n",
    "axs_l[0].set_ylabel('number of nodes',fontsize=ax)\n",
    "axs_l[3].set_ylabel('number of nodes',fontsize=ax)\n",
    "\n",
    "for ax_idx in [3,4,5]:\n",
    "    axs_l[ax_idx].set_xlabel('RMCA',fontsize=ax)\n",
    "\n",
    "titles = ['i', 'ii', 'iii', 'iv', 'v', 'vi']\n",
    "for i in range(len(titles)):\n",
    "    axs_l[i].text(0.58, 1e4, titles[i], \n",
    "                 bbox=dict(boxstyle='round', facecolor='white', alpha=1.0))\n",
    "\n",
    "plt.ylim(10,)\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig('/figures/RMCA_distribution.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
